{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author: Lee Meng\n",
    "- date: 2018-10-30 08:00\n",
    "- title: 自然語言處理 - 學習資源整理\n",
    "- slug: nlp\n",
    "- tags: 自然語言處理, 機器學習\n",
    "- description: \n",
    "- summary: \n",
    "- image: alisa-anton-393305-unsplash.jpg\n",
    "- image_credit_url: https://unsplash.com/photos/rjhLxgmP8bA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\n",
    "- status: draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the Dots\n",
    "- 前面待補（Androw Ng 的課、Coursera Machine Learning）\n",
    "- 李鴻毅 Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Know the terminologies / key concept / history\n",
    "* Conferences\n",
    "    - ICLR\n",
    "    - ICML\n",
    "    - NIPS\n",
    "    - PMLR\n",
    "    - ACL, EMNLP, NAACL\n",
    "    - 找出他們的 best reward paper\n",
    "* Find the best references\n",
    "    * AllenNLP - Writing Code for NLP Research Slide \n",
    "* learn the best framework to enable quick prototyping \n",
    "    * AllenNLP: Config as Model variations\n",
    "    * Write code that can be reused and be more productive in the future\n",
    "* Keep up with trend and try many things \n",
    "* Follow influential researchers and those can explain things very intuitive\n",
    "    * 華裔\n",
    "        * 台大李鴻毅\n",
    "    * Ian Goodfellow\n",
    "* Solve real world problem that make people and yourself better\n",
    "    * 英文文章摘要\n",
    "    * Learn English more effective using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 學習資源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN\n",
    "    * 一分鐘學ＡＩ(4) — 循環神經網路 – 陳鍾誠 – Medium\n",
    "* NLP\n",
    "    * 徐阿城\n",
    "    * Google AI - Language\n",
    "* 知乎討論\n",
    "    * 神經機器翻譯\n",
    "    * 機器翻譯\n",
    "* Framework\n",
    "    * OpenNMT\n",
    "    * TensorFlow - seq2seq Tutorial\n",
    "    * AllenNLP\n",
    "* Blog\n",
    "    * Wildml.com\n",
    "* Deep Learning\n",
    "    * 台大李鴻毅\n",
    "        * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相關技術文章\n",
    "- [Google open-sources BERT, a state-of-the-art pretraining technique for natural language processing](https://venturebeat.com/2018/11/02/google-open-sources-bert-a-state-of-the-art-training-technique-for-natural-language-processing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 術語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NLP 研究領域\n",
    "    * Social Applications\n",
    "    * Semantics\n",
    "    * Vision\n",
    "    * Entities & Coreference\n",
    "        * Named Entity Recognition (NER)\n",
    "    * Machin Translation\n",
    "    * Multilingual Methods\n",
    "    * Question Answering\n",
    "        * Question Classification\n",
    "    * Dialogue and Discourse\n",
    "    * Generation and Summarization\n",
    "    * Text Classification, Text Mining and Information Retrieval\n",
    "    * Reading Comprehension\n",
    "    * Generation\n",
    "* Deep Learning\n",
    "    * Network structure\n",
    "* Language Model / n-gram model 很好的回顧\n",
    "    * NLP 笔记 - Language models and smoothing\n",
    "    * Language Model / f(e)\n",
    "    * N-gram model / Markov Assumption / Chain Rule / MLE\n",
    "    * Uniform Model / Unigram Model (依據 work fq) / N-gram Model / Full History Model\n",
    "    * Smoothing: Laplace smoothing (add-one) / Good Turing smoothing, Normalization\n",
    "* SMT\n",
    "    * Statistical Machine Translation，統計機器翻譯\n",
    "        * 非限定領域機器翻譯中效能較佳的一種方法\n",
    "        * 從早期基於詞的機器翻譯已經過渡到基於短語的翻譯，並正在融合句法資訊，以進一步提高翻譯的精確性。\n",
    "    * Noise Channel \n",
    "        * 做機器翻譯 (Lexical Translation Model)\n",
    "            * 將目標語言視為 cipher text\n",
    "            * word alignment / EM algorithm  / parallel corpus（平行語料庫）/ aligned corpus \n",
    "            * 利用統計方式(MLE + EM algorithm)以及平行 corpus，找出最有可能的原文與譯文的各個詞之間的對齊方式\n",
    "            * 从 lexical 层面着手，先进行词对词的翻译，然后再进行词的排序\n",
    "            * 最好的翻譯就等於 argmax ( language model p(e) * translation model p(f | e)\n",
    "            * Ref: \n",
    "                * 機器翻譯 -- Statistical Machine Translation\n",
    "        * 做拼寫糾正\n",
    "            * NLP 笔记 - Spelling, Edit Distance, and Noisy Channels\n",
    "            * 給定一個錯字 Observation / Noisy Word, 我們想知道最有可能產生此結果的原始正確字 (word)推薦給使用者。最有可能的結果是 argmaxP(W)∗P(O|W) 。理想上，我們要推薦給使用者自動校正的詞，該詞真的是使用者要的機率跟 (1) 該詞本身出現的機率 (2) 該正確詞被錯誤打成 noisy word 的機率成正比。(1) 用 LM, (2) 用 Channel Model / edit distance 估計\n",
    "* NMT\n",
    "    * Phase based machine translation (PBMT)\n",
    "    * Neural network language model(NNLM)\n",
    "    * google Neural machine translation(GNMT)\n",
    "        * Zero shot translation\n",
    "    * Transformer\n",
    "        * Layers, heads\n",
    "* Self-Attention Mechanism\n",
    "    * Ref\n",
    "        * http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n",
    "        * 注意力机制（Attention Mechanism）在自然语言处理中的应用\n",
    "    * 模擬人眼在看圖片時能聚焦/專注（加大權重）在局部圖片，讓 NMT 的 decoder 在產生一個 output (word) 時，不只看前一個 hidden state, 而是能學習，在做翻譯的時候，應該要選擇關注過往的哪些 states。這樣能同時處理 aligement 跟 translation，並讓語順不同的語言也可以有不錯的翻譯（ex; 日文 & 英文）\n",
    "    * Easier to visualize what the model is doing\n",
    "    * Visualization\n",
    "        * Vaswani, 2017\n",
    "    * Hard / soft attention\n",
    "* Reinforcement learning\n",
    "    * Reinforcement Learning: An Introduction\n",
    "    * RL4NMT\n",
    "* Open / close domain conversation\n",
    "* LSTM, biLSTM\n",
    "    * \n",
    "* BERT\n",
    "    - Bidirectional Encoder Representations from Transformer\n",
    "* SRNN, IRNN\n",
    "* GRU, GRUStack\n",
    "* embedding\n",
    "* RNN, \n",
    "* Elmo, GloVe\n",
    "* Transformer  / transformer network\n",
    "* Unsupervised MT: Train only with monolingual data\n",
    "* CBOW\n",
    "* Beam search\n",
    "    * Beam size\n",
    "* LRL (low-resource language)\n",
    "* Neural Language Model\n",
    "* Encoder / Decoded Framework VS Transformer\n",
    "* Evaluation\n",
    "    * BLEU score\n",
    "* Monolingual / Mutltilingual data\n",
    "* Unsupervised NMT(this year)  / Unsupervised SMT (latest)\n",
    "- Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
