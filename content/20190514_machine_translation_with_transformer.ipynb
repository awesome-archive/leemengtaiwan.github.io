{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLTnvFZ7oKoq"
   },
   "source": [
    "- author: Lee Meng\n",
    "- date: 2019-06-17 05:40\n",
    "- title: 淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中\n",
    "- slug: neural-machine-translation-with-transformer-and-tensorflow2\n",
    "- tags: 自然語言處理, NLP, Tensorflow\n",
    "- summary: 本文分為兩大部分。前半將帶讀者簡單回顧 Seq2Seq 模型、自注意力機制以及 Transformer 等近年在機器翻譯領域裡頭的重要發展與概念；後半段則將帶著讀者實作一個可以將英文句子翻譯成中文的 Transformer。透過瞭解其背後運作原理，讀者將能把類似的概念應用到如圖像描述、閱讀理解以及語音辨識等各式各樣的機器學習任務之上。\n",
    "- description: 本文分為兩大部分。前半將帶讀者簡單回顧 Seq2Seq 模型、自注意力機制以及 Transformer 等近年在機器翻譯領域裡頭的重要發展與概念；後半段則將帶著讀者實作一個可以將英文句子翻譯成中文的 Transformer。透過瞭解其背後運作原理，讀者將能把類似的概念應用到如圖像描述、閱讀理解以及語音辨識等各式各樣的機器學習任務之上。\n",
    "- image: Tour_de_babel.jpg\n",
    "- image_credit_url: \n",
    "- left_nav_image_ids: transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rn4IVp5PxQOa"
   },
   "source": [
    "<style>\n",
    "   pre {\n",
    "      overflow-x: auto;\n",
    "      word-wrap: break-word;\n",
    "   }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj-jR_VUMBJJ"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# Copyright 2019 Meng Lee @ leemeng.tw\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YuYbjEu5ykKO"
   },
   "source": [
    "!quote\n",
    "- 那時，全世界的語言都一樣。人們說：『來吧，我們要建一座塔，塔頂通天，為了揚我們的名，免得我們被分散到世界各地。』耶和華說：『看哪！他們成爲一樣的人民、用同樣的語言。如今既蓋起塔來，以後就沒有他們無法完成的事情了。我們下去！在那裏變亂他們的口音，使他們的言語彼此不通。』\n",
    "- 《創世記》第十一章\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbqQM1a4eDMG"
   },
   "source": [
    "這是聖經中著名的[巴別塔](https://zh.wikipedia.org/wiki/%E5%B7%B4%E5%88%A5%E5%A1%94)橋段，用來解釋為何當今世上有那麼多種語言。當年的上帝或許過於杞人憂天，但近年多虧了[深度學習](https://zh.wikipedia.org/zh-hant/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0)，[機器翻譯](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91)的快速發展讓人不禁覺得，或許巴別塔很快就不再只是虛幻傳說了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZFeXuix-5RL"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/google-translate.mp4\n",
    "- images/transformer/google-translate.jpg\n",
    "- 以往被視為非常困難的中 -> 英翻譯如今在深度學習的加持下也有不錯的水準"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMksLROx-4kR"
   },
   "source": [
    "\n",
    "機器翻譯的研究之所以如此重要且迷人，是因為它將有機會讓未來任何人都不受語言的限制，獲得世界上任何他或她想要的資訊與知識。\n",
    "\n",
    "在這篇文章的前半部分，我們會先花點時間來回顧[神經機器翻譯](https://en.wikipedia.org/wiki/Neural_machine_translation)裡頭的一些重要概念。接著在具備這些概念以及[其他背景知識](#%E5%B8%AB%E5%82%85%E5%BC%95%E9%80%B2%E9%96%80%EF%BC%8C%E4%BF%AE%E8%A1%8C%E5%9C%A8%E5%80%8B%E4%BA%BA)的前提之下，利用最新的 [TensorFlow 2](https://www.tensorflow.org/) 來實作一個可以將英文句子翻譯成中文的神經網路架構：[Transformer](https://www.tensorflow.org/beta/tutorials/text/transformer)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1_GCfBJ3fhB"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/transformer-high-level-view.png\n",
    "- 利用 Transformer 將法文句子翻譯成英文\n",
    "- http://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWtN8N_l3fDu"
   },
   "source": [
    "這是一個非常簡化的示意圖。Transformer 實際上是一種基於自注意力機制的 [Seq2Seq 模型](https://youtu.be/ZjfjPzXw6og?t=3208)，近年在[圖像描述](https://paperswithcode.com/task/image-captioning)、[聊天機器人](https://zh.wikipedia.org/wiki/%E8%81%8A%E5%A4%A9%E6%A9%9F%E5%99%A8%E4%BA%BA)、[語音辨識](https://zh.wikipedia.org/zh-hant/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB)以及機器翻譯等各大領域大發異彩。但因為其相對複雜，到現在還是有種現象：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnKp8K_95V_n"
   },
   "source": [
    "!quote\n",
    "- 了解 Transformer 相關技術的人已經用了好一陣子且用得很開心，不知道的人還是不知道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0N7fwrBj5Veg"
   },
   "source": [
    "當然這並不僅限於 Transformer，因為深度學習牽涉的研究範圍實在太廣了。透過這篇文章，我希望能幫助你開始了解神經機器翻譯以及 Transformer 的相關知識。\n",
    "\n",
    "當我們完成實作並訓練出一個 Transformer 以後，除了可以英翻中以外，我們還能清楚地了解其是如何利用強大的[注意力機制](https://www.youtube.com/watch?v=jd9DtlR90ak&feature=youtu.be)（我們在 [Encoder-Decoder 模型 + 注意力機制](#Encoder-Decoder-模型-+-注意力機制)一節會仔細探討此概念）來做到精準且自然的翻譯。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3oN_aj5hq42u"
   },
   "source": [
    "!image\n",
    "- transformer/en-to-ch-attention-map.png\n",
    "- Transformer 在將英文句子翻譯成中文時會「關注」需要注意的英文詞彙來生成對應的中文字詞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bIbqdFq-q4LX"
   },
   "source": [
    "除了翻譯出來的中文正確無誤以外，從上圖你可以發現很有趣的現象。\n",
    "\n",
    "給定左側的英文，Transformer 在生成其對應的中文翻譯時都會給每個英文詞彙不同的「注意程度」。小方格越亮則代表模型在生成某中文字時放越多的注意力在左側對應的英文詞彙上。\n",
    "\n",
    "仔細看你會發現這個已經訓練好的 Transformer 在翻譯：\n",
    "- 「必」、「須」時會關注「must」\n",
    "- 「希」、「望」時會關注「hope」\n",
    "- 「公」、「民」時會關注「citizens」\n",
    "\n",
    "乍看之下好像稀鬆平常，但事實上我們在訓練模型時並不會告訴它這些詞彙之間的對應關係或是任何語言學的知識。我們就只是餵給它多組相同意思的中英句子，並讓它自己學會怎麼做翻譯。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IS2fM1F-efo"
   },
   "source": [
    "!quote\n",
    "- 好黑魔法，不學嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G97iSJ5W-kcF"
   },
   "source": [
    "在英翻中的情境下，神經網路要做的事情就是讀入左側的英文句子，接著生成右側的中文句子（繁中對英文的翻譯資料集稀少，此文將以簡體為例）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eN4pqL5D_p0Q"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/en-zh-training-sentences.jpg\n",
    "- 訓練資料是多組相同語義的成對中英句子（當然仍需前處理）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZyocsU7ru67c"
   },
   "source": [
    "## 一些你需先具備的基礎知識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN6tN4nju6fC"
   },
   "source": [
    "我在文中會盡量言簡意賅地介紹所有你需要了解的深度學習概念，並附上相關連結供你參考。但就像在[天龍八部](https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html)或是眾多武俠小說都有提過的重要準則：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GF9Ab8hwzlVV"
   },
   "source": [
    "!quote\n",
    "- 武功修習有先後順序，勿求一步登天。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tic8Q5SPzvaf"
   },
   "source": [
    "儘管在 [2017 年就已被提出](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)，本文即將探討並實作的 [Transformer](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) 仍算是相當進階的神經網路架構。因此具備以下的基礎知識能幫助你更順利地理解本文內容：\n",
    "\n",
    "- 一點點[卷積神經網路](https://demo.leemeng.tw/)的概念\n",
    "- 清楚理解[循環神經網路](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html)的運算方式\n",
    "- 基本的[自然語言處理](http://research.sinica.edu.tw/nlp-natural-language-processing-chinese-knowledge-information/)知識\n",
    "- 基本的[線性代數](https://youtu.be/uUrt8xgdMbs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW)如矩陣相乘運算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44W05E8NgSW3"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/nlp-intro.jpg\n",
    "- 中研院這篇文章清楚地說明了自然語言處理在中文上的研究與應用\n",
    "- http://research.sinica.edu.tw/nlp-natural-language-processing-chinese-knowledge-information/\n",
    "- 研之有物"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEoUgvJHgWh8"
   },
   "source": [
    "希望這樣的要求沒把你嚇跑，因為事實上你大約需要好幾倍的相關知識來成功實作 Transformer。[儘管在實作前你會看到一些額外要求](#%E5%B8%AB%E5%82%85%E5%BC%95%E9%80%B2%E9%96%80%EF%BC%8C%E4%BF%AE%E8%A1%8C%E5%9C%A8%E5%80%8B%E4%BA%BA)，本文的前半部分還是相當平易近人的，還請放心閱讀。\n",
    "\n",
    "當你想要深入了解某些細節的時候，可以參考這節附上的連結或是文內說明概念時附上的圖片來源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSs9oxwK4GzY"
   },
   "source": [
    "!quote\n",
    "- 想更深入了解文中講述的各種概念，點擊相關的「圖片來源」就對了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_aZVwOya4GWB"
   },
   "source": [
    "\n",
    "\n",
    "前言很長，但好戲才在後頭。如果你已經準備好進入神經機器翻譯的世界的話，現在就讓我們正式開始這趟旅程吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9MZfCKhImpA"
   },
   "source": [
    "## 機器翻譯近代史"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6_k8n_Ipa3Y"
   },
   "source": [
    "鑑往知來。了解一點機器翻譯的歷史以及 Transformer 是怎麼跑出來的會對實作很有幫助。\n",
    "\n",
    "機器翻譯（**M**achine **T**ranslation）本身的概念[最早可追溯到 17 世紀](https://zh.wikipedia.org/zh-tw/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91#%E6%AD%B7%E5%8F%B2)。自從那開始，人們嘗試並研究了各式各樣的方法，寫了一大堆規則、蒐集了數以萬計的翻譯結果來嘗試自動化翻譯。隨著時代演進，我們有了：\n",
    "- 基於規則的機器翻譯 RBMT\n",
    "- 基於範例的機器翻譯 EBMT\n",
    "- 統計機器翻譯 SMT\n",
    "- 近年的神經機器翻譯 NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpQo4xKd29R3"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/mt-history.jpg\n",
    "- 近代機器翻譯發展簡史\n",
    "- https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pd4qAW703446"
   },
   "source": [
    "很多遠古時代的東西我們不會討論，而 NMT 當然是本文的重點。不過在那之前讓我們非常簡短地看一下 SMT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72Nachk75jf7"
   },
   "source": [
    "### 統計機器翻譯：基於短語的翻譯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEjwBpDK27_m"
   },
   "source": [
    "機器翻譯的歷史很長，但一直要到 21 世紀初期[統計機器翻譯（**S**tatistical **M**achine **T**ranslation，簡稱 SMT）](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91)技術成熟以後，機器翻譯的品質才稍微使人滿意。其中最知名的例子當屬 [Google 在 2006 年發布的 SMT 翻譯系統](https://ai.googleblog.com/2006/04/statistical-machine-translation-live.html)。\n",
    "\n",
    "不限於 Google，當時不少最先進的 SMT 系統都採用了[基於短語的機器翻譯（Phrase-Based MT）](https://en.wikipedia.org/wiki/Statistical_machine_translation#Phrase-based_translation) 演算法。PBMT 最大的特色是先將來源語言（Source Language）的句子切成短語或是詞彙，接著大致上獨立地將這些詞彙翻譯成目標語言（Target Language）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8xFeIFK65bU"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/pbmt.jpg\n",
    "- 基於短語的 SMT（Phrase-Based SMT）\n",
    "- https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzkgWc6m64xJ"
   },
   "source": [
    "\n",
    "PBMT 的翻譯結果相較於早年基於規則（Rule-Based）的手法已經進步很多，但仍然需要大量的[平行語料](https://zh.wikipedia.org/wiki/%E5%B9%B3%E8%A1%8C%E8%AF%AD%E6%96%99)、對齊語料來取得較好的結果。且因為是以短語為單位在做翻譯，這些短語拼湊出來的句子仍然不夠自然。\n",
    "\n",
    "如果你跟我一樣有用過早年的 Google 翻譯，應該還能隱約記得當年那些充斥著「機械感」的翻譯結果。\n",
    "\n",
    "（如果你有當年 Google 翻譯結果的截圖的話歡迎提供）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-EpZ3d5LxHQH"
   },
   "source": [
    "### 神經機器翻譯：Encoder-Decoder 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EjH10OaXto3"
   },
   "source": [
    "顧名思義，神經機器翻譯 NMT 即代表使用[類神經網路（Neural Network）](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)來做機器翻譯。\n",
    "\n",
    "不管是英文、法文還是中文，一個自然語言的句子基本上可以被視為一個有時間順序的序列數據（Sequence Data）。而[我們曾提過 RNN 很適合用來處理有時間關係的序列數據](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF_1)。給定一個向量序列，RNN 就是回傳一個一樣長度的向量序列作為輸出。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1q6w1Rw0BlR"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- nlp-kaggle-intro/rnn-animate.gif\n",
    "- RNN 很適合拿來處理具有時間順序的序列數據（下方的詞在丟入 RNN 前會被轉成詞向量）\n",
    "- https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjZEGV1uzgmd"
   },
   "source": [
    "\n",
    "當我們把來源語言以及目標語言的句子都視為一個獨立的序列以後，機器翻譯事實上就是一個[序列生成（Sequence Generation）](https://youtu.be/ZjfjPzXw6og)任務：對一個輸入序列（來源語言）做些有意義的轉換與處理以後，輸出一個新的序列（目標語言）。\n",
    "\n",
    "而在深度學習時代，我們一般會使用以 RNN 為基礎的 [Encoder-Decoder 架構（又被稱作 Sequence to Sequence / Seq2Seq 模型）](https://youtu.be/ZjfjPzXw6og?t=3208)來做序列生成："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-5HdzcabPYR"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/seq2seq-animate.mp4\n",
    "- images/transformer/seq2seq-animate.jpg\n",
    "- 一個以 RNN 為基礎的 Encoder-Decoder / Seq2Seq 模型將法文翻譯成英文的步驟\n",
    "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elTBlum6k7f6"
   },
   "source": [
    "Seq2Seq 模型裡頭 Encoder 跟 Decoder 是各自獨立的 RNN。Encoder 把輸入的句子做處理後所得到的隱狀態向量（圖中的 `Hidden State#3`）交給 Decoder 來生成目標語言。\n",
    "\n",
    "你可以想像兩個語義相同的法英句子雖然使用的語言、語順不一樣，但因為它們有相同的語義，Encoder 在將整個**法文**句子濃縮成一個嵌入空間（Embedding Space）中的向量後，Decoder 能利用隱含在該向量中的語義資訊來重新生成具有相同意涵的**英文**句子。\n",
    "\n",
    "\n",
    "這樣的模型就像是在模擬人類做翻譯的[兩個主要過程](https://zh.wikipedia.org/zh-tw/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91#%E7%BF%BB%E8%AD%AF%E6%B5%81%E7%A8%8B)：\n",
    "- （Encoder）解譯來源文字的文意\n",
    "- （Decoder）重新編譯該文意至目標語言\n",
    "\n",
    "當然人類在做翻譯時有更多步驟、也會考慮更多東西，但 Seq2Seq 模型的表現已經很不錯了。\n",
    "\n",
    "有些人閱讀到這裡可能會問：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6a8TBoAq0ijN"
   },
   "source": [
    "!quote\n",
    "- 如果我們利用 Seq2Seq 模型將多種語言的句子都轉換到某個嵌入空間裡頭，該空間會長成什麼樣子呢？是相同語言的句子靠得比較近，還是不同語言但擁有同語義的句子會靠得比較近呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXMwfAqy0huj"
   },
   "source": [
    "這是一個很好的研究問題。\n",
    "\n",
    "而如果我們試著把這個問題圖像化，則結果可能長得像這樣：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJr7qYjR3L0p"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/multi-lang-emb.jpg\n",
    "- 大哉問：神經網路將句子轉換完所形成的向量空間比較靠近左邊還是右邊？\n",
    "- https://youtu.be/ulLx2iPTIcs?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&t=1035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fuj6Jbsb3LS5"
   },
   "source": [
    "圖中的點代表不同句子，不同顏色則代表不同語言。如果結果是左邊，代表神經網路並沒有創出一個「語義」空間，而只是把不同語言都投射到該嵌入空間裡頭的不同位置，接著才在該空間裡進行不同語言之間的轉換（中轉英、英轉法 etc.）。\n",
    "\n",
    "我們比較想要的是右邊的情況：無關語言，只要句子的語義接近，彼此的距離就相近的語義空間。\n",
    "\n",
    "而 [Google 在 2016 年的研究結果](https://aclweb.org/anthology/Q17-1024)發現，在此空間裡頭語言相異但擁有同語義的句子之間的距離 `d1`，要比同語言但不同語義的句子之間的距離 `d2` 要小得多（即  `d1 << d2`）。\n",
    "\n",
    "換句話說，在此空間中同語義的句子會靠得比較近，我們實際得到的空間比較像右邊。\n",
    "\n",
    "而如果我們將這些句子做 [t-SNE](https://distill.pub/2016/misread-tsne/) ，甚至可以得到這樣的結果：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nELXB6wo7yi"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/gnmt-multilingual.mp4\n",
    "- images/transformer/gnmt-multilingual.jpg\n",
    "- 在 Seq2Seq 模型創造出來的「語義」空間裡頭，不同語言但同語義的句子彼此相當接近\n",
    "- https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2Cy5iZAo79z"
   },
   "source": [
    "此研究告訴我們，只要對自然語言做正確的轉換，就能將語言相異但同語義的句子都轉換成彼此距離相近的語義向量，並以此做出好的翻譯。\n",
    "\n",
    "以下是我隨意挑選出來的一組句子，它們在該空間裡的距離相近：\n",
    "\n",
    "\n",
    "```text\n",
    "英文：\n",
    "From low-cost pharmacy brand moisturizers to high-priced cosmetics brand moisturizers, competition is fierce.\n",
    "\n",
    "日文：\n",
    "低価格の薬品ブランドの保湿剤から高価な百貨店の化粧品ブランドのためには, 競争が激しい\n",
    "\n",
    "韓文：\n",
    "싸구려백화점화장품브랜드 moisturizers 에 저렴한약국브랜드 moisturizers 에서 , 경쟁이큰있습니다 \n",
    "```\n",
    "\n",
    "這些句子都代表著類似的意思：「從低價的保濕劑到高價的化妝品牌，競爭都十分激烈」。\n",
    "\n",
    "如果你想進一步了解這個視覺化結果，可以閱讀 [Google Brain 的詳細解說](https://youtu.be/ulLx2iPTIcs?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&t=789)或是上 [Embedding Projector\n",
    "](https://projector.tensorflow.org/) 自己試看看。\n",
    "\n",
    "另外值得注意的是，機器翻譯本身是一種[有條件的序列生成任務（Conditional Sequence Generation）](https://youtu.be/ZjfjPzXw6og?t=2816)：給定一個特定的輸入句子（文字序列），依此條件輸出另外一個句子（文字序列）。這跟在[讓 AI 寫點金庸](https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html)一文中會隨機生成天龍八部文章的[語言模型（Language Model）](https://zh.wikipedia.org/wiki/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B)是有所差異的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSQDXkRJeQ_C"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/lstm-sequence-generation.mp4\n",
    "- images/transformer/lstm-sequence-generation.jpg\n",
    "- 隨機序列生成的例子：一個以 LSTM 實作的簡單語言模型\n",
    "- https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-QPa9ktkAd4"
   },
   "source": [
    "一般來說，語言模型可以在不給定任何輸入的情況下生成非常隨機的文字序列；但針對機器翻譯這種有條件的序列生成任務，我們通常希望給定相同輸入，輸出的結果越穩定越好（或是每次都一模一樣）。\n",
    "\n",
    "我們在[實作的時候](#TODO)會看到怎麼達成這件事情。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0C_V77LXtdR"
   },
   "source": [
    "### Encoder-Decoder 模型 + 注意力機制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnU_2gNTXPEs"
   },
   "source": [
    "好啦，你現在應該已經了解如何使用 Seq2Seq 模型來做 NMT 了，不過現在讓我們再次複習其運作方式。這次我們把用 RNN 實作的 Encoder / Decoder 在每個時間點做的事情從左到右一字排開：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-9F5Qf3nYv4"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/seq2seq-unrolled-no-attention.mp4\n",
    "- images/transformer/seq2seq-unrolled-no-attention.jpg\n",
    "- 以 RNN 為基礎的 Seq2Seq 模型做 NMT 的流程\n",
    "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rg3A1mvVXO4H"
   },
   "source": [
    "基本款的 Seq2Seq 模型表現得不錯，但其實有可以改善的地方。你有看出來了嗎？上圖的輸入句子只有 3 個詞彙，但如果我們想輸入一個很長的句子呢？\n",
    "\n",
    "我們前面曾提過 Seq2Seq 模型裡的一個重要假設是 Encoder 能把輸入句子的語義 / 文本脈絡全都壓縮成**一個**固定維度的語義向量。之後 Decoder 只要利用該向量裡頭的資訊就能重新生成具有相同意義，但不同語言的句子。\n",
    "\n",
    "但你可以想像當我們只有一個向量的時候，是不太可能把一個很長的句子的所有資訊打包起來的。\n",
    "\n",
    "這時候怎麼辦呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zaTqvyGutU72"
   },
   "source": [
    "!quote\n",
    "- 與其只把 Encoder 處理完句子產生的最後「一個」向量交給 Decoder 並要求其從中萃取整句資訊，不如將 Encoder 在處理每個詞彙後所生成的「所有」輸出向量都交給 Decoder，讓 Decoder 自己決定在生成新序列的時候要把「注意」放在 Encoder 的哪些輸出向量上面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9j4sNusPtvog"
   },
   "source": [
    "這事實上就是[注意力機制（Attention Mechanism）](https://www.youtube.com/watch?v=jd9DtlR90ak&feature=youtu.be)的中心思想：提供更多資訊給 Decoder，並透過類似資料庫存取的概念，令其自行學會該怎麼提取資訊。兩篇核心論文分別在 [2014 年 9 月](https://arxiv.org/abs/1409.0473)及 [2015 年 8 月](https://arxiv.org/abs/1508.04025)釋出，概念不難但威力十分強大。\n",
    "\n",
    "以下就是將注意力機制加到 Seq2Seq 模型後的結果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aj48lAkatUsJ"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/seq2seq-unrolled-with-attention.mp4\n",
    "- images/transformer/seq2seq-unrolled-with-attention.jpg\n",
    "- 注意力機制讓 Decoder 在生成新序列時能查看 Encoder 裡所有可能有用的隱狀態向量\n",
    "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfcapLkyxOIW"
   },
   "source": [
    "你可以拉回去跟沒有注意力機制的 Seq2Seq 模型比較一下差異。\n",
    "\n",
    "現在你會看到 Encoder 把處理完每個詞彙所產生的向量都交給 Decoder 了。且透過注意力機制，Decoder 在生成新序列的每個元素時都能**動態地**考慮自己要看哪些 Encoder 的向量（還有決定從中該擷取多少資訊），因此這種運用注意力機制的 Seq2Seq 架構又被稱作[動態的條件序列生成（Dynamic Conditional Generation）](https://youtu.be/ZjfjPzXw6og?t=3528)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQgUrOVfEU64"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/seq2seq_detail.mp4\n",
    "- images/transformer/seq2seq_detail.jpg\n",
    "- 法翻英時，Decoder 在生成每個英文詞彙時都在 Encoder 的每個輸出向量上放不同的注意程度\n",
    "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhlVApWR3lM8"
   },
   "source": [
    "實際構想並證明其有效的研究者們十分厲害，且其概念也挺符合人類直覺的，對吧？\n",
    "\n",
    "為了方便讀者理解，上面動畫實際上隱藏了一些細節：\n",
    "- 呈現算好的注意程度而不是計算過程\n",
    "- Encoder / 跟 Decoder 的實際架構\n",
    "\n",
    "既然是深度學習，Encoder / Decoder 一般來說都是由多個 [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) / [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit) 等 RNN Layers 所疊起來的。而注意力機制在這種情境下實際的運作方式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mri1P69Y3oAC"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/attention_mechanism_luong.jpg\n",
    "- 英翻法情境下，Decoder 在第一個時間點進行的注意力機制\n",
    "- https://github.com/tensorflow/nmt#background-on-the-attention-mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARn28QH97rXc"
   },
   "source": [
    "左右兩邊分別是 Encoder 與 Decoder ，縱軸則是多層的神經網路區塊 / 層。\n",
    "\n",
    "雖然上張動畫是法翻英（這邊是英翻法），但該動畫也是以一樣的概念將圖中的注意權重（attention weights ）視覺化出來（注意權重和為 1）。\n",
    "\n",
    "現在讓我們看一下注意力機制實際的計算步驟。在 Decoder 的每個時間點，我們都會進行注意力機制以讓 Decoder 從 Encoder 取得語境資訊：\n",
    "\n",
    "1. 拿 Decoder 當下的紅色隱狀態向量 `ht` 跟 Encoder 所有藍色隱狀態向量 `hs` 做比較，利用 `score` 函式計算出 `ht` 對每個 `hs` 的注意程度\n",
    "2. 以此注意程度為權重，**加權平均**所有 Encoder 隱狀態 `hs` 以取得上下文向量 `context vector`\n",
    "3. 將此上下文向量與 Decoder 隱狀態結合成一個注意向量 `attention vector` 並作為該時間的輸出\n",
    "4. 該注意向量會作為 Decoder 下個時間點的輸入\n",
    "\n",
    "定義 `score` 函式的方式不少，現在就先讓我們假設有這麼一個函式。\n",
    "\n",
    "至此為止，你應該已經能夠看懂注意力機制的計算公式：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkY0Y6AJnrEI"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/attention-equation.jpg\n",
    "- 注意力機制前 3 步驟的數學式子\n",
    "- https://github.com/tensorflow/nmt#background-on-the-attention-mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wmyP-wPnla_"
   },
   "source": [
    "而之所以稱為注意權重（attention weights），是因為注意力機制可以被視為是一個學習來源語言和目標語言**每一個單詞之間關係**的小型神經網路，而這些權重是該神經網路的參數。\n",
    "\n",
    "我們在[後面的章節](#TODO)會實際看到，在訓練還沒開始前，這些權重都是隨機且無意義的。是透過訓練，神經網路才知道該為這些權重賦予什麼值。\n",
    "\n",
    "你也會發現我在文中提及多次的「注意程度」就是這裡的「注意權重」，而前者是一種擬人化的說法。你可以想像這些權重值讓當下的 Decoder 曉得該放多少關注在 Encoder 個別的隱狀態身上，並依此從它們身上取得上下文資訊（步驟 2）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_KZIQb7AbNq"
   },
   "source": [
    "!quote\n",
    "- 而事實上神經網路並沒有意識，因此也不會有感知層次上的「注意」。它學到的是讓注意力機制產生最好結果的「參數權重」，而不是我們人類想像的「注意程度」。只有人類可以賦予神經網路裡頭的計算意義。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBmOp86P641z"
   },
   "source": [
    "有點扯遠了，畢竟這裡應該沒有人文學系的讀者。\n",
    "\n",
    "讓我們拉回注意力機制。\n",
    "\n",
    "將此機制加入 Seq2Seq 模型後，NMT 系統的翻譯水準再次起飛。Google 在 2016 年推出的 [Google Neural Machine Translation system（GNMT）](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html) 是一個知名的案例。除了注意力機制以外，GNMT [在 Encoder 跟 Decoder 都採用了多達 8 層的 LSTM 神經網路](https://arxiv.org/abs/1609.08144)，讓更多人見識到深度學習的威力。\n",
    "\n",
    "跟 Google 10 年前推出的 PBMT 系統比起來，翻譯錯誤率平均下降了 60 %。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxyE4uJZpbD3"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/nmt-model-fast.mp4\n",
    "- images/transformer/nmt-model-fast.jpg\n",
    "- 利用注意力機制的 GNMT 讓 Decoder 在生成「Knowledge」時能放注意力在 Encoder 處理完「知」與「識」的兩個輸出向量 e0 & e1\n",
    "- https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRKN4SbcpbR6"
   },
   "source": [
    "上圖為 GNMT 做中翻英的過程。Encoder 跟 Decoder 之間的線條代表注意力（Attention），線條越粗代表下面的 Decoder 在生成某英文字時越關注上方的某些中文字。模型自己學會在翻譯時該看來源句子中的哪些資訊，很聰明，不是嗎？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncLSFUoM95VG"
   },
   "source": [
    "\n",
    "因為其卓越的翻譯品質，在 GNMT 推出的那段時間，搭配注意力機制的 Seq2Seq 模型基本上就是拿來做 NMT 系統的不二人選。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKqd-8CUFZ4y"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/nmt-vs-pbmt.png\n",
    "- NMT、PBMT 以及人類在中英翻譯時的結果比較\n",
    "- https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3I9_Sa6fweNY"
   },
   "source": [
    "話說當年 Google 導入 GNMT 時釋出了 8 個語言之間的對應翻譯，[涵蓋了約 1/3 的世界人口以及超過 35 % 的 Google 翻譯查詢](https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/)，是機器翻譯發展的一個重要里程碑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsxdrIFcYZVg"
   },
   "source": [
    "### Transformer：Seq2Seq 模型 + 自注意力機制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgO3zKOtfn-c"
   },
   "source": [
    "好酒沉甕底，萬眾矚目的時刻來了。\n",
    "\n",
    "標題已經破梗。你已經知道我們將探討本文主角 Transformer，且理論上越後面出來的 BOSS 越強。\n",
    "\n",
    "但你現在可能在想："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "attSUtRDhXnO"
   },
   "source": [
    "!quote\n",
    "- Seq2Seq 模型搭配注意力機制感覺已經很猛了，難道還有什麼可以改善的嗎？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3noKick8hY1j"
   },
   "source": [
    "答案是肯定的 Yes。\n",
    "\n",
    "不過這次問題不是出在 Encoder 跟 Decoder 中間交換的資訊不夠，也不是 Seq2Seq 架構本身有什麼問題，問題是出在我們是用 **RNN** 來實作 Encoder 以及 Decoder。\n",
    "\n",
    "[循環神經網路 RNN](http://colah.github.io/posts/2015-09-NN-Types-FP/) 時常被拿來處理序列數據，但其運作方式存在著一個困擾研究者已久的問題：無法有效地平行運算。以一個有 4 個元素的輸入序列為例：\n",
    "\n",
    "```text\n",
    "[a1, a2, a3, a4]\n",
    "```\n",
    "\n",
    "要獲得最後一個時間點的輸出向量 `b4` 得把整個輸入序列跑過一遍才行："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXW45LOBkKH3"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/rnn-vs-self-attn-layer.jpg\n",
    "- 自注意層可以做到跟雙向 RNN 一樣的事情，還可以平行運算\n",
    "- https://www.youtube.com/watch?v=ugWDIIOHtPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_P_qkrdMn2Hp"
   },
   "source": [
    "[Google 在 2017 年 6 月的一篇論文：Attention Is All You Need](https://arxiv.org/abs/1706.03762) 裡參考了注意力機制，提出了**自**注意力機制（Self-Attention mechanism）。這個機制不只跟 RNN 一樣可以處理序列數據，還可以平行運算。\n",
    "\n",
    "以剛剛的輸入序列 `a[]` 為例：\n",
    "\n",
    "```text\n",
    "[a1, a2, a3, a4]\n",
    "```\n",
    "\n",
    "一個自注意層（Self-Attention Layer）可以利用矩陣運算在等同於 RNN 的一個時間點內就回傳所有 `bi` ，且每個 `bi` 都包含了整個輸入序列的資訊。相比之下，RNN 得經過 4 個時間點依序看過 `[a1, a2, a3, a4]` 以後才能取得序列中最後一個元素的輸出 `b4` 。\n",
    "\n",
    "雖然我們還沒講到實際的運作過程，但在給定一個輸入序列的情境下，自注意力機制的基本精神就是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAxMuD8hEvWA"
   },
   "source": [
    "!quote\n",
    "- 在建立序列中每個元素的 repr. 時，同時去「注意」並擷取同個序列中其他元素的語義資訊。接著將這些語義資訊合併成上下文資訊並當作自己的 repr. 回傳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2AfMZWTwK9j"
   },
   "source": [
    "repr. 為 [representation](https://dictionary.cambridge.org/zht/%E8%A9%9E%E5%85%B8/%E8%8B%B1%E8%AA%9E-%E6%BC%A2%E8%AA%9E-%E7%B9%81%E9%AB%94/representation) 縮寫，在本文的機器翻譯情境裡頭，其意味著可以用來描述某個詞彙、句子意涵的多維實數張量。\n",
    "\n",
    "雖然我們一直強調自注意力機制的平行能力，如果你還記得我們在[上一節](#Encoder-Decoder-模型-+-注意力機制)講述的注意力機制，就會發現在 Seq2Seq 架構裡頭自注意力機制跟注意力機制講的根本是同樣一件事情：\n",
    "- 注意力機制讓 Decoder 在生成輸出元素的 repr. 時關注 Encoder 的輸出序列，從中獲得上下文資訊\n",
    "- 自注意力機制讓 Encoder 在生成輸入元素的 repr. 時關注自己序列中的其他元素，從中獲得上下文資訊\n",
    "- 自注意力機制讓 Decoder 在生成輸出元素的 repr. 時關注自己序列中的其他元素，從中獲得上下文資訊\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9-S-zSHx7-K"
   },
   "source": [
    "\n",
    "我們發現一個非常重要的模式：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8RcwZGusl6z"
   },
   "source": [
    "!quote\n",
    "- 注意力機制跟自注意力機制都是讓序列 q 關注序列 k 來將上下文資訊 v 匯總到序列 q 的 repr. 裡頭，只是使用的序列不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6R33cyUOzDUu"
   },
   "source": [
    "這也是為何[在後面實作時我們只需要一個注意函式](#Scaled-dot-product-attention：一種注意函式)就好了。總之透過新設計的自注意力機制以及原有的注意力機制，[Attention Is All You Need 論文](https://arxiv.org/abs/1706.03762)作者們打造了一個完全不需使用 RNN 的 Seq2Seq 模型：Transformer。以下是 Transformer 中非常簡化的 Encoder-Decoder 版本，讓我們找找哪邊用到了（自）注意力機制："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiXFGK1DzQm1"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/Transformer_decoder.png\n",
    "- 在 Transformer 裡頭共有 3 個地方用到（自）注意力機制\n",
    "- http://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SffndzoxzPE5"
   },
   "source": [
    "在 Transformer 裡頭，Decoder 利用注意力機制關注 Encoder 的輸出序列（Encoder-Decoder Attention），而 Encoder 跟 Decoder 各自利用自注意力機制關注自己處理的序列（Self-Attention）。無法平行運算的 RNN 完全消失，名符其實的 Attention is all you need.\n",
    "\n",
    "以下則是 Transformer 實際上將英文句子翻譯到法文的過程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xdn3dm2C7l0"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/transformer-nmt-encode-decode.mp4\n",
    "- images/transformer/transformer-nmt-encode-decode.jpg\n",
    "- 用 Transformer 將英文句子翻譯到法文的例子\n",
    "- https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZyUcEfWC7Gc"
   },
   "source": [
    "以 Transformer 實作的 NMT 系統基本上可以分為 6 個步驟：\n",
    "1. Encoder 為輸入序列裡的每個詞彙產生初始的 repr. （即詞向量），以空圈表示\n",
    "2. 利用自注意力機制將序列中所有詞彙的語義資訊各自匯總成每個詞彙的 repr.，以實圈表示\n",
    "3. Encoder 重複 N 次自注意力機制，讓每個詞彙的 repr. 彼此持續修正以完整納入上下文語義\n",
    "4. Decoder 在生成每個法文字時也運用了自注意力機制，關注自己之前已生成的元素，將其語義也納入之後生成的元素\n",
    "5. 在自注意力機制後，Decoder 接著利用注意力機制關注 Encoder 的所有輸出並將其資訊納入當前生成元素的 repr.\n",
    "6. Decoder 重複步驟 4, 5 以讓當前元素完整包含整體語義\n",
    "\n",
    "上面動畫的 N 為 3，代表著 Encoder 與 Decoder 的層數。這是一個可以依照任務調整的超參數。\n",
    "\n",
    "如果你看懂這張圖的資訊流動，就等於瞭解 Transformer 的核心精神了，恭喜！如果仍然有不明瞭的地方，可以搭配我上面的說明多看幾遍動畫或是直接閱讀 [Google AI 部落格的原文介紹](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xcf2PlAgJNPR"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/en-ge-bleu-comparison.png\n",
    "- Transformer 釋出時與其他模型在英德翻譯資料集上的比較\n",
    "- https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE208OAnJMz_"
   },
   "source": [
    "自注意力機制解開了 RNN 加在 GPU 上的拘束器。作者們用了 8 個 [NVIDIA P100 GPU](https://www.nvidia.com.tw/object/tesla-p100-tw.html)，花了 3 天半訓練了一個 Transformer，而該模型在 [WMT 2014](http://statmt.org/wmt14/)  英法 / 英德翻譯都取得了最高水準的成績。\n",
    "\n",
    "跟其他模型相比，這訓練時間跟其創造的優異成績在當時可以說是逆天的存在。自此「大注意時代」展開，該論文至今超過 1800 次引用，所有研究領域都被自注意力機制相關的論文洗了一波。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcA7mmc2yY-S"
   },
   "source": [
    "!image\n",
    "- transformer/ramona-flwrs-1310216-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JeCod_p7yYjq"
   },
   "source": [
    "沒能趕上開心洗論文的最佳時機也別傷心難過，對我們來說仍然有個十分重要的訊息：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGgIPwOcn10o"
   },
   "source": [
    "!quote\n",
    "- 多數以 RNN 做過的研究，都可以用自注意力機制來取代；多數用 Seq2Seq 架構實現過的應用，也都可以用 Transformer 來替換。模型訓練速度更快，結果可能更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4xVNKtlzYTl"
   },
   "source": [
    "這也是我決定寫這篇文章的理由之一。雖然本文是以機器翻譯的角度來介紹 Transformer，但事實上只要是能用 RNN 或 Seq2Seq 模型進行的研究領域，你都會看到已經有大量跟（自）注意力機制或是 Transformer 有關的論文了：\n",
    "- 文本摘要（Text Summarization）\n",
    "- 圖像描述（Image Captioning）\n",
    "- 閱讀理解（Reading Comprehension）\n",
    "- 語音辨識（Voice Recognition）\n",
    "- 語言模型（Language Model）\n",
    "- 聊天機器人（Chat Bot）\n",
    "- 其他任何可以用 RNN 的潛在應用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- transformer/bert-and-gpt-post.jpg\n",
    "- 知名的 BERT 與 GPT-2 都是 Transformer 的延伸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4xVNKtlzYTl"
   },
   "source": [
    "當然不是每個人都喜歡或需要看論文。如果你只是想要應用 Transformer 也沒問題。我在[進擊的 BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)一文詳細說明你可以如何用 Transformer-based 的語言代表模型進行遷移學習（transfer learning），輕鬆利用前人智慧來完成手上的 NLP 任務； [OpenAI 的 GPT](https://openai.com/blog/better-language-models/) 則是非常厲害的語言模型，能產生非常順暢的文章。你可以參考我的 GPT-2 文章：[直觀理解 GPT-2 語言模型並生成金庸武俠小說](https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html)。\n",
    "\n",
    "這些都是 Transformer 的應用。想了解更多，我推薦李宏毅教授最近[講解 ELMO、BERT 以及 GPT 的 YouTube 影片](https://www.youtube.com/watch?v=UYPa347-DdE)，十分通俗易懂 ："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esOkyQvwqOzs"
   },
   "source": [
    "!youtube\n",
    "- UYPa347-DdE\n",
    "- 李宏毅教授講解目前 NLP 領域的最新研究是如何讓機器讀懂文字的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vlrk8938qON7"
   },
   "source": [
    "如果你接下來想往深度學習領域發展（尤其是自然語言處理這塊），了解（自）注意力機制以及 Transformer 的運作方式幾乎可以說是必經之路。就算沒打算自己手刻 Transformer，你現在應該也稍微能夠體會現代的神經網路到底在在對自然語言做些什麼了。\n",
    "\n",
    "至此本文的上半部分結束。在下半段我們將實作一個能進行英翻中的 Transformer。等等會說明一項要你完成的事情，不過現在先離開位置喝點東西、讓眼睛跟腦袋休息一下吧！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1REuXhcvu91"
   },
   "source": [
    "!image\n",
    "- transformer/adam-jaime-119551-unsplash.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_DgLan9Q2SY"
   },
   "source": [
    "## 師傅引進門，修行在個人"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v4gFBBPIUQoZ"
   },
   "source": [
    "你回來了嗎？還是等不及待地想繼續往下閱讀？\n",
    "\n",
    "接下來我們會進入實際的程式實作。但跟前半段相比難度呈指數型上升，因此我只推薦符合以下條件的讀者閱讀：\n",
    "\n",
    "- 想透過實作 Transformer 來徹底了解其內部運作原理的人\n",
    "- 願意先花 1 小時了解 Transformer 的細節概念與理論的人"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJBIij8mVcj7"
   },
   "source": [
    "你馬上就會知道 1 個小時代表什麼意思。如果你覺得這聽起來很 ok，那可以繼續閱讀。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EBWKcVaFi6rG"
   },
   "source": [
    "!image\n",
    "- transformer/obama-not-bad.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlw0_JhG-c7U"
   },
   "source": [
    "在[機器翻譯近代史](#機器翻譯近代史)一章我們已經花了不少篇幅講解了許多在實作 Transformer 時會有幫助的重要概念，其中包含：\n",
    "- [Seq2Seq 模型的運作原理](#神經機器翻譯：Encoder-Decoder-模型)\n",
    "- [注意力機制的概念與計算過程](#Encoder-Decoder-模型-+-注意力機制)\n",
    "- [自注意力機制與 Transformer 的精神](#Transformer：Seq2Seq-模型-+-自注意力機制)\n",
    "\n",
    "\n",
    "壞消息是，深度學習裡頭理論跟實作的差異常常是很大的。儘管這些背景知識對理解 Transformer 的精神非常有幫助，對從來沒有用過 [RNN 實現文本生成](https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html)或是以[ Seq2Seq 模型 + 注意力機制實現過 NMT](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) 的人來說，要在第一次就正確實現 Transformer 仍是一個巨大的挑戰。\n",
    "\n",
    "就算不說理論跟實作的差異，讓我們看看 [TensorFlow 官方釋出的最新 Transformer 教學](https://www.tensorflow.org/alpha/tutorials/text/transformer)裡頭有多少內容：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGReIpVUKseM"
   },
   "source": [
    "!mp4\n",
    "- images/transformer/tf-tutorial-oveview.mp4\n",
    "- images/transformer/tf-tutorial-oveview.jpg\n",
    "- TensorFlow 官方的 Transformer 教學"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uOgPpchOT3A"
   },
   "source": [
    "上面是我用這輩子最快的速度捲動該頁面再加速後的結果，可以看出內容還真不少。儘管中文化很重要，我在這篇文章裡不會幫你把其中的敘述翻成中文（畢竟你的英文可能比我好）\n",
    "\n",
    "反之，我將利用 TensorFlow 官方的程式碼，以最適合「初心者」理解的實作順序來講述 Transformer 的重要技術細節及概念。在閱讀本文之後，你將有能力自行理解 TensorFlow 官方教學以及其他網路上的實作（比方說 HarvardNLP 以 [Pytorch](https://pytorch.org/) 實現的 [The Annotated Transformer](http://nlp.seas.harvard.edu//2018/04/03/attention.html#additional-components-bpe-search-averaging)）。\n",
    "\n",
    "但在實作前有件事情要請你完成：觀看個 YouTube 影片。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQql0GyCX01a"
   },
   "source": [
    "!youtube\n",
    "- ugWDIIOHtPA\n",
    "- 教授講解 self-attention 計算方式及 Transformer 的運作原理，強力推薦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piXU-v8cDyic"
   },
   "source": [
    "現在閱讀此文的讀者真的很幸福。\n",
    "\n",
    "李宏毅教授前陣子才在[他 2019 年的台大機器學習課程](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html)發佈了 [Transformer 的教學影片](https://www.youtube.com/watch?v=ugWDIIOHtPA)，而這可以說是世界上最好的中文教學影片。如果你真的想要深入理解 Transformer，在實作前至少把上面的影片看完吧！你可以少走很多彎路。\n",
    "\n",
    "實作時我會盡量重述關鍵概念，但如果有先看影片你會比較容易理解我在碎碎念什麼。如果看完影片你的小宇宙開始發光發熱，也可以先讀讀 [Transformer 的原始論文](https://arxiv.org/abs/1706.03762)，跟很多學術論文比起來相當好讀，真心不騙。\n",
    "\n",
    "重申一次，除非你已經了解基本注意力機制的運算以及 Transformer 的整體架構，否則我不建議繼續閱讀。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6av47j8gJtX0"
   },
   "source": [
    "!image\n",
    "- transformer/you-should-not-pass.jpg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TeCxGAmGCodO"
   },
   "source": [
    "## 11 個重要 Transformer 概念回顧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOmQ0QaFXbsl"
   },
   "source": [
    "怎麼樣？你應該已經從教授的課程中學到不少重要概念了吧？我不知道你還記得多少，但讓我非常簡單地幫你複習一下。\n",
    "\n",
    "1. 自注意層（Self-Attention Layer）跟 RNN 一樣，輸入是一個序列，輸出一個序列。但是該層可以平行計算，且輸出序列中的每個向量都已經看了整個序列的資訊。\n",
    "\n",
    "2. 自注意層將輸入序列 `I` 裡頭的每個位置的向量 `i` 透過 3 個線性轉換分別變成 3 個向量：`q`、`k` 和 `v`，並將每個位置的 `q` 拿去跟序列中其他位置的 `k` 做匹配，算出匹配程度後利用 softmax 層取得介於 0 到 1 之間的權重值，並以此權重跟每個位置的 `v` 作加權平均，最後取得該位置的輸出向量 `o`。全部位置的輸出向量可以同時平行計算，最後輸出序列 `O`。\n",
    "\n",
    "3. 計算匹配程度（注意）的方法不只一種，只要能吃進 2 個向量並吐出一個數值即可。但在 Transformer 論文原文是將 2 向量做 dot product 算匹配程度。\n",
    "\n",
    "4. 我們可以透過大量矩陣運算以及 GPU 將概念 2 提到的注意力機制的計算全部平行化，加快訓練效率（也是本文實作的重點）。\n",
    "\n",
    "5. 多頭注意力機制（Multi-head Attention）是將輸入序列中的每個位置的 `q`、`k` 和 `v` 切割成多個 `qi`、`ki` 和 `vi` 再分別各自進行注意力機制。各自處理完以後把所有結果串接並視情況降維。這樣的好處是能讓各個 head 各司其職，學會關注序列中不同位置在不同 representaton spaces 的資訊。\n",
    "\n",
    "6. 自注意力機制這樣的計算的好處是「天涯若比鄰」：序列中每個位置都可以在 O(1) 的距離內關注任一其他位置的資訊，運算效率較雙向 RNN 優秀。\n",
    "\n",
    "7. 自注意層可以取代 Seq2Seq 模型裡頭以 RNN 為基礎的 Encoder / Decoder，而實際上全部替換掉後就（大致上）是 Transformer。\n",
    "\n",
    "8. 自注意力機制預設沒有「先後順序」的概念，而這也是為何其可以快速平行運算的原因。在進行如機器翻譯等序列生成任務時，我們需要額外加入位置編碼（Positioning Encoding）來加入順序資訊。而在 Transformer 原論文中此值為手設而非訓練出來的模型權重。\n",
    "\n",
    "9. Transformer 是一個 Seq2Seq 模型，自然包含了 Encoder / Decoder，而 Encoder 及 Decoder 可以包含多層結構相同的 blocks，裡頭每層都會有 multi-head attention 以及 Feed Forward Network。\n",
    "\n",
    "10. 在每個 Encoder / Decoder block 裡頭，我們還會使用殘差連結（Residual Connection）以及 Layer Normalization。這些能幫助模型穩定訓練。\n",
    "\n",
    "11. Decoder 在關注 Encoder 輸出時會需要遮罩（mask）來避免看到未來資訊。我們後面會看到，事實上還會需要其他遮罩。\n",
    "\n",
    "\n",
    "這些應該是你在看完影片後學到的東西。如果你想要快速複習，這裡則是[教授課程的 PDF 檔](https://bit.ly/2QT4loG)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epXIxe_QHdMY"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "另外你之後也可以隨時透過左側導覽的圖片 icon 來快速回顧 Transformer 的整體架構以及教授添加的註解。我相信在實作的時候它可以幫得上點忙：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRBJQcymkmwc"
   },
   "source": [
    "!mp4\n",
    "- images/transformer/transformer-left-nav.mp4\n",
    "- images/transformer/transformer-left-nav.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1LmKyB-B2aG"
   },
   "source": [
    "有了這些背景知識以後，在理解程式碼時會輕鬆許多。你也可以一邊執行 [TensorFlow 官方的 Colab 筆記本](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb)一邊參考底下實作。\n",
    "\n",
    "好戲登場！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13OjuLcfZQxr"
   },
   "source": [
    "## 安裝函式庫並設置環境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvPyIISrZ7AR"
   },
   "source": [
    "在這邊我們引進一些常用的 [Python](https://www.python.org/) 函式庫，這應該不需要特別說明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whVkuN7xZXJY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDZS53uEaadL"
   },
   "source": [
    "比較值得注意的是我們將以[最新的 TensorFlow 2 Beta 版本](https://pypi.org/project/tf-nightly-2.0-preview/)來實作本文的 Transformer。另外也會透過 [TensorFlow Datasets](https://www.tensorflow.org/datasets) 來使用前人幫我們準備好的英中翻譯資料集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V_sMDzDT9x4r",
    "outputId": "1d9dd548-2895-4efb-ab1d-bbec513d07ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==2.0.0-beta0\n",
    "clear_output()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlecXVYp2KVk"
   },
   "source": [
    "另外為了避免 TensorFlow 吐給我們太多不必要的資訊，在此文中我也將改變 logging 等級。[在 TensorFlow 2 裡頭因為 `tf.logging` 被 deprecated](https://www.tensorflow.org/alpha/guide/effective_tf2#api_cleanup)，我們可以直接用 `logging`  模組來做到這件事情：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hs9pbw4DcAKR"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=\"error\")\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pw2CWqAKkKgY"
   },
   "source": [
    "我們同時也讓 numpy 不要顯示科學記號。這樣可以讓我們之後在做一些 Tensor 運算的時候版面能乾淨一點。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOY-yIyRgCxT"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "!pip install pysnooper\n",
    "import pysnooper\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7Xs8MsJkLOB"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# seaborn 中文字體\n",
    "if not os.path.exists(\"/usr/share/fonts/SimHei/simhei.ttf\"):\n",
    "  !mkdir /usr/share/fonts/SimHei\n",
    "  !wget  http://d.xiazaiziti.com/en_fonts/fonts/s/SimHei.ttf\n",
    "  !wget -O /usr/share/fonts/SimHei/simhei.ttf \\\n",
    "      \"http://d.xiazaiziti.com/en_fonts/fonts/s/SimHei.ttf\"\n",
    "  !chmod 755 /usr/share/fonts/SimHei/*.ttf\n",
    "  !sudo apt-get install ttf-mscorefonts-installer\n",
    "  !sudo mkfontscale\n",
    "  !sudo mkfontdir\n",
    "  !fc-cache -fv\n",
    "  !rm -rf ~/.cache/matplotlib\n",
    "  clear_output()\n",
    "import seaborn as sns\n",
    "sns.set(font='SimHei')\n",
    "plt.rcParams['font.sans-serif']=['SimHei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRnp7vAF6zpB"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# chinese font setup for matplotlib \n",
    "# ref: https://www.jianshu.com/p/fc9a502ad243\n",
    "# !wget -O /usr/share/fonts/truetype/liberation/simhei.ttf \\\n",
    "#     \"http://d.xiazaiziti.com/en_fonts/fonts/s/SimHei.ttf\"\n",
    "# zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/truetype/liberation/simhei.ttf')\n",
    "# import matplotlib as mpl\n",
    "# zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/SimHei/simhei.ttf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6GuRHOQXDf9"
   },
   "source": [
    "接著定義一些之後在儲存檔案時會用到的路徑變數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDhJBoOjXFQk"
   },
   "outputs": [],
   "source": [
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOUcsMHIKeZI"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# google drive sync\n",
    "\n",
    "save_to_gdrive = True\n",
    "dataset_in_gdrive = False # set to True for speed up without training\n",
    "\n",
    "\n",
    "if save_to_gdrive:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  output_dir = os.path.join(\"/content/gdrive/My Drive\", output_dir)\n",
    "    \n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "\n",
    "if dataset_in_gdrive:\n",
    "  download_dir = os.path.join(output_dir, \"tensorflow-datasets/downloads\")\n",
    "else:\n",
    "  download_dir = \"tensorflow-datasets/downloads\"\n",
    "    \n",
    "# print(f\"Save result to {output_dir}\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZJo2mNDq9s2"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# [\n",
    "#   en_vocab_file,\n",
    "#   zh_vocab_file,\n",
    "#   checkpoint_path,\n",
    "#   log_dir,\n",
    "#   download_dir\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5jCaEJVSjbT"
   },
   "source": [
    "## 建立輸入管道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qwe7BEElejnN"
   },
   "source": [
    "現行的 GPU 以及 TPU 能透過平行運算幫我們顯著地縮短訓練一個 step 所需的時間。而為了讓平行計算能發揮最佳性能，我們需要最佳化[輸入管道（Input pipeline）](https://www.tensorflow.org/guide/performance/datasets?hl=zh_cn)，以在當前訓練步驟完成之前就準備好下一個時間點 GPU 要用的數據。\n",
    "\n",
    "而我們將透過 [tf.data API](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data) 以及前面導入的 [TensorFlow Datasets](https://www.tensorflow.org/datasets) 來建置高效的輸入管道，並將[機器翻譯競賽 WMT 2019](http://www.statmt.org/wmt19/) 的中英資料集準備好。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPKF8Leq74om"
   },
   "source": [
    "### 下載並準備資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "doooKTAP74MQ"
   },
   "source": [
    "首先看看 `tfds` 裡頭 WMT 2019 的中英翻譯有哪些資料來源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "WmXV0rmS8mgJ",
    "outputId": "b7184c38-2957-4a6a-a41f-603dd11d811e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NamedSplit('train'): ['newscommentary_v14',\n",
      "                       'wikititles_v1',\n",
      "                       'uncorpus_v1',\n",
      "                       'casia2015',\n",
      "                       'casict2011',\n",
      "                       'casict2015',\n",
      "                       'datum2015',\n",
      "                       'datum2017',\n",
      "                       'neu2017'],\n",
      " NamedSplit('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1O86GihLXHV"
   },
   "source": [
    "可以看到在 WMT 2019 裡中英對照的數據來源還算不少。其中幾個很好猜到其性質：\n",
    "- 聯合國數據：`uncorpus_v1`\n",
    "- 維基百科標題：`wikititles_v1`\n",
    "- 新聞評論：`newscommentary_v14`\n",
    "\n",
    "雖然大量數據對訓練神經網路很有幫助，本文為了節省訓練 Transformer 所需的時間，在這裡我們就只選擇一個資料來源當作資料集。至於要選哪個資料來源呢？\n",
    "\n",
    "聯合國的數據非常龐大，而維基百科標題通常內容很短，[新聞評論](http://www.casmacat.eu/corpus/news-commentary.html)感覺是一個相對適合的選擇。我們可以在設定檔 `config` 裡頭指定新聞評論這個資料來源並請 TensorFlow Datasets 下載："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CREyOxF9NsJ"
   },
   "outputs": [],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "  version=tfds.core.Version('0.0.3', experiments={tfds.core.Experiment.S3: False}),\n",
    "  language_pair=(\"zh\", \"en\"),\n",
    "  subsets={\n",
    "    tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "  }\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHBg0Lq4tQ6N"
   },
   "source": [
    "!mp4\n",
    "- dark\n",
    "- images/transformer/tfds-demo.mp4\n",
    "- images/transformer/tfds-demo.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXLxuxitqraK"
   },
   "source": [
    "上面的指令約需 2 分鐘完成，而在過程中 `tfds` 幫我們完成不少工作：\n",
    "- 下載包含原始數據的壓縮檔\n",
    "- 解壓縮得到 CSV 檔案\n",
    "- 逐行讀取該 CSV 裡頭所有中英句子\n",
    "- 將不符合格式的 row 自動過濾\n",
    "- Shuffle 數據\n",
    "- 將原數據轉換成 [TFRecord 數據](https://www.tensorflow.org/alpha/guide/data#consuming_tfrecord_data)以加速讀取\n",
    "\n",
    "多花點時間把相關 [API 文件](https://www.tensorflow.org/datasets/datasets#wmt19_translate)看熟，你就能把清理、準備數據的時間花在建構模型以及跑實驗上面。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mbNrqAj_B71"
   },
   "source": [
    "### 切割資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLbFHztk_BoV"
   },
   "source": [
    "\n",
    "雖然我們只下載了一個新聞評論的數據集，裡頭還是有超過 30 萬筆的中英平行句子。為了減少訓練所需的時間，讓我們使用 `tfds.Split` 定義一個將此數據集切成多個部分的 `split`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9fhB5kLFWvZz",
    "outputId": "dbf38342-9ac0-4bce-bb73-23b959076083"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NamedSplit('train')(tfds.percent[0:20]),\n",
       " NamedSplit('train')(tfds.percent[20:21]),\n",
       " NamedSplit('train')(tfds.percent[21:100]))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_perc = 20\n",
    "val_prec = 1\n",
    "drop_prec = 100 - train_perc - val_prec\n",
    "\n",
    "split = tfds.Split.TRAIN.subsplit([train_perc, val_prec, drop_prec])\n",
    "split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kM2Yob_5s7ii"
   },
   "source": [
    "這個 `split` 請 `tfds` 將剛剛處理好的新聞評論資料集再進一步切成 3 個部分，數據量分佈如下：\n",
    "- Split 1：20% 數據\n",
    "- Split 2：1% 數據\n",
    "- Split 3：79% 數據\n",
    "\n",
    "我們將前兩個 splits 拿來當作訓練以及驗證集，剩餘的部分（第 3 個 split）捨棄不用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jkJhE6bj9I86",
    "outputId": "7dcbfaa9-ce7f-4a43-d5e9-3ac8bb712b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "examples = builder.as_dataset(split=split, as_supervised=True)\n",
    "train_examples, val_examples, _ = examples\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DROvJXUem4v2"
   },
   "source": [
    "你可以在[這邊](https://github.com/tensorflow/datasets/blob/master/docs/splits.md)找到更多跟 `split` 相關的用法。\n",
    "\n",
    "這時候 `train_examples` 跟 `val_examples` 都已經是 [tf.data.Dataset](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset)。我們在[前處理數據](#前處理數據)一節會看到這些數據在被丟入神經網路前需要經過什麼轉換，不過現在先讓我們簡單讀幾筆數據出來看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "yiAC9BkH0Z0k",
    "outputId": "fe7b9ba7-2604-404e-d4e3-eb2aa333b308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Making Do With More', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'If the Putins, Erdo\\xc4\\x9fans, and Orb\\xc3\\xa1ns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xa6\\x82\\xe6\\x9e\\x9c\\xe6\\x99\\xae\\xe4\\xba\\xac\\xe3\\x80\\x81\\xe5\\x9f\\x83\\xe5\\xb0\\x94\\xe5\\xa4\\x9a\\xe5\\xae\\x89\\xe5\\x92\\x8c\\xe6\\xac\\xa7\\xe5\\xb0\\x94\\xe7\\x8f\\xad\\xe5\\xb8\\x8c\\xe6\\x9c\\x9b\\xe7\\xbb\\xa7\\xe7\\xbb\\xad\\xe4\\xba\\xab\\xe6\\x9c\\x89\\xe5\\xbc\\x80\\xe6\\x94\\xbe\\xe5\\x9b\\xbd\\xe9\\x99\\x85\\xe4\\xbd\\x93\\xe7\\xb3\\xbb\\xe6\\x8f\\x90\\xe4\\xbe\\x9b\\xe7\\x9a\\x84\\xe7\\xbb\\x8f\\xe6\\xb5\\x8e\\xe5\\x88\\xa9\\xe7\\x9b\\x8a\\xef\\xbc\\x8c\\xe5\\xb0\\xb1\\xe4\\xb8\\x8d\\xe8\\x83\\xbd\\xe7\\xae\\x80\\xe5\\x8d\\x95\\xe5\\x9c\\xb0\\xe5\\x88\\xb6\\xe5\\xae\\x9a\\xe8\\x87\\xaa\\xe5\\xb7\\xb1\\xe7\\x9a\\x84\\xe8\\xa7\\x84\\xe5\\x88\\x99\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\x8f\\xaa\\xe6\\x9c\\x89\\xe5\\x9c\\xa8\\xe5\\x8f\\x91\\xe7\\x94\\x9f\\xe6\\xb7\\xb1\\xe5\\xba\\xa6\\xe8\\x90\\xa7\\xe6\\x9d\\xa1\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe5\\x8f\\x8d\\xe5\\xb8\\xb8\\xe4\\xba\\x8b\\xe4\\xbb\\xb6\\xe6\\x97\\xb6\\xef\\xbc\\x8c\\xe8\\xbf\\x99\\xe4\\xb8\\x80\\xe4\\xb8\\x8a\\xe9\\x99\\x90\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\x81\\x9a\\xe5\\x87\\xba\\xe8\\xb0\\x83\\xe6\\x95\\xb4\\xef\\xbc\\x8c\\xe4\\xbb\\xa5\\xe4\\xbe\\xbf\\xe8\\xae\\xa9\\xe5\\x8f\\x8d\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x94\\xbf\\xe7\\xad\\x96\\xe5\\xae\\x9e\\xe6\\x96\\xbd\\xe8\\xb6\\xb3\\xe5\\xa4\\x9f\\xe7\\x9a\\x84\\xe9\\x95\\xbf\\xe5\\xba\\xa6\\xef\\xbc\\x8c\\xe4\\xbd\\xbf\\xe4\\xba\\xba\\xe4\\xbb\\xac\\xe4\\xb8\\x80\\xe8\\x87\\xb4\\xe8\\xae\\xa4\\xe4\\xb8\\xba\\xe5\\xa2\\x9e\\xe5\\x8a\\xa0\\xe7\\x9a\\x84\\xe8\\xb5\\xa4\\xe5\\xad\\x97\\xe6\\x98\\xaf\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xef\\xbc\\x8c\\xe8\\x80\\x8c\\xe4\\xb8\\x8d\\xe6\\x98\\xaf\\xe7\\xbb\\x93\\xe6\\x9e\\x84\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "  print(en)\n",
    "  print(zh)\n",
    "  print('-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKWx_mb-5_CK"
   },
   "source": [
    "跟預期一樣，每一個例子（每一次的 `take`）都包含了 2 個以 unicode 呈現的 `tf.Tensor`。它們有一樣的語義，只是一個是英文，一個是中文。\n",
    "\n",
    "讓我們將這些 Tensors 實際儲存的字串利用 `numpy()` 取出並解碼看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "9wZBtwzarsWo",
    "outputId": "f01e1ee8-6c04-4457-a934-c4f199591dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Do With More\n",
      "多劳应多得\n",
      "----------\n",
      "If the Putins, Erdoğans, and Orbáns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.\n",
      "如果普京、埃尔多安和欧尔班希望继续享有开放国际体系提供的经济利益，就不能简单地制定自己的规则。\n",
      "----------\n",
      "This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.\n",
      "只有在发生深度萧条或其他反常事件时，这一上限才能做出调整，以便让反周期政策实施足够的长度，使人们一致认为增加的赤字是周期性的，而不是结构性的。\n",
      "----------\n",
      "Fascist and communist regimes of the past, which followed a similar instrumentalist approach to democracy, come to mind here.\n",
      "在此我们想起了过去的法西斯主义和共产主义。 它们都相似地将民主作为实现其目的的工具。\n",
      "----------\n",
      "This phase culminated with the collapse of communism in 1989, but the chance to overcome the Continent’s historical divisions now required a redefinition of the European project.\n",
      "这种状态随着1989年共产主义崩溃而达至巅峰，但是克服欧洲大陆历史性分裂的机遇现在需要重新定义欧洲计划。\n",
      "----------\n",
      "The eurozone’s collapse (and, for all practical purposes, that of the EU itself) forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "----------\n",
      "With energy and enthusiasm, Burden turned that operation into a thriving health (not health-care) agency that covers three cities and about 300,000 people on the western edge of Los Angeles.\n",
      "在能量与激情的推动下，波顿将BCHD打造成了欣欣向荣的健康（而非医疗）机构，其服务范围覆盖了洛杉矶西侧三座城市的30万人。\n",
      "----------\n",
      "The result could be a world of fragmented blocs – an outcome that would undermine not only global prosperity, but also cooperation on shared challenges.\n",
      "其结果可能是一个四分五裂的世界 — — 这一结果不但会破坏全球繁荣，也会破坏面对共同挑战的合作。\n",
      "----------\n",
      "Among the questions being asked by NGOs, the UN, and national donors is how to prevent the recurrence of past mistakes.\n",
      "现在NGO们、联合国和捐助国们问得最多的一个问题就是如何避免再犯过去的错误。\n",
      "----------\n",
      "Managing the rise of NCDs will require long-term thinking, and government leaders will have to make investments that might pay off only after they are no longer in office.\n",
      "管理NCD的增加需要长期思维，政府领导人必须进行要在他们离任多年后才能收回成本的投资。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "  en = en_t.numpy().decode(\"utf-8\")\n",
    "  zh = zh_t.numpy().decode(\"utf-8\")\n",
    "  \n",
    "  print(en)\n",
    "  print(zh)\n",
    "  print('-' * 10)\n",
    "  \n",
    "  # 之後用來簡單評估模型的訓練情況\n",
    "  sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wU9dZ_OV-q70"
   },
   "source": [
    "想像一下沒有對應的中文，要閱讀這些英文得花多少時間。你可以試著消化其中幾句中文與其對應的英文句子，並比較一下所需要的時間差異。\n",
    "\n",
    "雖然只是隨意列出的 10 個中英句子，你應該跟我一樣也能感受到機器翻譯研究的重要以及其能帶給我們的價值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3LnQcuJA1Cl"
   },
   "source": [
    "### 建立中文與英文字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oY0mYN9G5_f"
   },
   "source": [
    "就跟大多數 NLP 專案相同，有了原始的中英句子以後我們得分別為其建立字典來將每個詞彙轉成索引（Index）。`tfds.features.text` 底下的 `SubwordTextEncoder` 提供非常方便的 API 讓我們掃過整個訓練資料集並建立字典。\n",
    "\n",
    "首先為英文語料建立字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "SfGERJsxSHnn",
    "outputId": "943b1481-78db-41a4-edd5-2b0d6b3714b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入已建立的字典： /content/gdrive/My Drive/nmt/en_vocab\n",
      "字典大小：8135\n",
      "前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'that_', 'is_']\n",
      "\n",
      "CPU times: user 41 ms, sys: 7.43 ms, total: 48.4 ms\n",
      "Wall time: 391 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "  subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "  print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "  print(\"沒有已建立的字典，從頭建立。\")\n",
    "  subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "  \n",
    "  # 將字典檔案存下以方便下次 warmstart\n",
    "  subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "\n",
    "print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aok4q-9zNddZ"
   },
   "source": [
    "如果你的語料庫（corpus） 不小，要掃過整個資料集並建立一個字典得花不少時間。因此實務上我們會先使用 `load_from_file` 函式嘗試讀取之前已經建好的字典檔案，失敗才 `build_from_corpus`。\n",
    "\n",
    "這招很基本，但在你需要重複處理巨大語料庫時非常重要。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oh1yEXa-fLRY"
   },
   "source": [
    "`subword_encoder_en` 則是利用 [GNMT 當初推出的 wordpieces](https://arxiv.org/pdf/1609.08144.pdf) 來進行斷詞，而簡單來說其產生的子詞（subword）介於這兩者之間：\n",
    "- 用英文字母分隔的斷詞（character-delimited）\n",
    "- 用空白分隔的斷詞（word-delimited）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "claz-vigtRJp"
   },
   "source": [
    "在掃過所有英文句子以後，`subword_encoder_en` 建立一個有 8135 個子詞的字典。我們可以用該字典來幫我們將一個英文句子轉成對應的索引序列（index sequence）：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "asUAh2XZNdOn",
    "outputId": "a90777a9-4afc-4e41-8ded-2006e90c0960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2700, 7911, 10, 2942, 7457, 1163, 7925]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7uGPkUPZwK6u"
   },
   "source": [
    "這樣的索引序列你應該已經見怪不怪了。我們在[以前的 NLP 入門文章](https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html)也使用 `tf.keras` 裡頭的 `Tokenizer` 做過類似的事情。\n",
    "\n",
    "接著讓我們將這些索引還原，看看它們的長相："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "MkuICgt-D7E8",
    "outputId": "be1bb2e1-3ea7-49b3-8437-7ea48943e34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      " 2700     Taiwan\n",
      " 7911      \n",
      "   10     is \n",
      " 2942     bea\n",
      " 7457     uti\n",
      " 1163     ful\n",
      " 7925     .\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "  subword = subword_encoder_en.decode([idx])\n",
    "  print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4C_1ZMGZxHtC"
   },
   "source": [
    "當 subword tokenizer 遇到從沒出現過在字典裡的詞彙，會將該詞拆成多個子詞（subwords）。比方說上面句中的 `beautiful` 就被拆成 `bea uti ful`。這也是為何這種斷詞方法比較不怕沒有出現過在字典裡的字（out-of-vocabulary words）。\n",
    "\n",
    "\n",
    "另外別在意我為了對齊寫的 `print` 語法。重點是我們可以用 `subword_encoder_en` 的 `decode` 函式再度將索引數字轉回其對應的子詞。編碼與解碼是 2 個完全可逆（invertable）的操作：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nrQ13zYvNdDI",
    "outputId": "7ea21f95-0f8c-4a7a-82fd-34b1ff7d8df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taiwan is beautiful.', 'Taiwan is beautiful.')\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UawduUikSFTw"
   },
   "source": [
    "酷！接著讓我們如法炮製，為中文也建立一個字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "KVBg5Q8tBk5z",
    "outputId": "e1a060ce-e67f-48a6-8337-d9475893a584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入已建立的字典： /content/gdrive/My Drive/nmt/zh_vocab\n",
      "字典大小：4201\n",
      "前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n",
      "\n",
      "CPU times: user 27.6 ms, sys: 121 µs, total: 27.7 ms\n",
      "Wall time: 337 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "  subword_encoder_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "  print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "  print(\"沒有已建立的字典，從頭建立。\")\n",
    "  subword_encoder_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "  \n",
    "  # 將字典檔案存下以方便下次 warmstart \n",
    "  subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1thwOrQ0cFS"
   },
   "source": [
    "在使用 `build_from_corpus` 函式掃過整個中文資料集時，我們將 `max_subword_length` 參數設置為 1。這樣可以讓每個漢字都會被視為字典裡頭的一個單位。畢竟跟英文的 abc 字母不同，一個漢字代表的意思可多得多了。而且如果使用 n-gram 的話可能的詞彙組合太多，在小數據集的情況非常容易遇到不存在字典裡頭的字。\n",
    "\n",
    "另外所有漢字也就大約 4000 ~ 5000 個可能，作為一個分類問題（classification problem）還是可以接受的。\n",
    "\n",
    "讓我們挑個中文句子來測試看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "puTz2oWDAMGn",
    "outputId": "9f2e7e49-f6ff-4acf-d0cc-03ee4f48de0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多劳应多得\n",
      "[48, 557, 116, 48, 81]\n"
     ]
    }
   ],
   "source": [
    "sample_string = sample_examples[0][1]\n",
    "indices = subword_encoder_zh.encode(sample_string)\n",
    "print(sample_string)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxEfpQdE4wsT"
   },
   "source": [
    "好的，我們把中英文斷詞及字典的部分都搞定了。現在給定一個例子（example，在這邊以及後文指的都是一組包含同語義的中英平行句子），我們都能將其轉換成對應的索引序列了：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "t0tcagOW9cyE",
    "outputId": "41084e6b-b5d0-4118-9ddb-304809f21c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[英中原文]（轉換前）\n",
      "The eurozone’s collapse forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "\n",
      "--------------------\n",
      "\n",
      "[英中序列]（轉換後）\n",
      "[17, 965, 11, 6, 1707, 676, 8, 211, 2712, 6683, 249, 3, 85, 1447, 7925]\n",
      "[45, 206, 171, 1, 847, 197, 236, 604, 45, 90, 17, 130, 102, 36, 7, 284, 80, 18, 212, 265, 3]\n"
     ]
    }
   ],
   "source": [
    "en = \"The eurozone’s collapse forces a major realignment of European politics.\"\n",
    "zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\"\n",
    "\n",
    "# 將文字轉成為 subword indices\n",
    "en_indices = subword_encoder_en.encode(en)\n",
    "zh_indices = subword_encoder_zh.encode(zh)\n",
    "\n",
    "print(\"[英中原文]（轉換前）\")\n",
    "print(en)\n",
    "print(zh)\n",
    "print()\n",
    "print('-' * 20)\n",
    "print()\n",
    "print(\"[英中序列]（轉換後）\")\n",
    "print(en_indices)\n",
    "print(zh_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-OISiij81qa"
   },
   "source": [
    "\n",
    "接著讓我們針對這些索引序列（index sequence）做一些前處理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT5Cf6nDBQ27"
   },
   "source": [
    "### 前處理數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RggNMpMq5JEe"
   },
   "source": [
    "在處理序列數據時我們時常會在一個序列的前後各加入一個特殊的 token，以標記該序列的開始與完結，而它們常有許多不同的稱呼：\n",
    "\n",
    "\n",
    "- 開始 token、**B**egin **o**f **S**entence、BOS、`<start>`\n",
    "- 結束 token、**E**nd **o**f **S**entence、EOS、`<end>`\n",
    "\n",
    "這邊我們定義了一個將被 `tf.data.Dataset` 使用的 `encode` 函式，它的輸入是一筆包含 2 個 `string` Tensors 的例子，輸出則是 2 個包含 BOS / EOS 的索引序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZwnPr4R055s"
   },
   "outputs": [],
   "source": [
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHNwJniYMm2V"
   },
   "source": [
    "因為 `tf.data.Dataset` 裡頭都是在操作 Tensors（而非 Python 字串），所以這個 `encode` 函式預期的輸入也是 TensorFlow 裡的 [Eager Tensors](https://www.tensorflow.org/guide/eager)。但只要我們使用 `numpy()` 將 Tensor 裡的實際字串取出以後，做的事情就跟上一節完全相同。\n",
    "\n",
    "\n",
    "讓我們從訓練集裡隨意取一組中英的 Tensors 來看看這個函式的實際輸出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "i17hbcUsMtGY",
    "outputId": "f8b6cd3f-e62b-4bb8-b403-f4d889715b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 BOS 的 index： 8135\n",
      "英文 EOS 的 index： 8136\n",
      "中文 BOS 的 index： 4201\n",
      "中文 EOS 的 index： 4202\n",
      "\n",
      "輸入為 2 個 Tensors：\n",
      "(<tf.Tensor: id=306, shape=(), dtype=string, numpy=b'Making Do With More'>,\n",
      " <tf.Tensor: id=307, shape=(), dtype=string, numpy=b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97'>)\n",
      "---------------\n",
      "輸出為 2 個索引序列：\n",
      "([8135, 4682, 19, 717, 7911, 298, 2701, 7980, 8136],\n",
      " [4201, 48, 557, 116, 48, 81, 4202])\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('英文 BOS 的 index：', subword_encoder_en.vocab_size)\n",
    "print('英文 EOS 的 index：', subword_encoder_en.vocab_size + 1)\n",
    "print('中文 BOS 的 index：', subword_encoder_zh.vocab_size)\n",
    "print('中文 EOS 的 index：', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\n輸入為 2 個 Tensors：')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('輸出為 2 個索引序列：')\n",
    "pprint((en_indices, zh_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPsYgetQM1UO"
   },
   "source": [
    "你可以看到不管是英文還是中文的索引序列，前面都加了一個代表 BOS 的索引（分別為 8135 與 4201），最後一個索引則代表 EOS（分別為 8136 與 4202）\n",
    "\n",
    "\n",
    "但如果我們將 `encode` 函式直接套用到整個訓練資料集時會產生以下的錯誤訊息：\n",
    "\n",
    "```python\n",
    "train_dataset = train_examples.map(encode)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmQ3ATcLWbq6"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/tf-dataset-map-error.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjAnpf5dYUWe"
   },
   "source": [
    "這是因為目前 `tf.data.Dataset.map` 函式裡頭的計算是在[計算圖模式（Graph mode）](https://www.tensorflow.org/guide/graphs)下執行，所以裡頭的 Tensors 並不會有 [Eager Execution](https://www.tensorflow.org/alpha/guide/eager) 下才有的 `numpy` 屬性。\n",
    "\n",
    "解法是使用 [tf.py_function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/py_function) 將我們剛剛定義的 `encode` 函式包成一個以 eager 模式執行的 TensorFlow Op："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15lzIAQhUXKW"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# TODO: release 之前把下面的 code 多執行幾次去掉 logging messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "pk90IehacoYY",
    "outputId": "bc8175ce-b4ca-4537-abcb-4b9a58d568de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 23:46:10.571188 140648854296320 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0616 23:46:10.573221 140648854296320 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([8135 4682   19  717 7911  298 2701 7980 8136], shape=(9,), dtype=int64)\n",
      "tf.Tensor([4201   48  557  116   48   81 4202], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdrLAUBpcoPG"
   },
   "source": [
    "有點 tricky 但任務完成！注意在套用 `map` 函式以後，`tmp_dataset` 的輸出已經是兩個索引序列，而非原文字串。\n",
    "\n",
    "為了讓 Transformer  快點完成訓練，讓我們將長度超過 40 個 tokens 的序列都去掉吧！我們在底下定義了一個布林（boolean）函式，其輸入為一個包含兩個英中序列 `en, zh` 的例子，並在只有這 2 個序列的長度都小於 40 的時候回傳真值（True）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_E-55L5VM7J"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_60yU_-7VLtR"
   },
   "source": [
    "簡單檢查是否有序列超過我們指定的長度，順便計算過濾掉過長序列後剩餘的訓練集筆數：\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "# 因為我們數據量小可以這樣 count\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in train_dataset:\n",
    "  cond1 = len(en_indices) <= MAX_LENGTH\n",
    "  cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "  assert cond1 and cond2\n",
    "  num_examples += 1\n",
    "\n",
    "print(f\"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\")\n",
    "print(f\"訓練資料集裡總共有 {num_examples} 筆數據\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eLW9ZfC_TURU",
    "outputId": "b00a8b30-4de6-4c6d-f216-3686c5ea11db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料集裡總共有 29914 筆數據\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "print(\"訓練資料集裡總共有 29914 筆數據\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-5M0UZUvyic"
   },
   "source": [
    "過濾掉較長句子後還有接近 3 萬筆的訓練例子，看來不用擔心數據太少。\n",
    "\n",
    "最後值得注意的是每個例子裡的索引序列長度不一，這在建立 batch 時可能會發生問題。不過別擔心，輪到 `padded_batch` 函式出場了：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yup9r8fyUKrl"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# TODO: release 之前把下面的 code 多執行幾次去掉 logging messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "faQmgjvHwKPu",
    "outputId": "588c8bb0-0c13-4d42-a53d-1b3c8bbed1a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 23:46:10.753194 140648845903616 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0616 23:46:10.760091 140648845903616 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0616 23:46:10.768630 140648845903616 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8135 4682   19 ...    0    0    0]\n",
      " [8135   17  965 ... 8136    0    0]\n",
      " [8135 6602    2 ...    0    0    0]\n",
      " ...\n",
      " [8135 1097  270 ...    0    0    0]\n",
      " [8135 1713   70 ...    0    0    0]\n",
      " [8135 2731 4553 ...    0    0    0]], shape=(64, 34), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4201   48  557 ...    0    0    0]\n",
      " [4201   45  206 ...    0    0    0]\n",
      " [4201   58    5 ...  683    3 4202]\n",
      " ...\n",
      " [4201   29  120 ...    0    0    0]\n",
      " [4201  297  161 ...    0    0    0]\n",
      " [4201  279  149 ... 4202    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsfltRM9wh85"
   },
   "source": [
    "`padded_batch` 函式能幫我們將每個 batch 裡頭的序列都補 0 到跟當下 batch 裡頭最長的序列一樣長。\n",
    "\n",
    "比方說英文 batch 裡最長的序列為 34；而中文 batch 裡最長的序列則長達 40 個 tokens，剛好是我們前面設定過的序列長度上限。\n",
    "\n",
    "好啦，現在讓我們從頭建立訓練集與驗證集，順便看看這些中英句子是如何被轉換成它們的最終形態的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2J0oW94lpO5M"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mah1cS-P70Iz"
   },
   "source": [
    "建構訓練資料集時我們還添加了些沒提過的函式。它們的用途大都是用來提高輸入效率，並不會影響到輸出格式。如果你想深入了解這些函式的運作方式，可以參考 [tf.data 的官方教學](https://www.tensorflow.org/guide/performance/datasets?hl=zh_cn)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TgcyQ_yBah9"
   },
   "source": [
    "現在讓我們看看最後建立出來的資料集長什麼樣子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "_fXvfYVfQr2n",
    "outputId": "e079fa80-ce4d-4d96-dadf-27e424e2346c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8135  222    1 ...    0    0    0]\n",
      " [8135 3812  162 ...    0    0    0]\n",
      " [8135 6267  838 ...    0    0    0]\n",
      " ...\n",
      " [8135   17 1042 ...    0    0    0]\n",
      " [8135 7877 1165 ...    0    0    0]\n",
      " [8135 6414 7911 ...    0    0    0]], shape=(128, 40), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4201  109   54 ...    3 4202    0]\n",
      " [4201   30    4 ...    0    0    0]\n",
      " [4201  402    4 ...    0    0    0]\n",
      " ...\n",
      " [4201  626  515 ...    0    0    0]\n",
      " [4201   49  249 ...    0    0    0]\n",
      " [4201  905  209 ...    0    0    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cM_qQ2wIjFlJ"
   },
   "source": [
    "嘿！我們建立了一個可供訓練的輸入管道（Input pipeline）！\n",
    "\n",
    "你會發現訓練集：\n",
    "- 一次回傳大小為 128 的 2 個 batch，分別包含 128 個英文、中文的索引序列\n",
    "- 序列開頭皆為 BOS，英文的 BOS 索引是 8135；中文的 BOS 索引則為 4201\n",
    "- 兩語言 batch 裡的序列都被「拉長」到我們先前定義的最長序列長度：40\n",
    "\n",
    "驗證集也是相同的輸出形式。\n",
    "\n",
    "現在你應該可以想像我們在每個訓練步驟會拿出來的數據長什麼樣子了：2 個 shape 為 (batch_size, seq_len) 的 Tensors，而裡頭的每一個索引數字都代表著一個中 / 英文子詞（或是 BOS / EOS）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKjvzD6Ub_XZ"
   },
   "source": [
    "在這一節我們建立了一個通用資料集。「通用」代表不限於 Transformer，你也能用[一般搭配注意力機制的 Seq2Seq 模型](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention)來處理這個資料集並做中英翻譯。\n",
    "\n",
    "但從下節開始讓我們把這個數據集先擺一邊，將注意力全部放到 Transformer 身上並逐一實作其架構裡頭的各個元件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0C3EXMh9a7d"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# @pysnooper.snoop()\n",
    "def plot_2d(value, ax=None, group=None, mask=None, matrix_id=0, mat_as_group=False, \n",
    "            group_id=None, linewidths=0, is_string=False, fmt=\"d\", square=False):\n",
    "  \n",
    "  if hasattr(value, \"numpy\"):\n",
    "    value = value.numpy()\n",
    "  if group is not None and hasattr(group, \"numpy\"):\n",
    "    group = group.numpy()\n",
    "  if mask is not None and hasattr(mask, \"numpy\"):\n",
    "    mask = tf.squeeze(mask)\n",
    "    mask = tf.ones_like(value) * mask\n",
    "    mask = mask.numpy()\n",
    "    \n",
    "\n",
    "  cmaps = ['PuOr', 'tab20b', 'RdBu']\n",
    "  group_id = int(group[0][0])\n",
    "  cmap = cmaps[group_id]\n",
    "  \n",
    "  if is_string:\n",
    "    fmt = ''\n",
    "  \n",
    "  sns.heatmap(group, \n",
    "              fmt=fmt,\n",
    "#               cmap=cmap,\n",
    "              cmap=cmap,\n",
    "              annot=value, \n",
    "              cbar=False, \n",
    "              xticklabels=False, \n",
    "              yticklabels=False, \n",
    "              square=square,\n",
    "              mask=mask,\n",
    "              linewidths=linewidths,\n",
    "              ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUYX2cYuqOau"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# 設定 global label size \n",
    "# ref: https://stackoverflow.com/questions/12444716/how-do-i-set-the-figure-title-and-axes-labels-font-size-in-matplotlib\n",
    "plt.rcParams.update({'axes.labelsize': '20'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGj8v2Kc9b-Q"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# @pysnooper.snoop()\n",
    "def plot(value, group=None, group_dim=0, mask=None, labels=[], square=False, shape_desc='', width_prec=None, bottom_prec=None, \n",
    "         is_string=False, name='', fmt=\"d\", single_plot_size=4, h_dist_ratio=0.7, w_dist_ratio=0.7, linewidths=None):\n",
    "\n",
    "  shape = value.shape\n",
    "  num_groups = shape[0]\n",
    "  \n",
    "  if hasattr(value, 'numpy'):\n",
    "    if value.dtype in [tf.int32, tf.int64]:\n",
    "      value = tf.cast(value, tf.int32)\n",
    "    elif value.dtype in [tf.float32, tf.float64] and fmt == \"d\":\n",
    "      fmt = \".2g\"\n",
    "    \n",
    "    value = value.numpy()\n",
    "\n",
    "  if hasattr(value, 'ndim'):\n",
    "    ndim = value.ndim\n",
    "  else:\n",
    "    ndim = len(value)\n",
    "    \n",
    "    \n",
    "  if ndim == 2:\n",
    "    value = value[np.newaxis, np.newaxis, :, :]\n",
    "  if ndim == 3:\n",
    "    value = value[np.newaxis, :, :, :]\n",
    "  if ndim == 4:\n",
    "    pass\n",
    "  \n",
    "  # decide how to group sub-tensors smartly\n",
    "  if not group_dim:\n",
    "    group_dim = ndim - 1\n",
    "    \n",
    "  # generate group identifier tensor for differentiating between\n",
    "  # different bactch / sentence\n",
    "  if group is None:\n",
    "    group_ids = tf.range(num_groups, dtype=tf.int64).numpy()\n",
    "    if group_dim == 1:\n",
    "      group = group_ids[:, tf.newaxis]\n",
    "    elif group_dim == 2:\n",
    "      group = group_ids[:, tf.newaxis, tf.newaxis]\n",
    "    elif group_dim == 3:\n",
    "      group = group_ids[:, tf.newaxis, tf.newaxis, tf.newaxis]\n",
    "\n",
    "    # broadcast to all groups    \n",
    "    group = tf.ones(shape=value.shape) * group\n",
    "\n",
    "  d0, d1, d2, d3 = value.shape\n",
    "\n",
    "  # set figure size based on tensor dimensions\n",
    "  fig_width = (d3 * 1.0 / 4) * d0 * single_plot_size\n",
    "  fig_height = (d2 * 1.0 / 4) * single_plot_size\n",
    "  figsize = (fig_width, fig_height)\n",
    "  \n",
    "  if width_prec is None:\n",
    "    width_prec = 1.0 / d0\n",
    "  \n",
    "  if bottom_prec is None:\n",
    "    bottom_prec = 1.0 / d1\n",
    "\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  fig_title = f'name: {name}, shape: {shape}' if name else f'shape: {shape}'\n",
    "  \n",
    "  if shape_desc:\n",
    "    fig_title = fig_title + ' = ' + shape_desc\n",
    "    \n",
    "  for e0 in range(d0):\n",
    "\n",
    "    # plot 2d array in reverse order since the earlier plot will be covered\n",
    "    for e1 in reversed(range(d1)):\n",
    "      annot = value[e0, e1]\n",
    "\n",
    "      # select corresponding matplotlib axis      \n",
    "      cur_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3) * w_dist_ratio, \n",
    "                             e1 / d2 * h_dist_ratio, \n",
    "                             width_prec, \n",
    "                             bottom_prec]) \n",
    "\n",
    "      matrix_id = e0 + e1 * 2\n",
    "      \n",
    "      if mask is not None:\n",
    "        if ndim == 2:\n",
    "          mask_idx = e0\n",
    "        elif ndim == 3:\n",
    "          mask_idx = e1\n",
    "        elif ndim ==4:\n",
    "          mask_idx = e0\n",
    "          \n",
    "        # mimic broadcasting\n",
    "        if mask.shape[0] == 1:\n",
    "          mask_idx = 0\n",
    "          \n",
    "        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, \n",
    "                is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n",
    "      else:\n",
    "        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, is_string=is_string, fmt=fmt, square=square)\n",
    "      \n",
    "      # minic shadowing for each 2d matrix\n",
    "      width_delta_prec = 0.0005\n",
    "      height_delta_prec = width_delta_prec * d2 / d3\n",
    "      \n",
    "      for k in range(1, 3):\n",
    "        shadow_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3)  * w_dist_ratio - width_delta_prec * k, \n",
    "                                  e1 / d2 * h_dist_ratio - height_delta_prec * k, \n",
    "                                  width_prec, \n",
    "                                  bottom_prec])  \n",
    "        \n",
    "        if k == 2:\n",
    "          linewidths = 1\n",
    "        else:\n",
    "          linewidths = 0\n",
    "          \n",
    "        if mask is not None:\n",
    "          if ndim == 2:\n",
    "            mask_idx = e0\n",
    "          elif ndim == 3:\n",
    "            mask_idx = e1\n",
    "          elif ndim ==4:\n",
    "            mask_idx = e0\n",
    "            \n",
    "          # mimic broadcasting\n",
    "          if mask.shape[0] == 1:\n",
    "            mask_idx = 0  \n",
    "            \n",
    "            \n",
    "          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n",
    "                  linewidths=linewidths, is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n",
    "        else:\n",
    "          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n",
    "                  linewidths=linewidths, is_string=is_string, fmt=fmt, square=square)\n",
    "\n",
    "      if e0 == 0 and e1 == 0:\n",
    "        ax1 = cur_ax\n",
    "        \n",
    "        if labels:\n",
    "            plt.ylabel(labels[-2])\n",
    "            plt.xlabel(labels[-1] + '\\n' + fig_title)\n",
    "        else:\n",
    "          plt.xlabel(fig_title)\n",
    "\n",
    "        # 4D 中的 axis1 說明 label\n",
    "#           if len(labels) >= 3:\n",
    "#             plt.text(d3 + 2, 1 + 0.5, labels[-3],\n",
    "#                      rotation=0, rotation_mode='anchor')\n",
    "\n",
    "      if e1 == d0 - 1:\n",
    "        ax2 = cur_ax\n",
    "        \n",
    "\n",
    "#       transFigure = fig.transFigure.inverted()\n",
    "#       coord1 = transFigure.transform(ax1.transData.transform([d3 + 2 + 0.5, 0]))\n",
    "#       coord2 = transFigure.transform(ax2.transData.transform([d3 + 0.5, d2]))\n",
    "\n",
    "\n",
    "#       line = mpl.lines.Line2D((coord1[0],coord2[0]),(coord1[1],coord2[1]), \n",
    "#                               transform=fig.transFigure, \n",
    "#                               linestyle='--',\n",
    "#                               color='black')\n",
    "#       fig.lines.append(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BI2tE34R2uHY"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "def decode(batch, lang=\"en\"):\n",
    "  if lang == 'en':\n",
    "    tokenizer = subword_encoder_en\n",
    "  else:\n",
    "    tokenizer = subword_encoder_zh\n",
    "\n",
    "  result = []\n",
    "  for e0 in range(batch.shape[0]):\n",
    "    idx_sequence = batch[e0].numpy()\n",
    "    sentence = []\n",
    "    for idx in idx_sequence:\n",
    "      if idx == 0:\n",
    "        token = '<pad>'\n",
    "      elif idx == tokenizer.vocab_size:\n",
    "        token = '<start>'\n",
    "      elif idx == tokenizer.vocab_size + 1:\n",
    "        token = '<end>'\n",
    "      else:\n",
    "        token = tokenizer.decode([idx])\n",
    "      sentence.append(token)\n",
    "    result.append(sentence)\n",
    "    \n",
    "  return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbxlP36hvzfP"
   },
   "source": [
    "## 理解 Transformer 之旅：跟著多維向量去冒險"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZOIoN4WZ0Cd"
   },
   "source": [
    "在實作 Transformer 及注意力機制這種高度平行運算的模型時，你將需要一點「空間想像力」，能夠想像最高高達 4 維的向量是怎麼在 Transformer 的各個元件被處理與轉換的。\n",
    "\n",
    "如果你跟我一樣腦袋並不是那麼靈光的話，這可不是一件簡單的事情。不過別擔心，從這節開始我會把 Transfomer （主要針對注意力機制）裡頭的矩陣運算過程視覺化（visualize）出來，讓你在這個多維空間裡頭也能悠遊自在。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6xgoxJMBRep"
   },
   "source": [
    "!image\n",
    "- transformer/the-matrix-world.jpg\n",
    "- Welcome to matrix, 準備進入多維空間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1w0DolBmzPi"
   },
   "source": [
    "就好像一般你在寫程式時會追蹤某些變數在函式裡頭的變化，一個直觀理解 Transformer 的方法是將幾個句子丟入其中，並觀察 Transformer 對它們做了些什麼轉換。\n",
    "\n",
    "首先讓我們建立兩個要拿來持續追蹤的中英平行句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2xYOgr1Jv_Cq",
    "outputId": "4852da81-650e-4d9d-e78c-4b3d5cde3587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlIZ_Octq8UQ"
   },
   "source": [
    "接著利用[之前建立資料集的方法](#建立輸入管道)將這 2 組中英句子做些前處理並以 Tensor 的方式讀出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "wEbjVhqwwMKy",
    "outputId": "41c78bcb-6d80-43b6-ee14-88d2257e5b44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HS4BbW1vNKV"
   },
   "source": [
    "上節建立的數據集屍骨未寒，你應該還記得 `inp`  shape 裡頭第一個維度的 `2` 代表著這個 batch 有 2 個句子，而第二維度的 `8` 則代表著句子的長度（單位：子詞）；`tar` 則為中文子詞序列（subword sequence），不過因為中文我們以漢字為單位作斷詞，長度一般會比對應的英文句子來的長（shape 中的 `10`）。\n",
    "\n",
    "2 維矩陣還很容易想像，但我擔心等到你進入 3 維空間後就會想放棄人生了。所以還是先讓我們用人類比較容易理解的方式來呈現這些數據。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0EM9KwGvFzq"
   },
   "source": [
    "### 視覺化原始句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXhl-nqCwMgS"
   },
   "source": [
    "\n",
    "\n",
    "如果我們把這 2 個 batch 用你比較熟悉的方式呈現的話會長這樣："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwcHhtjHxvJ0"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/inp_tar.mp4\n",
    "- images/transformer/inp_tar.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rj7NlRicujQH"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# labels = ['句子 sentence', '子詞 subword']\n",
    "# plot(inp, name='inp', labels=labels, shape_desc='(batch_size, inp_seq_len)')\n",
    "# plot(tar, name='tar', labels=labels, shape_desc='(batch_size, tar_seq_len)')\n",
    "\n",
    "# # string\n",
    "# plot(decode(inp), name='inp', is_string=True, \n",
    "#      labels=labels, shape_desc='(batch_size, inp_seq_len)')\n",
    "# plot(decode(tar, lang='zh'), name='tar', is_string=True, \n",
    "#      labels=labels, shape_desc='(batch_size, tar_seq_len)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KY0mw9VQujBw"
   },
   "source": [
    "這樣清楚多了不是嗎？現在點擊播放鍵，將索引序列還原成原始的子詞序列。\n",
    "\n",
    "你可以清楚地看到每個**原始**句子前後都有 `<start>` 與 `<end>`。而為了讓同個 batch 裡頭的序列長度相同，我們在較短的序列後面也補上足夠的 0，代表著 `<pad>`。\n",
    "\n",
    "\n",
    "這個視覺化非常簡單，但十分強大。我現在要你記住一些本文會使用的慣例：\n",
    "- 不管[張量（Tensor）](https://zh.wikipedia.org/wiki/%E5%BC%B5%E9%87%8F)變幾維，其第一個維度 `shape[0]` 永遠代表 `batch_size`，也就代表著句子的數目\n",
    "- 不同句子我們用不同顏色表示，方便你之後對照這些句子在轉換前後的差異\n",
    "- x 軸（橫軸）代表張量的最後一個維度 `shape[-1]`，以上例來說分別為 `8` 和 `10`\n",
    "- x, y 軸上的標籤分別代表倒數兩個維度 `shape[-2]` 及 `shape[-1]` 其所代表的物理含義，如圖中的**句子**與**子詞**\n",
    "- 圖中張量的 `name` 會對應到程式碼裡頭定義的變數名稱，方便你對照並理解實作邏輯。我也會秀出張量的 shape  幫助你想像該向量在多維空間的長相。一個簡單的例子是：`(batch_size, tar_seq_len)` \n",
    "\n",
    "這些準則與資訊現在看似多餘，但我保證你很快就會需要它們。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1iZagnMI_Bin"
   },
   "source": [
    "### 視覺化 3 維詞嵌入張量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJFsC-YU_BBU"
   },
   "source": [
    "\n",
    "在將索引序列丟入神經網路之前，我們一般會做[詞嵌入（word embedding）](https://zh.wikipedia.org/wiki/%E8%AF%8D%E5%B5%8C%E5%85%A5)，將一個維度為字典大小的高維離散空間「嵌入」到低維的連續空間裡頭。\n",
    "\n",
    "讓我們為英文與中文分別建立一個詞嵌入層並實際對 `inp` 及 `tar` 做轉換："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STxJSe07H3w1"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# 讀取之前建立的 emb layer 來保證每次的 emb_inp, emb_tar 結果都一樣\n",
    "d_model = 4\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "emb_en_model_path = os.path.join(output_dir, \"demo_emb_en_model.h5\")\n",
    "emb_zh_model_path = os.path.join(output_dir, \"demo_emb_zh_model.h5\")\n",
    "\n",
    "# en\n",
    "if not os.path.exists(emb_en_model_path):\n",
    "  demo_emb_en_model = tf.keras.Sequential()\n",
    "  demo_emb_en_model.add(tf.keras.layers.Embedding(vocab_size_en, d_model))\n",
    "  demo_emb_en_model.save(emb_en_model_path)\n",
    "  embedding_layer_en = demo_emb_en_model\n",
    "else:\n",
    "  embedding_layer_en = tf.keras.models.load_model(emb_en_model_path)\n",
    "# zh\n",
    "if not os.path.exists(emb_zh_model_path):\n",
    "  demo_emb_zh_model = tf.keras.Sequential()\n",
    "  demo_emb_zh_model.add(tf.keras.layers.Embedding(vocab_size_zh, d_model))\n",
    "  demo_emb_zh_model.save(emb_zh_model_path)\n",
    "  embedding_layer_zh = demo_emb_zh_model\n",
    "else:\n",
    "  embedding_layer_zh = tf.keras.models.load_model(emb_zh_model_path)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMdROD4GTIYO"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# 底下如果執行會產生隨機結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_sMlv2i7xg-"
   },
   "source": [
    "\n",
    "```python\n",
    "\n",
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "VQRYy3FuQnsO",
    "outputId": "600419ac-9d0b-4f65-8249-04630daaddca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_inp: tf.Tensor(\n",
      "[[[ 0.00695511 -0.03370368 -0.03656032 -0.03336458]\n",
      "  [-0.02707888 -0.03917687 -0.01213828  0.00909697]\n",
      "  [ 0.0355427   0.04111305  0.00751223 -0.01974255]\n",
      "  [ 0.02443342 -0.03273199  0.01267544  0.03127003]\n",
      "  [-0.04879753 -0.00119017 -0.00157104  0.01117355]\n",
      "  [-0.02148524 -0.03413673  0.00708324  0.0121879 ]\n",
      "  [-0.00680635  0.02136201 -0.02036932 -0.04211974]\n",
      "  [-0.00680635  0.02136201 -0.02036932 -0.04211974]]\n",
      "\n",
      " [[ 0.00695511 -0.03370368 -0.03656032 -0.03336458]\n",
      "  [-0.0325227  -0.03433502 -0.01849879  0.01439226]\n",
      "  [ 0.00144588 -0.00377025 -0.00798036 -0.04099905]\n",
      "  [ 0.04524285  0.02524642 -0.00924555 -0.01368124]\n",
      "  [-0.0159062   0.01108797 -0.0177028  -0.0435766 ]\n",
      "  [ 0.00240784 -0.04652226  0.01821991 -0.04349295]\n",
      "  [-0.04879753 -0.00119017 -0.00157104  0.01117355]\n",
      "  [-0.02148524 -0.03413673  0.00708324  0.0121879 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "emb_tar: tf.Tensor(\n",
      "[[[-0.0441955  -0.01026772  0.03740635  0.02017349]\n",
      "  [ 0.02129837 -0.00746276  0.03881821 -0.01586295]\n",
      "  [-0.01179456  0.02825376  0.00738146  0.02963744]\n",
      "  [ 0.01171205  0.04350302 -0.01190796  0.02526634]\n",
      "  [ 0.03814722 -0.03364048 -0.03744673  0.04369817]\n",
      "  [ 0.0280853   0.01269842  0.04268574 -0.04069148]\n",
      "  [ 0.04029209 -0.00619308 -0.04934603  0.02242902]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]]\n",
      "\n",
      " [[-0.0441955  -0.01026772  0.03740635  0.02017349]\n",
      "  [-0.00359621 -0.01380367 -0.02875998 -0.03855735]\n",
      "  [ 0.04516688 -0.04480755 -0.03278694 -0.0093614 ]\n",
      "  [ 0.04131394 -0.04065727 -0.04330624 -0.03341667]\n",
      "  [ 0.03572228 -0.04500845  0.0470326   0.03095007]\n",
      "  [-0.03566641 -0.03730996 -0.00597564 -0.03933349]\n",
      "  [ 0.01850356  0.03993076  0.02729526 -0.04848848]\n",
      "  [-0.02294568 -0.02494572 -0.0136737  -0.04278342]\n",
      "  [ 0.0280853   0.01269842  0.04268574 -0.04069148]\n",
      "  [ 0.04029209 -0.00619308 -0.04934603  0.02242902]]], shape=(2, 10, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "# 這個才是每次都須執行的\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar:\", emb_tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TMhPneC7xWN"
   },
   "source": [
    "注意你的詞嵌入層的隨機初始值會跟我不同，結果可能會有一點差異。 \n",
    "\n",
    "但重點是你能在腦海中理解這兩個 3 維張量嗎？花了幾秒鐘？我相信在座不乏各路高手，而且事實上在這一行混久了，你也必須能直覺地理解這個表示方式。\n",
    "\n",
    "但如果有更好的呈現方式幫助我們理解數據，何樂而不為呢？讓我們再次視覺化這兩個 3 維詞嵌入張量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AicIhJI52rj"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# plot(emb_inp, name='emb_inp', labels=['子詞 subword', 'embedded_dim'], \n",
    "#      shape_desc='\\n(batch_size, inp_seq_len, d_model)')\n",
    "\n",
    "# plot(emb_tar, name='emb_tar', labels=['子詞 subword', 'embedded_dim'], \n",
    "#      shape_desc='\\n(batch_size, tar_seq_len, d_model)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7XJiRWk_EXk"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/emb_inp_tar.mp4\n",
    "- images/transformer/emb_inp_tar.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzeVto7Z53Cn"
   },
   "source": [
    "依照前面提過的準則，張量中第一個維度的 `2` 代表著句子數 `batch_size`。在 3 維空間裡頭，我會將不同句子畫在 z 軸上，也就是你現在把臉貼近 /  遠離螢幕這個維度。你同時也能用不同顏色來區分句子。\n",
    "\n",
    "緊跟著句子的下一個維度則一樣是本來的子詞（subword）。只是現在每個子詞都已從一個索引數字被轉換成一個 4 維的詞嵌入向量，因此每個子詞都以 y 軸來表示。最後一維則代表著詞嵌入空間的維度，一樣以 x 軸來表示。\n",
    "\n",
    "現在再次點擊播放鍵。\n",
    "\n",
    "在學會怎麼解讀這個 3 維詞嵌入張量以後，你就能明白為何 `emb_tar` 第一個中文句子裡頭的倒數 3 行（row) 都長得一樣了：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "2ecZ7WbxzU3k",
    "outputId": "3b2bf802-6a11-49a1-a563-acd8b1e8616e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n",
      "--------------------\n",
      "emb_tar[0]: tf.Tensor(\n",
      "[[-0.00285894  0.02392108 -0.03126474  0.01345349]\n",
      " [-0.00285894  0.02392108 -0.03126474  0.01345349]\n",
      " [-0.00285894  0.02392108 -0.03126474  0.01345349]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"tar[0]:\", tar[0][-3:])\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar[0]:\", emb_tar[0][-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDiDY9cLzUYF"
   },
   "source": [
    "\n",
    "\n",
    "它們都是 `<pad>` token（以 `0` 表示），理當有一樣的詞嵌入向量。\n",
    "\n",
    "不同顏色也讓我們可以很直觀地理解一個句子是怎麼從一個 1 維向量被轉換到 2 維的。你後面就會發現，你將需要能夠非常直覺地理解像是 `emb_tar` 這種 3 維張量裡頭每個維度所代表的意義。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8kFUd_lA4wB"
   },
   "source": [
    "### 遮罩：Transformer 的祕密配方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_1tNKULClUU"
   },
   "source": [
    "我們在前面並沒有仔細談過遮罩（masking）的概念，但事實上它可以說是在實作 Transformer 時最重要卻也最容易被搞砸的一環。它讓 Transformer 在進行自注意力機制（Self-Attention Mechanism）時不至於偷看到不該看的。\n",
    "\n",
    "在 Transformer 裡頭有兩種 masks：\n",
    "- padding mask\n",
    "- look ahead mask\n",
    "\n",
    "padding mask 是讓 Transformer 用來識別序列實際的內容到哪裡。此遮罩負責的就是將序列中被補 0 的地方（也就是 `<pad>`）的位置蓋住，讓 Transformer 可以避免「關注」到這些位置。\n",
    "\n",
    "look ahead mask 人如其名，是用來確保 Decoder 在進行自注意力機制時每個子詞只會「往前看」：關注（包含）自己之前的字詞，不會不小心關注「未來」Decoder 產生的子詞。我們後面還會看到 look ahead mask 的詳細介紹，但不管是哪一種遮罩向量，那些值為 1 的位置就是遮罩存在的地方。\n",
    "\n",
    "因為 padding mask 的概念相對簡單，讓我們先看這種遮罩：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "4lrNbzPVj0NQ",
    "outputId": "a3d02d44-db9c-4a01-a6a9-acafb7d473bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=193029, shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lxSQj7tjz-u"
   },
   "source": [
    "登登！我們的第一個 4 維張量！不過別緊張，我們在中間加了 2 個新維度是為了之後可以做 [broadcasting](https://www.numpy.org/devdocs/user/theory.broadcasting.html)，現在可以忽視。喔！不過如果這是你第一次聽到 broadcasting，我強烈建議你現在就閱讀 [numpy 官方的簡短教學](https://www.numpy.org/devdocs/user/theory.broadcasting.html)了解其概念。我們後面也會看到實際的 broadcasting 例子。\n",
    "\n",
    "回到我們的 `inp_mask` 遮罩。現在我們可以先將額外的維度去掉以方便跟 `inp` 作比較：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Dm-6Y-HaOjhT",
    "outputId": "0052aa74-1212-41b5-9b2d-0c7468ba500e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-B6XJVjOjB_"
   },
   "source": [
    "你可以看到 `inp_mask` 將 `inp` 裡頭為 `0` 的對應位置設為 1 凸顯出來，這樣之後其他函式就知道要把哪邊「遮住」。 讓我們看看被降到 2 維的 `inp_mask` 是怎麼被套用在 `inp` 身上的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suQRnY-d05WJ"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# plot(inp, name='inp')\n",
    "# plot(tf.squeeze(inp_mask), name=\"inp_mask\")\n",
    "# # todo: 要自己修正下面的 masked_inp\n",
    "# plot(inp, mask=tf.cast(tf.squeeze(inp_mask), tf.int32), name=\"masked_inp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9_cYJfsmSt3"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/padding_mask.mp4\n",
    "- images/transformer/padding_mask.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXbhYuePT0CY"
   },
   "source": [
    "很好懂，不是嗎？但這只是小暖身，等到之後要將遮罩 broadcast 到 3、4 維張量的時候你可能會黑人問號，所以最好做點心理準備（笑\n",
    "\n",
    "至於另外一種遮罩 look ahead mask，等我們說明完下節的注意函式以後你會比較容易理解它的作用，所以先賣個關子。現在讓我們進入 Tranformer 最核心的部分：注意力機制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_Lau78n8gLl"
   },
   "source": [
    "### Scaled dot product attention：一種注意函式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqrHvrRcXK_h"
   },
   "source": [
    "我們在文中以及教授的影片已經多次看到，所謂的注意力機制（或稱注意函式，attention function）概念上就是拿一個查詢（query）去跟一組 key-values 做運算，最後產生一個輸出。只是我們會利用矩陣運算同時讓多個查詢跟一組 key-values 做運算，最大化計算效率。\n",
    "\n",
    "而不管是查詢（query）、鍵值（keys）還是值（values）或是輸出，全部都是向量（vectors）。該輸出是 values 的加權平均，而每個 value  獲得的權重則是由當初 value 對應的 key 跟 query 計算匹配程度所得來的。（[論文原文](https://arxiv.org/pdf/1706.03762.pdf)稱此計算匹配程度的函式為 compatibility function）\n",
    "\n",
    "將此運算以圖表示的話則會長得像這樣："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E318qG998ct1"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/scaled-dot-product.jpg\n",
    "- 左右兩邊大致上講的是一樣的事情，不過右側省略 Scale 以及 Mask 步驟，而左側則假設我們已經拿到經過線性轉換的 Q, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6tHfQq2bjQf"
   },
   "source": [
    "我們是第一次秀出論文裡頭的圖片（左），但右邊你應該不陌生才對。\n",
    "\n",
    "Scaled dot product attention 跟以往 multiplicative attention 一樣是先將維度相同的 Q 跟 K 做[點積](https://zh.wikipedia.org/wiki/%E7%82%B9%E7%A7%AF)：將對應維度的值兩兩相乘後相加得到單一數值，接著把這些數值除以一個 scaling factor `sqrt(dk)` ，然後再丟入 [softmax 函式](https://www.youtube.com/watch?v=mlaLLQofmR8)得到相加為 1 的注意權重（attention weights）。\n",
    "\n",
    "最後以此權重對 V 作加權平均得到輸出結果。\n",
    "\n",
    "除以 scaling factor 的目的是為了讓點積出來的值不會因為 Q 以及 K 的維度 `dk` 太大而跟著太大（舌頭打結）。因為太大的點積值丟入 softmax 函式有可能使得其梯度變得極小，導致訓練結果不理想。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5V2HsDnBoCjs"
   },
   "source": [
    "!image\n",
    "- transformer/softmax-function.jpg\n",
    "- Softmax 函式讓某個 Q 與多個 K 之間的匹配值和為 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AR8SAWk1oCO0"
   },
   "source": [
    "說完概念，讓我們看看 Transformer 論文中的這個注意函式怎麼運作吧！首先我們得先準備這個函式的輸入 Q, K, V 才行。我們在 [Multi-head attention](#Multi-head-attention：你看你的，我看我的) 一節會看到，在進行 scaled dot product attention 時會需要先將 Q、K 以及 V 分別做一次線性轉換，但現在讓我們先忽略這部分。\n",
    "\n",
    "這邊我們可以拿已經被轉換成詞嵌入空間的英文張量 `emb_inp` 來充當左圖中的 Q 以及 K，讓它自己跟自己做匹配。V 則讓我隨機產生一個 binary 張量（裡頭只有 1 或 0）來當作每個 K 所對應的值，方便我們直觀解讀 scaled dot product attention 的輸出結果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "ff4qRfDKa2Rg",
    "outputId": "660d0d3e-26b5-4bf8-cc22-56ab20c0ff71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=193043, shape=(2, 8, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjhPJqRXkJ0B"
   },
   "source": [
    "好啦，我想你現在應該能快速地解讀 3 維張量了，但還是讓我雞婆點，將現在的 Q, K, V 都畫出來讓你參考："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-X9-8FO2bK7D"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# plot(q, name='q', labels=['子詞 subword', 'depth'], \n",
    "#      shape_desc='\\n(batch_size, seq_len_q, depth)')\n",
    "\n",
    "# plot(k, name='k', labels=['子詞 subword', 'depth'], \n",
    "#      shape_desc='\\n(batch_size, seq_len_k, depth)')\n",
    "\n",
    "# plot(v, name='v', labels=['子詞 subword', 'depth_v'], \n",
    "#      shape_desc='\\n(batch_size, seq_len_q, depth_v)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Q0M_eV_qEqZ"
   },
   "source": [
    "!image\n",
    "- transformer/q_k_v.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPZdWdLNkyRe"
   },
   "source": [
    "注意顏色。雖然我們將拿 Q 跟 K 來做匹配，這個匹配只會發生在同個句子（同個顏色）底下（即 `shape[1:]`）。在深度學習世界，我們會為了最大化 GPU 的運算效率而一次將 64 個、128 個或是更多個 `batch_size` 的句子丟入模型。習慣 batch 維度的存在是非常實際的。 \n",
    "\n",
    "接著讓我們看看 scaled dot product attention 在 TensorFlow 裡是[怎麼被實作](https://www.tensorflow.org/beta/tutorials/text/transformer?authuser=1#scaled_dot_product_attention)的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHeaHY0Va2Gq"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "  # 將 `q`、 `k` 做點積再 scale\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "  # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # 以注意權重對 v 做加權平均（weighted average）\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RxfzrQZ0zRxs"
   },
   "source": [
    "別被嚇到了。除了遮罩的運算部分我們還沒解釋，這 Python 函式事實上就是用 TensorFlow API 來實現剛剛才說的注意力機制邏輯罷了：\n",
    "1. 將 `q` 和 `k` 做點積得到 `matmul_qk`\n",
    "2. 將 `matmul_qk` 除以 scaling factor `sqrt(dk)`\n",
    "3. 有遮罩的話在丟入 softmax **前**套用\n",
    "4. 通過 softmax 取得加總為 1 的注意權重\n",
    "5. 以該權重加權平均 `v` 作為輸出結果\n",
    "6. 回傳輸出結果以及注意權重\n",
    "\n",
    "扣掉註解事實上也就只有 8 行代碼（當然有很多實作細節）。現在先讓我們實際將 `q`, `k`, `v` 輸入此函式看看得到的結果。假設沒有遮罩的存在：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "7xnJle1v5vXg",
    "outputId": "0796f9ec-eb0d-482b-f6d4-e2e76b4c0e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.37502408 0.37503672 0.37488326 0.49993956]\n",
      "  [0.37513658 0.37514552 0.37500778 0.49994028]\n",
      "  [0.37483314 0.37482613 0.3749625  0.50006175]\n",
      "  [0.37516367 0.37501514 0.3750258  0.49997073]\n",
      "  [0.37503195 0.3751256  0.3750621  0.49998796]\n",
      "  [0.37512696 0.37512186 0.37502852 0.49996266]\n",
      "  [0.3748441  0.3749599  0.37492597 0.50001484]\n",
      "  [0.3748441  0.3749599  0.37492597 0.50001484]]\n",
      "\n",
      " [[0.62516296 0.2500847  0.6250717  0.37522966]\n",
      "  [0.62490153 0.24994145 0.62504375 0.37497035]\n",
      "  [0.62509674 0.2501282  0.6249581  0.37518966]\n",
      "  [0.62518024 0.25003165 0.6250133  0.37507355]\n",
      "  [0.6250232  0.25011832 0.62486345 0.37516582]\n",
      "  [0.6251376  0.25018096 0.625095   0.37525034]\n",
      "  [0.62478966 0.24995528 0.6248975  0.37490302]\n",
      "  [0.62492853 0.24997747 0.62507135 0.37497336]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.12517719 0.12502946 0.12490283 0.12493535 0.12491155 0.12497091\n",
      "   0.12503636 0.12503636]\n",
      "  [0.12505189 0.12512855 0.12479477 0.1250193  0.12506542 0.12509388\n",
      "   0.12492308 0.12492308]\n",
      "  [0.12497574 0.12484524 0.1252356  0.12496044 0.12489695 0.1248758\n",
      "   0.12510511 0.12510511]\n",
      "  [0.12500346 0.12506503 0.1249556  0.12519364 0.12496658 0.12508455\n",
      "   0.12486558 0.12486558]\n",
      "  [0.12494988 0.12508136 0.12486238 0.12493681 0.12514524 0.12506418\n",
      "   0.12498005 0.12498005]\n",
      "  [0.12500885 0.12510943 0.12484082 0.12505434 0.12506378 0.12510203\n",
      "   0.12491038 0.12491038]\n",
      "  [0.1250592  0.12492351 0.12505497 0.12482036 0.12496454 0.12489527\n",
      "   0.12514108 0.12514108]\n",
      "  [0.1250592  0.12492351 0.12505497 0.12482036 0.12496454 0.12489527\n",
      "   0.12514108 0.12514108]]\n",
      "\n",
      " [[0.12514497 0.1249882  0.12503006 0.12493392 0.1250188  0.12506588\n",
      "   0.1248794  0.12493874]\n",
      "  [0.1250289  0.12513264 0.12493595 0.12481083 0.12494826 0.12499319\n",
      "   0.12507208 0.12507817]\n",
      "  [0.12506142 0.12492662 0.12505917 0.12498691 0.12506557 0.12506266\n",
      "   0.12491715 0.12492047]\n",
      "  [0.12504192 0.12487808 0.1250636  0.12521076 0.12504579 0.12498584\n",
      "   0.12487733 0.12489669]\n",
      "  [0.12504749 0.12493626 0.12506288 0.12496644 0.12510824 0.12501009\n",
      "   0.12496544 0.12490314]\n",
      "  [0.12506938 0.12495602 0.1250348  0.12488137 0.12498492 0.12519602\n",
      "   0.12488527 0.12499221]\n",
      "  [0.12494776 0.12509981 0.1249542  0.12483776 0.12500516 0.12495013\n",
      "   0.12514311 0.12506206]\n",
      "  [0.12499588 0.12509465 0.12494626 0.12484587 0.1249316  0.12504588\n",
      "   0.12505081 0.12508905]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmHNLCy45mH_"
   },
   "source": [
    "`scaled_dot_product_attention` 函式輸出兩個張量：\n",
    "- `output` 代表注意力機制的結果\n",
    "- `attention_weights` 代表句子 `q` 裡頭每個子詞對句子 `k` 裡頭的每個子詞的注意權重\n",
    "\n",
    "而因為你知道目前的 `q` 跟 `k` 都代表同個張量 `emb_inp`，因此 `attention_weights` 事實上就代表了 `emb_inp` 裡頭每個英文序列中的子詞對其他位置的子詞的注意權重。你可以再次參考之前 Transformer 是如何做 encoding 的動畫。\n",
    "\n",
    "\n",
    "`output` 則是句子裡頭每個位置的子詞將 `attention_weights` 當作權重，從其他位置的子詞對應的資訊 `v` 裡頭抽取有用訊息後匯總出來的結果。你可以想像 `ouput` 裡頭的每個子詞都獲得了一個包含自己以及周遭子詞語義資訊的新 representation。而因為現在每個字詞的注意權重都相同，最後得到的每個 repr. 都長得一樣。\n",
    "\n",
    "下面則是我們實作的注意函式的所有輸入與輸出張量。透過多次的矩陣運算，注意力機制能讓查詢 Q 跟鍵值 K 做匹配，再依據此匹配程度將值 V 做加權平均獲得新的 representation。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiowFLCrxI4Q"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/scaled_dot_product_attention.mp4\n",
    "- images/transformer/scaled_dot_product_attention.jpg\n",
    "- Scaled dot product attention 的實際運算過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-LMvH9b1j1-"
   },
   "source": [
    "別只聽我碎碎念，自己點擊播放鍵來了解背後到底發生什麼事情吧！\n",
    "\n",
    "\n",
    "動畫裡包含許多細節，但只要有矩陣運算的基本概念，你應該已經能夠直觀且正確地理解注意函式是怎麼運作的了。在真實世界裡我們當然會用更長的序列、更大的 `batch_size` 來處理數據，但上面呈現的是程式碼的實際結果，而非示意圖而已。這是注意力機制真正的「所見即所得」。\n",
    "\n",
    "一般來說注意函式的輸出 `output`張量維度會跟 `q` 張量相同（假設圖上的 `depth_v` 等於 `depth`）。此張量也被稱作「注意張量」，你可以將其解讀為 `q` 在關注 `k` 並從 `v` 得到上下文訊息後的所獲得的新 representation。而注意權重 `attention_weights` 則是 `q` 裡頭每個句子的每個子詞對其他位置的子詞的關注程度。\n",
    "\n",
    "P.S. 一般注意函式只需輸出注意張量。而我們在這邊將注意權重 `attention_weights` 也輸出是為了方便之後觀察 Transformer 在訓練的時候將「注意力」放在哪裡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IH47Vpra18g"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "def scaled_dot_product_attention_demo(q, k, v, mask, draw=False):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "  labels= ['subwords in q', 'subwords in k']\n",
    "  shape_desc = '\\n(batch_size, seq_len_q, seq_len_k)'\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  if draw:\n",
    "    plot(matmul_qk, name='matmul_qk', labels=labels, shape_desc=shape_desc)\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "  if draw:\n",
    "    plot(scaled_attention_logits, name=\"scaled_attention_logits\", \n",
    "         labels=labels, shape_desc=shape_desc)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  mask_name = ''\n",
    "  if mask is not None:\n",
    "    mask_name = 'masked_'\n",
    "    orig_mask = mask\n",
    "    # demo usage for q, k, v less than ndim=4\n",
    "    if q.ndim == 4:\n",
    "      pass\n",
    "    elif q.ndim == 3 and mask.ndim == 4:\n",
    "      orig_mask = mask\n",
    "      mask = tf.squeeze(mask, axis=[1])\n",
    "#       mask = tf.reshape(mask, shape=(mask.shape[0], 1, mask.shape[-1]))\n",
    "    \n",
    "    if draw:\n",
    "      plot(mask, name=\"mask\", \n",
    "         labels=['sentence', 'subwords in seq_q'], shape_desc='\\n(batch_size, 1, seq_len_q)')\n",
    "      \n",
    "      temp_mask = mask * -1e9\n",
    "      plot(temp_mask, name=\"mask\", \n",
    "         labels=['sentence', 'subwords in seq_q'], shape_desc='\\n(batch_size, 1, seq_len_q)')\n",
    "      # padding mask broadcasting demo\n",
    "#       for i in range(2, 9):\n",
    "#         t = tf.constant(np.repeat(temp_mask, i, axis=1))\n",
    "\n",
    "#         plot(t, name=\"mask\", \n",
    "#          labels=['subwords in seq_q', 'subwords in seq_q'], shape_desc=f'\\n(batch_size, {i}, seq_len_q)')\n",
    "      \n",
    "  \n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "    if draw:\n",
    "      plot(scaled_attention_logits, name=\"scaled_attention_logits\", \n",
    "           labels=labels, shape_desc=shape_desc)\n",
    "#       plot(scaled_attention_logits, name=\"scaled_attention_logits\", \n",
    "#            labels=labels, mask=orig_mask, shape_desc=shape_desc)\n",
    "    \n",
    "    \n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  if draw:\n",
    "    plot(attention_weights, name='attention_weights', labels=labels, \n",
    "         shape_desc=shape_desc)\n",
    "    \n",
    "#     plot(attention_weights, name='attention_weights', labels=labels, \n",
    "#          shape_desc=shape_desc,\n",
    "#          mask=mask)\n",
    "    \n",
    "  \n",
    "  if draw:\n",
    "    plot(v, name='v', labels=['subwords in q', 'depth_v'], \n",
    "       shape_desc='\\n(batch_size, seq_len_v, depth_v)')\n",
    "  \n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "  if draw:\n",
    "    plot(output, name=\"output\", labels=['subwords in q', 'depth_v'], \n",
    "         shape_desc='\\n(batch_size, seq_len_q, depth_v)')\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nBzlF9Ha1vq"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# mask = None\n",
    "# draw = True\n",
    "# output, attention_weights = scaled_dot_product_attention_demo(q, k, v, mask, draw=draw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vM60FBxMa1eG"
   },
   "source": [
    "### 直觀理解遮罩在注意函式中的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_N-bA1HQa4U"
   },
   "source": [
    "剛剛為了讓你消化注意函式裡頭最重要的核心邏輯，我刻意忽略了遮罩（masking）的存在。現在讓我們重新把 `scaled_dot_product_attention` 裡頭跟遮罩相關的程式碼拿出來瞧瞧：\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "# 將 `q`、 `k` 做點積再 scale\n",
    "scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "# 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "if mask is not None:\n",
    "  scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "# 取 softmax 是為了得到總和為 1 的比例做加權平均\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "...\n",
    " ```\n",
    "\n",
    "如果你剛剛有仔細看上面的動畫的話（17 秒之後），應該能想像 `scaled_attention_logits` 的 shape 為 （batch_size, seq_len_q, seq_len_k）。其最後一個維度代表某個序列 `q` 裡的某個子詞與序列 `k` 的**每個**子詞的匹配程度，但加總不為 1。而為了之後跟與 `k` 對應的 `v` 做加權平均，我們針對最後一個維度做 softmax 運算使其和為 1，也就是上圖 `axis=-1` 的部分：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Emy7nRA04vU1"
   },
   "source": [
    "!mp4\n",
    "- options: controls\n",
    "- images/transformer/softmax.mp4\n",
    "- images/transformer/softmax.jpg\n",
    "- 對最後一維做 softmax。模型還沒經過訓練所以「注意力」非常平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mn4JXUIb4yib"
   },
   "source": [
    "如果序列 `k` 裡頭的每個子詞 `sub_k` 都是實際存在的中文字或英文詞彙，這運算當然沒問題。我們會希望序列 `q` 裡頭的每個子詞 `sub_q` 都能從每個 `sub_k` 獲得它所需要的語義資訊。\n",
    "\n",
    "但李組長眉頭一皺，發現案情並不單純。\n",
    "\n",
    "回想一下，我們的 `q` 跟 `k` 都是從 `emb_inp` 來的。`emb_inp` 代表著英文句子的詞嵌入張量，而裡頭的第一個句子應該是有 `<pad>` token 的。啊哈！誰會想要放注意力在沒有實際語義的傢伙上呢？\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "if mask is not None:\n",
    "  scaled_attention_logits += (mask * -1e9) # 是 -1e9 不是 1e-9\n",
    "\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "因此在注意函式裡頭，我們將遮罩乘上一個接近**負**無窮大的 `-1e9`，並把它加到進入 softmax **前**的 logits 上面。這樣可以讓這些被加上**極大負值**的位置變得無關緊要，在經過 softmax 以後的值趨近於 0。這效果等同於序列 `q` 中的某個子詞 `sub_q` 完全沒放注意力在這些被遮罩蓋住的子詞 `sub_k` 之上（此例中 `sub_k` 指是的 `<pad>`）。\n",
    "\n",
    "（動腦時間：為何遮罩要放在 softmax 之前而不能放之後？）\n",
    "\n",
    "聽我說那麼多不如看實際的運算結果。讓我們再次為英文句子 `inp` 產生對應的 padding mask："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "gvDht6Fa4yO-",
    "outputId": "de47dcb4-73b9-4583-9d88-5bb7bd21cc87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdbdhhI2Gfkn"
   },
   "source": [
    "很明顯地，**第一個**英文序列的最後 2 個位置是不具任何語義的 `<pad>`（圖中為 `0` 的部分）。而這也是為何我們需要將遮罩 `inp_mask` 輸入到注意函式，避免序列中的子詞關注到這 2 個傢伙的原因。\n",
    "\n",
    "我們這次把 `inp_mask` 降到 3 維，並且將其跟剛剛的 `q`、`k` 和 `v` 一起丟進注意函式裡頭，看看注意權重有什麼變化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "sZNeNzCtsJzD",
    "outputId": "0e8150a6-5216-4730-d4b1-ba51429360a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[0.16691911 0.1667221  0.16655324 0.16659662 0.1665649  0.16664404\n",
      "   0.         0.        ]\n",
      "  [0.16670164 0.16680385 0.1663589  0.16665822 0.16671969 0.16675764\n",
      "   0.         0.        ]\n",
      "  [0.16668104 0.16650699 0.16702762 0.16666064 0.16657597 0.16654775\n",
      "   0.         0.        ]\n",
      "  [0.16661155 0.16669361 0.16654776 0.16686502 0.16656238 0.16671962\n",
      "   0.         0.        ]\n",
      "  [0.16659099 0.16676629 0.16647433 0.16657357 0.16685146 0.16674338\n",
      "   0.         0.        ]\n",
      "  [0.16663864 0.16677272 0.16641466 0.16669929 0.16671185 0.16676286\n",
      "   0.         0.        ]\n",
      "  [0.16680835 0.16662736 0.1668027  0.16648975 0.16668208 0.1665897\n",
      "   0.         0.        ]\n",
      "  [0.16680835 0.16662736 0.1668027  0.16648975 0.16668208 0.1665897\n",
      "   0.         0.        ]]\n",
      "\n",
      " [[0.12514497 0.1249882  0.12503006 0.12493392 0.1250188  0.12506588\n",
      "   0.1248794  0.12493874]\n",
      "  [0.1250289  0.12513264 0.12493595 0.12481083 0.12494826 0.12499319\n",
      "   0.12507208 0.12507817]\n",
      "  [0.12506142 0.12492662 0.12505917 0.12498691 0.12506557 0.12506266\n",
      "   0.12491715 0.12492047]\n",
      "  [0.12504192 0.12487808 0.1250636  0.12521076 0.12504579 0.12498584\n",
      "   0.12487733 0.12489669]\n",
      "  [0.12504749 0.12493626 0.12506288 0.12496644 0.12510824 0.12501009\n",
      "   0.12496544 0.12490314]\n",
      "  [0.12506938 0.12495602 0.1250348  0.12488137 0.12498492 0.12519602\n",
      "   0.12488527 0.12499221]\n",
      "  [0.12494776 0.12509981 0.1249542  0.12483776 0.12500516 0.12495013\n",
      "   0.12514311 0.12506206]\n",
      "  [0.12499588 0.12509465 0.12494626 0.12484587 0.1249316  0.12504588\n",
      "   0.12505081 0.12508905]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 這次讓我們將 padding mask 放入注意函式並觀察\n",
    "# 注意權重的變化\n",
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUTWJY3_IzoW"
   },
   "source": [
    "加了 padding mask 後，第一個句子裡頭的每個子詞針對倒數兩個字詞的「注意權重」的值都變成 0 了。上句話非常饒舌，但我相信已經是非常精準的說法了。讓我把這句話翻譯成 numpy 的 slice 語法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "7IYfmpWYIpLb",
    "outputId": "3a12d3c6-05bd-4a51-abb5-e998c277dcfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=193086, shape=(2, 8, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.1248794 , 0.12493874],\n",
       "        [0.12507208, 0.12507817],\n",
       "        [0.12491715, 0.12492047],\n",
       "        [0.12487733, 0.12489669],\n",
       "        [0.12496544, 0.12490314],\n",
       "        [0.12488527, 0.12499221],\n",
       "        [0.12514311, 0.12506206],\n",
       "        [0.12505081, 0.12508905]]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事實上也不完全是上句話的翻譯，\n",
    "# 因為我們在第一個維度還是把兩個句子都拿出來方便你比較\n",
    "attention_weights[:, :, -2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMm4zd8SIkka"
   },
   "source": [
    "第一個英文句子的最後 2 個位置因為是 `<pad>` 所以被遮罩「蓋住」而沒有權重值（上方 2 維陣列）；第二個句子的序列（下方 2 維陣列）則因為最後 2 個位置仍是正常的英文子詞，因此都有被其他子詞關注。\n",
    "\n",
    "如果聽完我的碎碎念你還是無法理解以上結果，或是不確定有遮罩的注意函式到底怎麼運作，就實際看看其中的計算過程吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_0I4BXFOI4z"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/padding_mask_in_attn_func.mp4\n",
    "- images/transformer/padding_mask_in_attn_func.jpg\n",
    "- 將 padding mask 應用到自注意力機制運算（q = k）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nm7eSh7Lmiq2"
   },
   "source": [
    "一張圖勝過千言萬語。在 padding mask 的幫助下，注意函式輸出的新序列 `output` 裡頭的每個子詞都只從序列 `k` （也就是序列 `q` 自己）的前 6 個實際子詞而非 `<pad>` 來獲得語義資訊（最後一張圖的黑框部分）。\n",
    "\n",
    "再次提醒，因為我們輸入注意函式的 `q` 跟 `k` 都是同樣的英文詞嵌入張量 `emb_inp`，事實上這邊做的就是讓英文句子裡頭的每個子詞都去關注同句子中其他位置的子詞的資訊，並從中獲得上下文語義，而這就是所謂的自注意力機制（self-attention）：序列關注自己。\n",
    "\n",
    "當序列 `q` 換成 Decoder 的輸出序列而序列 `k` 變成 Encoder 的輸出序列時，我們就變成在計算一般 Seq2Seq 模型中的注意力機制。這點觀察非常重要，且[我們在前面就已經提過了](#Transformer：Seq2Seq-模型-+-自注意力機制)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3fF3pDMi_qu"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# mask = tf.squeeze(inp_mask, axis=1)\n",
    "# draw = True\n",
    "# output, attention_weights = scaled_dot_product_attention_demo(q, k, v, mask, draw=draw)\n",
    "# print(\"output:\", output)\n",
    "# print(\"-\" * 20)\n",
    "# print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7vmNnsupKWg"
   },
   "source": [
    "打鐵趁熱，讓我們看看前面提過的另一種遮罩 look ahead mask："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "yPuW1HBsrxay",
    "outputId": "6b52a360-1bce-4f7c-b0d1-3b1b4ec19117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[-0.0441955  -0.01026772  0.03740635  0.02017349]\n",
      "  [ 0.02129837 -0.00746276  0.03881821 -0.01586295]\n",
      "  [-0.01179456  0.02825376  0.00738146  0.02963744]\n",
      "  [ 0.01171205  0.04350302 -0.01190796  0.02526634]\n",
      "  [ 0.03814722 -0.03364048 -0.03744673  0.04369817]\n",
      "  [ 0.0280853   0.01269842  0.04268574 -0.04069148]\n",
      "  [ 0.04029209 -0.00619308 -0.04934603  0.02242902]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]]\n",
      "\n",
      " [[-0.0441955  -0.01026772  0.03740635  0.02017349]\n",
      "  [-0.00359621 -0.01380367 -0.02875998 -0.03855735]\n",
      "  [ 0.04516688 -0.04480755 -0.03278694 -0.0093614 ]\n",
      "  [ 0.04131394 -0.04065727 -0.04330624 -0.03341667]\n",
      "  [ 0.03572228 -0.04500845  0.0470326   0.03095007]\n",
      "  [-0.03566641 -0.03730996 -0.00597564 -0.03933349]\n",
      "  [ 0.01850356  0.03993076  0.02729526 -0.04848848]\n",
      "  [-0.02294568 -0.02494572 -0.0136737  -0.04278342]\n",
      "  [ 0.0280853   0.01269842  0.04268574 -0.04069148]\n",
      "  [ 0.04029209 -0.00619308 -0.04934603  0.02242902]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S29Jb760q9lL"
   },
   "source": [
    "我們已經知道 demo 用的中文（目標語言）的序列長度為 `10`，而 look ahead 遮罩就是產生一個 2 維矩陣，其兩個維度都跟中文的詞嵌入張量 `emb_tar` 的倒數第 2 個維度（序列長度）一樣，且裡頭是一個倒三角形（1 的部分）。\n",
    "\n",
    "我們[前面曾經說過](#%E9%81%AE%E7%BD%A9%EF%BC%9ATransformer-%E7%9A%84%E7%A5%95%E5%AF%86%E9%85%8D%E6%96%B9) `look_ahead_mask` 是用來確保 Decoder 在進行自注意力機制時輸出序列裡頭的每個子詞只會關注到自己之前（左邊）的字詞，不會不小心關注到未來（右邊）理論上還沒被  Decoder 生成的子詞。\n",
    "\n",
    "運用從 padding mask 學到的概念，想像一下如果把這個倒三角的遮罩跟之前一樣套用到進入 softmax **之前**的 `scaled_attention_logits`，輸出序列 `output` 裡頭的每個子詞的 repr. 會有什麼性質？\n",
    "\n",
    "溫馨小提醒：`scaled_attention_logits` 裡頭的每一 row 紀錄了某個特定子詞對其他子詞的注意權重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "7mdAR3rtAYcG",
    "outputId": "07eaccb9-cf65-491c-8bc9-b4c43eeafb32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49974996 0.50025004 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.33338806 0.33309633 0.3335156  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24980238 0.2497976  0.25013384 0.25026616 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19975185 0.19982941 0.19989952 0.199991   0.20052823 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16658378 0.16686733 0.16656147 0.16657883 0.1664059  0.16700274\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14259693 0.1427213  0.14279391 0.1429158  0.14314583 0.14267854\n",
      "   0.14314772 0.         0.         0.        ]\n",
      "  [0.12491751 0.12487698 0.12503591 0.12508857 0.12503389 0.12487747\n",
      "   0.12507991 0.12508978 0.         0.        ]\n",
      "  [0.11102892 0.1109929  0.11113416 0.11118097 0.11113235 0.11099333\n",
      "   0.11117328 0.11118205 0.11118205 0.        ]\n",
      "  [0.09991965 0.09988723 0.10001437 0.10005648 0.10001273 0.09988762\n",
      "   0.10004956 0.10005745 0.10005745 0.10005745]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.4994912  0.5005088  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.33261845 0.33340293 0.3339786  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24919374 0.25002357 0.25033304 0.25044966 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19997214 0.19964042 0.20002526 0.19986893 0.20049322 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16662474 0.16674054 0.16659829 0.16668092 0.16645522 0.16690029\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14276491 0.14288287 0.14274995 0.14281946 0.1427529  0.14282054\n",
      "   0.14320944 0.         0.         0.        ]\n",
      "  [0.12491003 0.1250709  0.12497466 0.12504703 0.12481265 0.1251362\n",
      "   0.12493407 0.12511446 0.         0.        ]\n",
      "  [0.11102156 0.11105824 0.11103692 0.11106326 0.11112017 0.11104742\n",
      "   0.11128615 0.11106552 0.11130078 0.        ]\n",
      "  [0.09983386 0.10001399 0.10016464 0.10015456 0.09999382 0.09989963\n",
      "   0.0998925  0.09993652 0.099891   0.10021948]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgrxoVrRDQB-"
   },
   "source": [
    "答案呼之欲出，套用 look ahead mask 的結果就是讓序列 `q` 裡的每個字詞只關注包含自己左側的子詞，在自己之後的位置的字詞都不看。比方說兩個中文句子的第一個字詞都只關注自己："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "xwKQS2Ao39Sp",
    "outputId": "6edcd26d-98bc-479f-e88e-022542e6efd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=193126, shape=(2, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohgCb0qa4DKf"
   },
   "source": [
    "注意到了嗎？兩個句子的第一個子詞因為自己前面已經沒有其他子詞，所以將全部的注意力 `1`都放在自己身上。讓我們看看第二個子詞："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "QBrghopi4iEA",
    "outputId": "fc1cc32d-3a0f-4399-dab5-996394e3e9e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=193131, shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.49974996, 0.50025004, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.4994912 , 0.5005088 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rK7JWqBP4lvl"
   },
   "source": [
    "兩個句子的第 2 個子詞因為只能看到序列中的第一個子詞以及自己，因此前兩個位置的注意權重加總即為 1，後面位置的權重皆為 0。而現在 2 個值都接近 0.5 是因為我們還沒開始訓練，Transformer 還不知道該把注意力放在哪裡。\n",
    "\n",
    "就跟一般的 [Seq2Seq 模型](#%E7%A5%9E%E7%B6%93%E6%A9%9F%E5%99%A8%E7%BF%BB%E8%AD%AF%EF%BC%9AEncoder-Decoder-%E6%A8%A1%E5%9E%8B)相同，Transformer 裡頭的 Decoder 在生成輸出序列時也是一次產生一個子詞。因此跟輸入的英文句子不同，中文句子裡頭的每個子詞都是在不同時間點產生的。所以理論上 Decoder 在時間點 `t - 1` （或者說是位置 `t - 1`）已經生成的子詞 `subword_t_minus_1` 在生成的時候是不可能能夠關注到下個時間點 `t`（位置 `t`）所生成的子詞 `subword_t` 的，儘管它們在 Transformer 裡頭同時被做矩陣運算。\n",
    "\n",
    "一個位置的子詞不能去關注未來會在自己之後生成的子詞，而這就像是[祖父悖論](https://zh.wikipedia.org/wiki/%E7%A5%96%E7%88%B6%E6%82%96%E8%AB%96)一樣有趣。\n",
    "\n",
    "實際上 look ahead mask 讓 Decoder 在生成第 1 個子詞時只看自己；在生成第 2 個子詞時關注前 1 個子詞與自己； 在生成第 3 個子詞時關注前兩個已經生成的子詞以及自己，以此類推。透過 look ahead mask，你可以想像 Transformer 既可以平行運算，又可以像是 RNN 一樣，在生成子詞時從前面已生成的子詞獲得必要的語義資訊。\n",
    "\n",
    "挺酷的，不是嗎？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EcxNGdnAYNi"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# draw = True\n",
    "# mask = tf.expand_dims(look_ahead_mask, axis=0)\n",
    "# output, attention_weights = scaled_dot_product_attention_demo(\n",
    "#     temp_q, temp_k, temp_v, mask, draw=draw)\n",
    "# print(\"output:\", output)\n",
    "# print(\"-\" * 20)\n",
    "# print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45gLaGuMHu9O"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/look_ahead_mask_in_attn_func.mp4\n",
    "- images/transformer/look_ahead_mask_in_attn_func.jpg\n",
    "- look ahead mask 讓每個子詞都只關注序列中自己與之前的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPxYZze8oOMv"
   },
   "source": [
    "在實際做矩陣運算的時候我們當然還是會讓注意權重為 0 的位置跟對應的 `v` 相乘，但是上圖的黑框才是實際會對最後的 `output` 值造成影響的權重與 `v`。\n",
    "\n",
    "\n",
    "我們在這節了解 Transformer 架構裡頭的兩種遮罩以及它們的作用：\n",
    "- padding mask：遮住 `<pad>` token 不讓所有子詞關注\n",
    "- look ahead mask：遮住 Decoder 未來生成的子詞不讓之前的子詞關注\n",
    "\n",
    "你現在應該能夠想像遮罩在注意力機制裡頭顯得有多麽重要了：它讓注意函式進行高效率的矩陣平行運算的時候不需擔心會關注到不該關注的位置，一次獲得序列中所有位置的子詞各自應有的注意權重以及新的 reprsentation。\n",
    "\n",
    "如果 Transformer 是[變形金剛](https://zh.wikipedia.org/wiki/%E5%8F%98%E5%BD%A2%E9%87%91%E5%88%9A)的話，注意力機制跟遮罩就是[火種源](https://www.easyatm.com.tw/wiki/%E7%81%AB%E7%A8%AE%E6%BA%90)了。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZbC8-hnyJ9F"
   },
   "source": [
    "!image\n",
    "- transformer/transformer-movie.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLqlEcUH51U0"
   },
   "source": [
    "### Multi-head attention：你看你的，我看我的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaxdcK1aQRR1"
   },
   "source": [
    "有好好聽教授講解 Transformer 的話，你應該還記得所謂的多頭注意（multi-head attention）概念。如果你到現在還沒看課程影片或者想複習一下，我把 multi-head attention 的開始跟結束時間都設置好了，你只需觀看大約 1 分半左右的影片："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2I0FIzMAtt5Z"
   },
   "source": [
    "!youtube\n",
    "- ugWDIIOHtPA\n",
    "- 李宏毅教授講解 multi-head attention 的概念\n",
    "- 1526\n",
    "- 1676"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3-rwmKju3i9"
   },
   "source": [
    "複習完了嗎？mutli-head attention 的概念本身並不難，用比較正式的說法就是將 Q、K 以及 V 這三個張量先**個別**轉換到 *d_model* 維空間，再將其拆成多個比較低維的 *depth* 維度 N 次以後，將這些產生的小 q、小 k 以及小 v 分別丟入前面的注意函式得到 N 個結果。接著將這 N 個 heads 的結果串接起來，最後通過一個線性轉換就能得到 multi-head attention 的輸出\n",
    "\n",
    "而為何要那麼「搞剛」把本來 `d_model` 維的空間投影到多個維度較小的子空間（subspace）以後才各自進行注意力機制呢？這是因為這給予模型更大的彈性，讓它可以同時關注不同位置的子詞在不同子空間下的 representation，而不只是本來 `d_model` 維度下的一個 representation。\n",
    "\n",
    "我們在文章最開頭看過的英翻中就是一個活生生的 mutli-head attention 例子：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXcXl2HmzXyv"
   },
   "source": [
    "!image\n",
    "- transformer/en-to-ch-attention-map.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9RrdL2G2bQ1"
   },
   "source": [
    "在經過[前面 2 節注意函式](#Scaled-dot-product-attention：一種注意函式)的洗禮之後，你應該已經能夠看出這裏每張小圖都是一個注意權重（為了方便渲染我做了 transpose）。而事實上每張小圖都是 multi-head attention 裡頭某一個 head 的結果，總共是 8 個 heads。\n",
    "\n",
    "你會發現每個 head 在 Transformer 生成同樣的中文字時關注的英文子詞有所差異：\n",
    "- Head 4 在生成「們」與「再」時特別關注「renewed」\n",
    "- Head 5 在生成「必」與「須」時特別關注「must」\n",
    "- Head 6 & 8 在生成「希」與「望」時特別關注「hope」\n",
    "\n",
    "透過這樣同時關注多個不同子空間裡頭的子詞的 representation，Transformer 最終可以生成更好的結果。\n",
    "\n",
    "話是這麼說，但程式碼該怎麼寫呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3r-vFmZfB2HR"
   },
   "source": [
    "!image\n",
    "- transformer/multi-head-imagining.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WVj0ON1D4mi"
   },
   "source": [
    "\n",
    "為了要實現 multi-head attention，得先能把一個 head 變成多個 heads。而這實際上就是把一個 `d_model` 維度的向量「折」成 `num_heads` 個 `depth` 維向量，使得：\n",
    "\n",
    "```text\n",
    "num_heads * depth = d_model\n",
    "```\n",
    "\n",
    "讓我們實作一個可以做到這件事情的函式，並將英文詞嵌入張量 `emb_inp` 實際丟進去看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "id": "aIc9xcvsi3yS",
    "outputId": "47d62fef-d277-4f3f-b165-3d1af256da8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[ 0.00695511 -0.03370368 -0.03656032 -0.03336458]\n",
      "  [-0.02707888 -0.03917687 -0.01213828  0.00909697]\n",
      "  [ 0.0355427   0.04111305  0.00751223 -0.01974255]\n",
      "  [ 0.02443342 -0.03273199  0.01267544  0.03127003]\n",
      "  [-0.04879753 -0.00119017 -0.00157104  0.01117355]\n",
      "  [-0.02148524 -0.03413673  0.00708324  0.0121879 ]\n",
      "  [-0.00680635  0.02136201 -0.02036932 -0.04211974]\n",
      "  [-0.00680635  0.02136201 -0.02036932 -0.04211974]]\n",
      "\n",
      " [[ 0.00695511 -0.03370368 -0.03656032 -0.03336458]\n",
      "  [-0.0325227  -0.03433502 -0.01849879  0.01439226]\n",
      "  [ 0.00144588 -0.00377025 -0.00798036 -0.04099905]\n",
      "  [ 0.04524285  0.02524642 -0.00924555 -0.01368124]\n",
      "  [-0.0159062   0.01108797 -0.0177028  -0.0435766 ]\n",
      "  [ 0.00240784 -0.04652226  0.01821991 -0.04349295]\n",
      "  [-0.04879753 -0.00119017 -0.00157104  0.01117355]\n",
      "  [-0.02148524 -0.03413673  0.00708324  0.0121879 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[ 0.00695511 -0.03370368]\n",
      "   [-0.02707888 -0.03917687]\n",
      "   [ 0.0355427   0.04111305]\n",
      "   [ 0.02443342 -0.03273199]\n",
      "   [-0.04879753 -0.00119017]\n",
      "   [-0.02148524 -0.03413673]\n",
      "   [-0.00680635  0.02136201]\n",
      "   [-0.00680635  0.02136201]]\n",
      "\n",
      "  [[-0.03656032 -0.03336458]\n",
      "   [-0.01213828  0.00909697]\n",
      "   [ 0.00751223 -0.01974255]\n",
      "   [ 0.01267544  0.03127003]\n",
      "   [-0.00157104  0.01117355]\n",
      "   [ 0.00708324  0.0121879 ]\n",
      "   [-0.02036932 -0.04211974]\n",
      "   [-0.02036932 -0.04211974]]]\n",
      "\n",
      "\n",
      " [[[ 0.00695511 -0.03370368]\n",
      "   [-0.0325227  -0.03433502]\n",
      "   [ 0.00144588 -0.00377025]\n",
      "   [ 0.04524285  0.02524642]\n",
      "   [-0.0159062   0.01108797]\n",
      "   [ 0.00240784 -0.04652226]\n",
      "   [-0.04879753 -0.00119017]\n",
      "   [-0.02148524 -0.03413673]]\n",
      "\n",
      "  [[-0.03656032 -0.03336458]\n",
      "   [-0.01849879  0.01439226]\n",
      "   [-0.00798036 -0.04099905]\n",
      "   [-0.00924555 -0.01368124]\n",
      "   [-0.0177028  -0.0435766 ]\n",
      "   [ 0.01821991 -0.04349295]\n",
      "   [-0.00157104  0.01117355]\n",
      "   [ 0.00708324  0.0121879 ]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "  # x.shape: (batch_size, seq_len, d_model)\n",
    "  batch_size = tf.shape(x)[0]\n",
    "  \n",
    "  # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "  assert d_model % num_heads == 0\n",
    "  depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "  \n",
    "  # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "  # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "  # (batch_size, seq_len, num_heads, depth)\n",
    "  reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "  \n",
    "  # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "  # (batch_size, num_heads, seq_len, depth)\n",
    "  output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6KqTR90i3iv"
   },
   "source": [
    "觀察 `output` 與 `emb_inp` 之間的關係，你會發現 3 維詞嵌入張量 `emb_inp` 已經被轉換成一個 4 維張量了，且最後一個維度 `shape[-1] = 4` 被拆成兩半。\n",
    "\n",
    "不過如果你不熟 TensorFlow API 或是矩陣運算，或許無法馬上理解 head 的維度在哪裡、還有不同 heads 之間有什麼差異。為了幫助你直觀理解 `split_heads` 函式，我將運算過程中產生的張量都視覺化出來給你瞧瞧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkR1pdArD4Xa"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# def split_heads_demo(x, d_model, num_heads):\n",
    "#   # x.shape: (batch_size, seq_len, d_model)\n",
    "#   batch_size = tf.shape(x)[0]\n",
    "  \n",
    "#   assert d_model % num_heads == 0\n",
    "#   depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "  \n",
    "#   plot(x, name='x', shape_desc='\\n(batch_size, seq_len, d_model)')\n",
    "  \n",
    "#   reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "#   plot(reshaped_x, name='reshaped_x', shape_desc='\\n(batch_size, seq_len, num_heads, depth)',\n",
    "#            single_plot_size=8, h_dist_ratio=0.3, w_dist_ratio=0.3, \n",
    "#        width_prec=1.0 / 2 / 2, bottom_prec=1.0 / 8 * 2, linewidths=2)\n",
    "  \n",
    "#   transposed_x = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "#   plot(transposed_x, name='output', shape_desc='\\n(batch_size, num_heads, seq_len, depth)')\n",
    "  \n",
    "#   return transposed_x\n",
    "\n",
    "# d_model = 4\n",
    "# num_heads = 2\n",
    "\n",
    "# two_heads_emb_inp = split_heads_demo(emb_inp, d_model, num_heads)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bdZl8qmB16W"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/split_heads.mp4\n",
    "- images/transformer/split_heads.jpg\n",
    "- split_heads 函式將 3 維張量轉換為 multi-head 的 4 維張量過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C_m8IFXlUJ4G"
   },
   "source": [
    "觀察 `split_heads` 的輸入輸出，你會發現序列裡每個子詞原來為 `d_model` 維的 reprsentation 被拆成多個相同但較短的 `depth` 維度。而每個 head 的 2 維矩陣事實上仍然代表原來的序列，只是裡頭子詞的 repr. 維度降低了。\n",
    "\n",
    "透過動畫，你現在應該已經能夠了解要產生 multi-head 就是將輸入張量中本來是 `d_model` 的最後一個維度平均地「折」成想要的 head 數，進而產生一個新的 head 維度。一個句子裡頭的子詞現在不只會有一個 `d_model` 的 repr.，而是會有 `num_heads` 個 `depth` 維度的 representation。\n",
    "\n",
    "接下來只要把 3 維的 Q、K 以及 V 用 `split_heads` 拆成多個 heads 的 4 維張量，利用 broadcasting 就能以之前定義的[ Scaled dot product attention](#Scaled-dot-product-attention：一種注意函式) 來為每個句子裡頭的每個 head 平行計算注意結果了，超有效率！\n",
    "\n",
    "在明白如何產生 multi-head 的 4 維張量以後，multi-head attention 的實現就比較容易理解了：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k70_HssPRcXo"
   },
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "  \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T14XYyH3MvNf"
   },
   "source": [
    "是的，就算你有自己實作過 keras layer，multi-head attention layer 也不算短的實作。如果這是你第一次碰到客製化的 keras layer，別打退堂鼓，你可以多看幾次我寫給你的註解，或是參考等等下方的動畫來加深對 multi-head attention 的理解。\n",
    "\n",
    "`split_heads` 函式我們在前面就已經看過了，你應該還有印象。`call` 函式則定義了這個 multi-head attention layer 實際的計算流程，而這流程跟我在本節開頭講的可以說是有 87% 相似：\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw9dCJsyMu4v"
   },
   "source": [
    "!quote\n",
    " - 將 Q、K 以及 V 這三個張量先個別轉換到 d_model 維空間，再將其拆成多個比較低維的 depth 維度 N 次以後，將這些產生的小 q、小 k 以及小 v 分別丟入前面的注意函式得到 N 個結果。接著將這 N 個 heads 的結果串接起來，最後通過一個線性轉換就能得到 multi-head attention 的輸出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxCtDrU8oPx1"
   },
   "source": [
    "差別只在於實際上我們是利用矩陣運算以及 broadcasting 讓 GPU 一次計算整個 batch 裡所有句子的所有 head 的注意結果。\n",
    "\n",
    "定義了一個新 layer 當然要實際試試。現在讓我們初始一個 multi-head attention layer 並將英文詞嵌入向量 `emb_inp` 輸入進去看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anZJCmtRKTr0"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "class MultiHeadAttentionDemo(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttentionDemo, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "  \n",
    "  def split_heads(self, x, batch_size, draw=False, only_draw_reshape=False):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    if draw:\n",
    "      plot(x, name='tf.reshape in split_heads', shape_desc='\\n(batch_size, seq_len, num_heads, depth)',\n",
    "           single_plot_size=8, h_dist_ratio=0.3, w_dist_ratio=0.3, width_prec=1.0 / 2 / 2, bottom_prec=1.0 / 8 * 2)\n",
    "      \n",
    "    x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    if draw and not only_draw_reshape:\n",
    "      plot(x, name='tf.transpose in split_heads', shape_desc='\\n(batch-size, num_heads, seq_len, depth)')\n",
    "    return x\n",
    "  \n",
    "  def call(self, v, k, q, mask, draw=False, only_draw_reshape=False):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    shape_desc = '\\n(batch_size, seq_len, d_model)'\n",
    "    \n",
    "    if draw and not only_draw_reshape:\n",
    "      plot(q, name=\"q\", shape_desc=shape_desc)\n",
    "      plot(k, name=\"k\", shape_desc=shape_desc)\n",
    "      plot(v, name=\"v\", shape_desc=shape_desc)\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    if draw and not only_draw_reshape:\n",
    "      plot(q, name=\"q\", shape_desc=shape_desc)\n",
    "      plot(k, name=\"k\", shape_desc=shape_desc)\n",
    "      plot(v, name=\"v\", shape_desc=shape_desc)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size, draw)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size, draw)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size, draw)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    if only_draw_reshape:\n",
    "      scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "          q, k, v, mask)\n",
    "    else:\n",
    "      scaled_attention, attention_weights = scaled_dot_product_attention_demo(\n",
    "          q, k, v, mask)\n",
    "      \n",
    "    if draw and not only_draw_reshape:\n",
    "      plot(scaled_attention, name='scaled_attention', shape_desc='\\n(batch_size, num_heads, seq_len, depth)')\n",
    "      plot(attention_weights, name='attention_weights', shape_desc='\\n(batch_size, num_heads, seq_len_q, seq_len_k)')\n",
    "    \n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "    if draw:\n",
    "      plot(scaled_attention, name='scaled_attention after transpose', \n",
    "           shape_desc='\\n(batch_size, seq_len_q, num_heads, depth)',\n",
    "           single_plot_size=8, h_dist_ratio=0.3, w_dist_ratio=0.3, width_prec=1.0 / 2 / 2, bottom_prec=1.0 / 8 * 2)\n",
    "    \n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    if draw and not only_draw_reshape:\n",
    "      plot(concat_attention, name='concat_attention', shape_desc='\\n(batch_size, seq_len_q, d_model)')\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "    if draw and not only_draw_reshape:\n",
    "      plot(output, name='output', shape_desc='\\n(batch-size, seq_len_q, d_model)')\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGOOmLIDJbDW"
   },
   "source": [
    "\n",
    "```python\n",
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model == emb_inp.shape[-1]  == 4\n",
    "num_heads = 2\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask.shape)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)\n",
    "print(\"output.shape:\", output.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "\n",
    "print(\"\\noutput:\", output)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "xQ2kNYtKqjMt",
    "outputId": "be3c53eb-b344-4884-fe5f-a2b90b1cbfd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 4\n",
      "num_heads: 2\n",
      "\n",
      "q.shape: (2, 8, 4)\n",
      "k.shape: (2, 8, 4)\n",
      "v.shape: (2, 8, 4)\n",
      "padding_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: (2, 8, 4)\n",
      "attention_weights.shape: (2, 2, 8, 8)\n",
      "\n",
      "output: tf.Tensor(\n",
      "[[[ 0.00862424  0.00463534  0.00123856  0.01982255]\n",
      "  [ 0.00860434  0.00464583  0.00125165  0.01984711]\n",
      "  [ 0.00863869  0.00461318  0.00122942  0.01981261]\n",
      "  [ 0.00858585  0.00465442  0.00125683  0.0198578 ]\n",
      "  [ 0.0086211   0.00462923  0.0012448   0.01983759]\n",
      "  [ 0.00860078  0.00464716  0.00125472  0.01985404]\n",
      "  [ 0.00865074  0.00461071  0.00122681  0.01980557]\n",
      "  [ 0.00865074  0.00461071  0.00122681  0.01980557]]\n",
      "\n",
      " [[-0.00233657  0.02963993  0.01171194  0.03959805]\n",
      "  [-0.00234752  0.02964369  0.01171828  0.03960991]\n",
      "  [-0.00232748  0.02962957  0.01170804  0.03959192]\n",
      "  [-0.00233163  0.02963142  0.0117076   0.03959151]\n",
      "  [-0.00231678  0.02962143  0.01170276  0.03957902]\n",
      "  [-0.00234718  0.02964409  0.01171941  0.03961902]\n",
      "  [-0.00233476  0.029631    0.01171241  0.03959794]\n",
      "  [-0.00235306  0.02964601  0.01172148  0.03961948]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "# 讀取之前建立的 mha 來保證每次的 mha 結果都一樣\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "mha_weights_dir = os.path.join(output_dir, \"demo_mha\")\n",
    "weight_names = [str(i) for i in range(8)]\n",
    "\n",
    "mha = MultiHeadAttentionDemo(d_model, num_heads)\n",
    "_, _ = mha(emb_inp, emb_inp, emb_inp, None)\n",
    "\n",
    "if not os.path.exists(mha_weights_dir):\n",
    "  os.makedirs(mha_weights_dir)\n",
    "\n",
    "if not os.path.exists(os.path.join(mha_weights_dir, weight_names[0] + '.npy')):\n",
    "  weights = mha.get_weights()\n",
    "  for name, weight in zip(weight_names, weights):\n",
    "    np.save(os.path.join(mha_weights_dir, name + '.npy'), weight)\n",
    "else:\n",
    "  weights = [np.load(os.path.join(mha_weights_dir, name + '.npy')) for name in weight_names]\n",
    "  mha.set_weights(weights)\n",
    "\n",
    "num_heads = 2\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask.shape)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)\n",
    "print(\"output.shape:\", output.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "\n",
    "print(\"\\noutput:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2K6YPnOH0tJK"
   },
   "source": [
    "你現在應該明白為何[我們當初要在 padding mask 加入兩個新維度了](#遮罩：Transformer-的祕密配方)：一個是用來遮住同個句子但是不同 head 的注意權重，一個則是用來 broadcast 到 2 維注意權重的（詳見[直觀理解遮罩](#直觀理解遮罩在注意函式中的效果)一節）。\n",
    "\n",
    "沒意外的話你也已經能夠解讀 mutli-head attention 的輸出了：\n",
    "- `output`：序列中每個子詞的新 repr. 都包含同序列其他位置的資訊\n",
    "- `attention_weights`：包含每個 head 的每個序列 `q` 中的字詞對序列 `k` 的注意權重\n",
    "\n",
    "如果你還無法想像每個計算步驟，讓我們看看 multi-head attention 是怎麼將輸入的 `q`、`k` 以及 `v` 轉換成最後的 `output` 的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "we0wA7GL8jja"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/multi-head-attention.mp4\n",
    "- images/transformer/multi-head-attention.jpg\n",
    "- Multi-head attention 完整計算過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Sb5GJmyKwVW"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# output, attention_weights = mha(v, k, q, padding_mask, draw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujz_qDY30rQq"
   },
   "source": [
    "這應該是你這輩子第一次也可能是唯一一次有機會看到 multi-head 注意力機制是怎麼處理 4 維張量的。\n",
    "\n",
    "細節不少，我建議將動畫跟程式碼比較一下，確保你能想像每一個步驟產生的張量以及其物理意義。到此為止，我們已經把 Transformer 裡最困難的 multi-head attention 的概念以及運算都看過一遍了。\n",
    "\n",
    "如果你腦袋還是一團亂，只要記得最後一個畫面：在 `q`、`k` 以及 `v` 的最後一維已經是 `d_model` 的情況下，multi-head attention 跟 scaled dot product attention 一樣，就是吐出一個完全一樣維度的張量 `output`。\n",
    "\n",
    "multi-head attention 的輸出張量 `output` 裡頭每個句子的每個字詞的 repr. 維度 `d_model` 雖然跟函式的輸入張量相同，但實際上已經是從同個序列中**不同位置且不同空間**中的 repr. 取得語義資訊的結果。\n",
    "\n",
    "要確保自己真的掌握了 multi-head attention 的精神，你可以試著向旁邊的朋友（如果他 / 她願意聽的話）解釋一下整個流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUU3s3GcKwGZ"
   },
   "source": [
    "!image\n",
    "- transformer/explain-mha.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqArpEWGDpiu"
   },
   "source": [
    " 喔對了，不用擔心我們做 multi-head 以後計算量會大增。因為 head 的數目雖然變多了，每個子空間的維度也下降了。跟 single-head attention 使用的計算量是差不多的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_Wt4tN2FRXh"
   },
   "source": [
    "## 打造 Transformer：疊疊樂時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAFkwrS5GXDB"
   },
   "source": [
    "[以前我們曾提到](https://leemeng.tw/deep-learning-resources.html)深度學習模型就是一層層的幾何運算過程。Transformer 也不例外，剛才實作的 mutli-head attention layer 就是一個最明顯的例子。而它正好是 Transformer 裡頭最重要的一層運算。\n",
    "\n",
    "在這節我們會把 Transformer 裡頭除了注意力機制的其他運算通通實作成一個個的 layers，並將它們全部「疊」起來。\n",
    "\n",
    "你可以點擊下方的影片來了解接下來的實作順序："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K74C9KA_GWzp"
   },
   "source": [
    "!mp4\n",
    "- options: controls, no-loop, no-autoplay\n",
    "- images/transformer/steps-to-build-transformer.mp4\n",
    "- images/transformer/steps-to-build-transformer.jpg\n",
    "- 一步步打造 Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-NX7T87OaMf"
   },
   "source": [
    "如果這是你第一次看到 Transformer 的架構圖 ... 代表你沒認真上教授的課，等等別忘記[去前面領補課號碼牌](#師傅引進門，修行在個人_1)。\n",
    "\n",
    "影片中左側就是我們接下來會依序實作的 layers。[Transformer 是一種使用自注意力機制的 Seq2Seq 模型](#Transformer：Seq2Seq-模型-+-自注意力機制) ，裡頭包含了兩個重要角色，分別為 Encoder 與 Decoder：\n",
    "- 最初輸入的英文序列會通過 Encoder 中 N 個 Encoder layers 並被轉換成一個**相同長度**的序列。每個 layer 都會為自己的輸入序列裡頭的子詞產生**新的** repr.，然後交給下一個 layer。\n",
    "- Decoder 在生成（預測）下一個中文子詞時會一邊觀察 Encoder 輸出序列裡**所有**英文子詞的 repr.，一邊觀察自己前面已經生成的中文子詞。\n",
    "\n",
    "值得一提的是，N = 1 （Encoder / Decoder layer 數目 = 1）時就是最陽春版的 Transformer。但在深度學習領域裡頭我們常常想對原始數據做多層的轉換，因此會將 N 設為影片最後出現的 2 層或是 Transformer 論文中的 6 層 Encoder / Decoder layers。\n",
    "\n",
    "Encoder 裡頭的 Encoder layer 裡又分兩個 sub-layers，而 Decoder 底下的 Decoder layer 則包含 3 個 sub-layers。真的是 layer layer 相扣。將這些 layers 的階層關係簡單列出來大概就長這樣（位置 Encoding 等實作時解釋）：\n",
    "\n",
    "- Transformer\n",
    "  - Encoder\n",
    "    - 輸入 Embedding\n",
    "    - 位置 Encoding\n",
    "    - N 個 Encoder layers\n",
    "      - sub-layer 1: Encoder 自注意力機制\n",
    "      - sub-layer 2: Feed Forward\n",
    "  - Decoder\n",
    "    - 輸出 Embedding\n",
    "    - 位置 Encoding\n",
    "    - N 個 Decoder layers\n",
    "      - sub-layer 1: Decoder 自注意力機制\n",
    "      - sub-layer 2: Decoder-Encoder 注意力機制\n",
    "      - sub-layer 3: Feed Forward\n",
    "  - Final Dense Layer\n",
    "\n",
    "不過就像影片中顯示的一樣，實作的時候我們傾向從下往上疊上去。畢竟地基打得好，樓才蓋得高，對吧？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWUFOI96GWm_"
   },
   "source": [
    "### Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKtVFbz2rPt8"
   },
   "source": [
    "如同影片中所看到的， Encoder layer 跟 Decoder layer 裡頭都各自有一個 Feed Forward 的元件。此元件構造簡單，不用像前面的 multi-head attention 建立[客製化的 keras layer](https://www.tensorflow.org/beta/tutorials/eager/custom_layers)，只需要寫一個 Python 函式讓它在被呼叫的時候回傳一個**新的** [tf.keras.Sequential 模型](https://www.tensorflow.org/beta/tutorials/quickstart/beginner)給我們即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xpmlQEntNEL"
   },
   "source": [
    "此函式在每次被呼叫的時候都會回傳一組新的全連接前饋神經網路（Fully-connected **F**eed **F**orward **N**etwork，FFN），其輸入張量與輸出張量的最後一個維度皆為 `d_model`，而在 FFN 中間層的維度則為 `dff`。一般會讓 `dff` 大於 `d_model`，讓 FFN 從輸入的 `d_model` 維度裡頭擷取些有用的資訊。在論文中 `d_model` 為 512，`dff` 則為 4 倍的 2048。兩個都是可以調整的超參數。\n",
    "\n",
    "讓我們建立一個 FFN 試試："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mytb1lPyOHLB",
    "outputId": "98304b35-8e7f-408c-f200-a766b1ecf1ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 10, 512)\n",
      "out.shape: (64, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "out = ffn(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZAnyn1l8nRA"
   },
   "source": [
    "在輸入張量的最後一維已經是 `d_model` 的情況，FFN 的輸出張量基本上會跟輸入一模一樣：\n",
    "- 輸入：（batch_size, seq_len, d_model）\n",
    "- 輸出：（batch_size, seq_len, d_model） \n",
    "\n",
    "FFN 輸出 / 輸入張量的 shape 相同很容易理解。比較沒那麼明顯的是這個 FFN  事實上對序列中的所有位置做的線性轉換都是一樣的。我們可以假想一個 2 維的 `duumy_sentence`，裡頭有 5 個以 4 維向量表示的子詞："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "-ojhRi1_4uQ4",
    "outputId": "7380143b-6f71-48b5-be95-5154168c4c76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=193585, shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 2.8674245, -2.174698 , -1.3073452, -6.4233937],\n",
       "       [ 2.8674245, -2.174698 , -1.3073452, -6.4233937],\n",
       "       [ 3.650207 , -0.973258 , -2.4126565, -6.5094995],\n",
       "       [ 3.650207 , -0.973258 , -2.4126565, -6.5094995],\n",
       "       [ 3.650207 , -0.973258 , -2.4126565, -6.5094995]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model`\n",
    "dff = 6\n",
    "\n",
    "# 建立一個小 FFN\n",
    "small_ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "# 懂子詞梗的站出來\n",
    "dummy_sentence = tf.constant([[5, 5, 6, 6], \n",
    "                              [5, 5, 6, 6], \n",
    "                              [9, 5, 2, 7], \n",
    "                              [9, 5, 2, 7],\n",
    "                              [9, 5, 2, 7]], dtype=tf.float32)\n",
    "small_ffn(dummy_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCRCrWX26l7s"
   },
   "source": [
    "你會發現同一個子詞不會因為**位置的改變**而造成 FFN 的輸出結果產生差異。但因為我們實際上會有多個 Encoder / Decoder layers，而每個 layers 都會有不同參數的 FFN，因此每個 layer 裡頭的 FFN 做的轉換都會有所不同。\n",
    "\n",
    "值得一提的是，儘管對所有位置的子詞都做一樣的轉換，這個轉換是獨立進行的，因此被稱作 Position-wise Feed-Forward Networks。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S4DTE7SQm5g"
   },
   "source": [
    "### Encoder layer：Encoder 小弟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPsE8NLsWAyx"
   },
   "source": [
    "有了 **M**ulti-**H**ead **A**ttention（MHA）以及 **F**eed-**F**orward **N**etwork（FFN），我們事實上已經可以實作第一個 Encoder layer 了。讓我們複習一下這 layer 裡頭有什麼重要元件："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxgWUHOmWAgX"
   },
   "source": [
    "!mp4\n",
    "- images/transformer/encoder-layer.mp4\n",
    "- images/transformer/encoder-layer.jpg\n",
    "- Encoder layer 裡的重要元件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvkoZXaYu72y"
   },
   "source": [
    "我想上面的動畫已經很清楚了。一個 Encoder layer 裡頭會有兩個 sub-layers，分別為 MHA 以及 FFN。在 Add & Norm 步驟裡頭，每個 sub-layer 會有一個[殘差連結（residual connection）](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)來幫助減緩梯度消失（Gradient Vanishing）的問題。接著兩個 sub-layers 都會針對最後一維 `d_model` 做 [layer normalization](https://arxiv.org/abs/1607.06450)，將 batch 裡頭每個子詞的輸出獨立做轉換，使其平均與標準差分別靠近 0 和 1 之後輸出。\n",
    "\n",
    "另外在將 sub-layer 的輸出與其輸入相加之前，我們還會做點 regularization，對該 sub-layer 的輸出使用 [dropout](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)。\n",
    "\n",
    "總結一下。如果輸入是 `x`，最後輸出寫作 `out` 的話，則每個 sub-layer 的處理邏輯如下：\n",
    "\n",
    "```text\n",
    "sub_layer_out = Sublayer(x)\n",
    "sub_layer_out = Dropout(sub_layer_out)\n",
    "out = LayerNorm(x + sub_layer_out)\n",
    "```\n",
    "\n",
    "`Sublayer` 則可以是 MHA 或是 FFN。現在讓我們看看 Encoder layer 的實作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ii9Jnl9mQmq1"
   },
   "outputs": [],
   "source": [
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  # Transformer 論文內預設 dropout rate 為 0.1\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 一樣，一個 sub-layer 一個 dropout layer\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n",
    "  def call(self, x, training, mask):\n",
    "    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n",
    "    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n",
    "    \n",
    "    # sub-layer 1: MHA\n",
    "    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "    attn_output, attn = self.mha(x, x, x, mask)  \n",
    "    attn_output = self.dropout1(attn_output, training=training) \n",
    "    out1 = self.layernorm1(x + attn_output)  \n",
    "    \n",
    "    # sub-layer 2: FFN\n",
    "    ffn_output = self.ffn(out1) \n",
    "    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_V2ExV3z_ql"
   },
   "source": [
    "跟當初 MHA layer 的實作比起來輕鬆多了，對吧？\n",
    "\n",
    "基本上 Encoder layer 裡頭就是兩個架構一模一樣的 sub-layer，只差在一個是 MHA，一個是 FFN。另外為了方便 residual connection 的計算，所有 sub-layers 的**輸出**維度都是 `d_model`。而 sub-layer 內部產生的維度當然就隨我們開心啦！我們可以為 FFN 設置不同的 `dff` 值，也能設定不同的 `num_heads` 來改變 MHA 內部每個 head 裡頭的維度。\n",
    "\n",
    "\n",
    "論文裡頭的 `d_model` 為 512，而我們 demo 用的英文詞嵌入張量的 `d_model` 維度則為 4："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "C3eIn6501vor",
    "outputId": "10fb1a7a-d647-49c0-e2b0-1baf4b196060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "emb_inp: tf.Tensor(\n",
      "[[[ 0.00695511 -0.03370368 -0.03656032 -0.03336458]\n",
      "  [-0.02707888 -0.03917687 -0.01213828  0.00909697]\n",
      "  [ 0.0355427   0.04111305  0.00751223 -0.01974255]\n",
      "  [ 0.02443342 -0.03273199  0.01267544  0.03127003]\n",
      "  [-0.04879753 -0.00119017 -0.00157104  0.01117355]\n",
      "  [-0.02148524 -0.03413673  0.00708324  0.0121879 ]\n",
      "  [-0.00680635  0.02136201 -0.02036932 -0.04211974]\n",
      "  [-0.00680635  0.02136201 -0.02036932 -0.04211974]]\n",
      "\n",
      " [[ 0.00695511 -0.03370368 -0.03656032 -0.03336458]\n",
      "  [-0.0325227  -0.03433502 -0.01849879  0.01439226]\n",
      "  [ 0.00144588 -0.00377025 -0.00798036 -0.04099905]\n",
      "  [ 0.04524285  0.02524642 -0.00924555 -0.01368124]\n",
      "  [-0.0159062   0.01108797 -0.0177028  -0.0435766 ]\n",
      "  [ 0.00240784 -0.04652226  0.01821991 -0.04349295]\n",
      "  [-0.04879753 -0.00119017 -0.00157104  0.01117355]\n",
      "  [-0.02148524 -0.03413673  0.00708324  0.0121879 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[ 1.2521563   0.3273945  -1.5237452  -0.0558054 ]\n",
      "  [-1.0591918  -0.42765176 -0.14816867  1.6350121 ]\n",
      "  [ 0.299005    1.3632457  -1.4101827  -0.252068  ]\n",
      "  [ 0.7023785  -1.479373   -0.32433346  1.1013279 ]\n",
      "  [-1.6220697   1.0153029   0.02592759  0.5808392 ]\n",
      "  [-1.0757908  -0.7200314   0.30136684  1.4944555 ]\n",
      "  [-0.22072682  1.5675467  -1.215218   -0.1316019 ]\n",
      "  [-0.22072682  1.5675467  -1.215218   -0.1316019 ]]\n",
      "\n",
      " [[ 1.475371    0.30539253 -1.1591307  -0.6216327 ]\n",
      "  [-1.4569639   0.00421676  0.08528362  1.3674635 ]\n",
      "  [ 0.61611307  1.3085197  -0.79488575 -1.1297472 ]\n",
      "  [ 0.80156547  0.9995991  -1.5072922  -0.29387245]\n",
      "  [-0.11611538  1.6353902  -1.0406278  -0.47864679]\n",
      "  [ 0.9602699  -0.3459822   0.8696089  -1.4838965 ]\n",
      "  [-1.6676238   0.9936579   0.2892594   0.38470644]\n",
      "  [-1.2698565  -0.67637944  1.1073651   0.8388707 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 之後可以調的超參數。這邊為了 demo 設小一點\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "enc_layer = EncoderLayer(d_model, num_heads, dff)\n",
    "padding_mask = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out = enc_layer(emb_inp, training=False, mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"padding_mask:\", padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "assert emb_inp.shape == enc_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_BmM4JB35bJ"
   },
   "source": [
    "在本來的輸入維度即為 `d_model` 的情況下，Encoder layer 就是給我們一個一模一樣 shape 的張量。當然，實際上內部透過 MHA 以及 FFN sub-layer 的轉換，每個子詞的 repr. 都大幅改變了。\n",
    "\n",
    "有了 Encoder layer，接著讓我們看看 Decoder layer 的實作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xps5bjZb4jJi"
   },
   "source": [
    "### Decoder layer：Decoder 小弟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwuBzpfSPkiu"
   },
   "source": [
    "一個 Decoder layer 裡頭有 3 個 sub-layers：\n",
    "1. Decoder layer 自身的 **Masked** MHA 1\n",
    "2. Decoder layer 關注 Encoder 輸出序列的 MHA 2\n",
    "3. FFN\n",
    "\n",
    "你也可以看一下影片來回顧它們所在的位置："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9WG7SGrYyERm"
   },
   "source": [
    "!mp4\n",
    "- images/transformer/decoder-layer.mp4\n",
    "- images/transformer/decoder-layer.jpg\n",
    "- Decoder layer 中的 sub-layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAmGJzOvVUMw"
   },
   "source": [
    "\n",
    "\n",
    "跟實作 Encoder layer 時一樣，每個 sub-layer 的邏輯同下：\n",
    "\n",
    "```text\n",
    "sub_layer_out = Sublayer(x)\n",
    "sub_layer_out = Dropout(sub_layer_out)\n",
    "out = LayerNorm(x + sub_layer_out)\n",
    "```\n",
    "\n",
    "Decoder layer 用 MHA 1 來關注輸出序列，查詢 Q、鍵值 K 以及值 V 都是自己。而之所以有個 masked 是因為（中文）輸出序列除了跟（英文）輸入序列一樣需要 padding mask 以外，還需要 look ahead mask 來避免 Decoder layer 關注到未來的子詞。look ahead mask 在[前面章節](#直觀理解遮罩在注意函式中的效果)已經有詳細說明了。\n",
    "\n",
    "MHA1 處理完的輸出序列會成為 MHA 2 的 Q，而 K 與 V 則使用 Encoder 的輸出序列。這個運算的概念是讓一個 Decoder layer 在生成新的中文子詞時先參考先前已經產生的中文字，並為當下要生成的子詞產生一個包含前文語義的 repr. 。接著將此 repr. 拿去跟 Encoder 那邊的英文序列做匹配，看當下字詞的 repr. 有多好並予以修正。\n",
    "\n",
    "\n",
    "用簡單點的說法就是 Decoder 在生成中文字詞時除了參考已經生成的中文字以外，也會去關注 Encoder 輸出的英文子詞（的 repr.）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S90dXcKGiLna"
   },
   "outputs": [],
   "source": [
    "# Decoder 裡頭會有 N 個 DecoderLayer，\n",
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    # 3 個 sub-layers 的主角們\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    # 定義每個 sub-layer 用的 LayerNorm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 定義每個 sub-layer 用的 Dropout\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "    # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "    # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "    # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "    # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "    # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "    # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "    # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNbu5xK8hcWH"
   },
   "source": [
    "Decoder layer 的實作跟 Encoder layer 大同小異，不過還是有幾點細節特別需要注意：\n",
    "- 在做 Masked MHA（MHA 1）的時候我們需要同時套用兩種遮罩：**輸出**序列的 padding mask 以及 look ahead mask。因此 Decoder layer 預期的遮罩是兩者結合的 `combined_mask`\n",
    "- MHA 1 因為是 Decoder layer 關注自己，multi-head attention 的參數 `v`、`k` 以及 `q` 都是 `x`\n",
    "- MHA 2 是 Decoder layer 關注 Encoder 輸出序列，因此，multi-head attention 的參數 `v`、`k` 為 `enc_output`，`q` 則為 MHA 1 sub-layer 的結果 `out1`\n",
    "\n",
    "產生 `comined_mask` 也很簡單，我們只要把兩個遮罩取大的即可：\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "jBfYm4PGmwZV",
    "outputId": "e4c2514c-6e0b-4cc9-d4ea-2aba5423d259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_padding_mask:\", tar_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask:\", look_ahead_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZzBufM8nzN5"
   },
   "source": [
    "注意 `combined_mask` 的 shape 以及裡頭遮罩所在的位置。利用 broadcasting 我們將 `combined_mask` 的 shape 也擴充到 4 維：\n",
    "\n",
    "```text\n",
    "(batch_size, num_heads, seq_len_tar, seq_len_tar)\n",
    "= (2, 1, 10, 10)\n",
    "```\n",
    "\n",
    "這方便之後 multi-head attention 的計算。另外因為我們 demo 的中文 batch 裡頭的第一個句子有 `<pad>`，`combined_mask` 除了 look ahead 的效果以外還加了 padding mask。\n",
    "\n",
    "因為剛剛實作的是 Decoder layer，這次讓我們把中文（目標語言）的詞嵌入張量以及相關的遮罩丟進去看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1156
    },
    "colab_type": "code",
    "id": "mwn3U8MtiLMX",
    "outputId": "70f63a8e-4ba6-464d-e183-cbe3810cb867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[-0.0441955  -0.01026772  0.03740635  0.02017349]\n",
      "  [ 0.02129837 -0.00746276  0.03881821 -0.01586295]\n",
      "  [-0.01179456  0.02825376  0.00738146  0.02963744]\n",
      "  [ 0.01171205  0.04350302 -0.01190796  0.02526634]\n",
      "  [ 0.03814722 -0.03364048 -0.03744673  0.04369817]\n",
      "  [ 0.0280853   0.01269842  0.04268574 -0.04069148]\n",
      "  [ 0.04029209 -0.00619308 -0.04934603  0.02242902]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]\n",
      "  [-0.00285894  0.02392108 -0.03126474  0.01345349]]\n",
      "\n",
      " [[-0.0441955  -0.01026772  0.03740635  0.02017349]\n",
      "  [-0.00359621 -0.01380367 -0.02875998 -0.03855735]\n",
      "  [ 0.04516688 -0.04480755 -0.03278694 -0.0093614 ]\n",
      "  [ 0.04131394 -0.04065727 -0.04330624 -0.03341667]\n",
      "  [ 0.03572228 -0.04500845  0.0470326   0.03095007]\n",
      "  [-0.03566641 -0.03730996 -0.00597564 -0.03933349]\n",
      "  [ 0.01850356  0.03993076  0.02729526 -0.04848848]\n",
      "  [-0.02294568 -0.02494572 -0.0136737  -0.04278342]\n",
      "  [ 0.0280853   0.01269842  0.04268574 -0.04069148]\n",
      "  [ 0.04029209 -0.00619308 -0.04934603  0.02242902]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[ 1.2521563   0.3273945  -1.5237452  -0.0558054 ]\n",
      "  [-1.0591918  -0.42765176 -0.14816867  1.6350121 ]\n",
      "  [ 0.299005    1.3632457  -1.4101827  -0.252068  ]\n",
      "  [ 0.7023785  -1.479373   -0.32433346  1.1013279 ]\n",
      "  [-1.6220697   1.0153029   0.02592759  0.5808392 ]\n",
      "  [-1.0757908  -0.7200314   0.30136684  1.4944555 ]\n",
      "  [-0.22072682  1.5675467  -1.215218   -0.1316019 ]\n",
      "  [-0.22072682  1.5675467  -1.215218   -0.1316019 ]]\n",
      "\n",
      " [[ 1.475371    0.30539253 -1.1591307  -0.6216327 ]\n",
      "  [-1.4569639   0.00421676  0.08528362  1.3674635 ]\n",
      "  [ 0.61611307  1.3085197  -0.79488575 -1.1297472 ]\n",
      "  [ 0.80156547  0.9995991  -1.5072922  -0.29387245]\n",
      "  [-0.11611538  1.6353902  -1.0406278  -0.47864679]\n",
      "  [ 0.9602699  -0.3459822   0.8696089  -1.4838965 ]\n",
      "  [-1.6676238   0.9936579   0.2892594   0.38470644]\n",
      "  [-1.2698565  -0.67637944  1.1073651   0.8388707 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[-0.4073423  -1.3681166   0.4482983   1.3271605 ]\n",
      "  [ 0.9023904  -1.6660724   0.1386456   0.6250363 ]\n",
      "  [-0.68705463  0.04485544 -0.9672582   1.6094574 ]\n",
      "  [ 0.40446007  0.7378753  -1.7199682   0.5776328 ]\n",
      "  [ 0.66626793 -0.7429294  -1.1866593   1.2633208 ]\n",
      "  [ 1.3847514   0.0595071  -0.00241444 -1.441844  ]\n",
      "  [ 0.77179515 -0.15832207 -1.5698854   0.9564123 ]\n",
      "  [ 0.19740774  0.9835156  -1.6620107   0.4810872 ]\n",
      "  [ 0.19740774  0.9835156  -1.6620107   0.4810872 ]\n",
      "  [ 0.19740774  0.9835156  -1.6620107   0.4810872 ]]\n",
      "\n",
      " [[-0.35176337 -1.3861214   0.39734656  1.3405383 ]\n",
      "  [ 1.0155624   0.28156188 -1.6605129   0.36338854]\n",
      "  [ 0.9295503  -0.96635836 -1.0307404   1.0675484 ]\n",
      "  [ 1.2389433  -0.7855455  -1.1608163   0.70741844]\n",
      "  [ 0.11645091 -1.565496    0.23167732  1.2173678 ]\n",
      "  [-0.44791234 -1.3678643   1.2819183   0.53385824]\n",
      "  [-0.05676413  0.90384555  0.7641177  -1.611199  ]\n",
      "  [-0.4362856  -1.3157362   1.397403    0.35461882]\n",
      "  [ 0.21431251 -0.8140781   1.5471766  -0.94741106]\n",
      "  [ 0.4220932  -0.4875322  -1.3055642   1.3710032 ]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_self_attn_weights.shape: (2, 2, 10, 10)\n",
      "dec_enc_attn_weights: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "dec_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "dec_out, dec_self_attn_weights, dec_enc_attn_weights = dec_layer(\n",
    "    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_out:\", dec_out)\n",
    "assert emb_tar.shape == dec_out.shape\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_self_attn_weights.shape:\", dec_self_attn_weights.shape)\n",
    "print(\"dec_enc_attn_weights:\", dec_enc_attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6l9hirnP4jZ0"
   },
   "source": [
    "跟 Encoder layer 相同，Decoder layer 輸出張量的最後一維也是 `d_model`。而 `dec_self_attn_weights` 則代表著 Decoder layer 的自注意權重，因此最後兩個維度皆為中文序列的長度 `10`；而 `dec_enc_attn_weights` 因為 Encoder 輸出序列的長度為 `8`，最後一維即爲 `8`。\n",
    "\n",
    "都讀到這裡了，判斷每一維的物理意義對你來說應該是小菜一碟了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBQuibYA4n0n"
   },
   "source": [
    "### Positional encoding：神奇數字\n",
    "\n",
    "透過多層的自注意力層，Transformer 在處理序列時裡頭所有子詞都是「天涯若比鄰」：想要關注序列中**任何**位置的資訊只要 O(1) 就能辦到。這讓 Transformer 能很好地 model 序列中長距離的依賴關係（long-range dependencise）。但反過來說 Transformer 則無法 model 序列中字詞的順序關係，所以我們得額外加入一些「位置資訊」給 Transformer。\n",
    "\n",
    "這個資訊被稱作位置編碼（Positional Encoding），實作上是直接加到最一開始的英文 / 中文詞嵌入向量（word embedding）裡頭。其直觀的想法是想辦法讓被加入位置編碼的 word embedding 在 `d_model` 維度的空間裡頭不只會因為**語義相近**而靠近，也會因為**位置靠近**而在該空間裡頭靠近。\n",
    "\n",
    "論文裡頭使用的位置編碼的公式如下：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahGau0MK2qgP"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/position-encoding-equation.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrOpeI5wBAMv"
   },
   "source": [
    "嗯 ... 第一次看到這函式的人會黑人問號是很正常。\n",
    "\n",
    "[論文裡頭提到](https://arxiv.org/pdf/1706.03762.pdf)他們之所以這樣設計位置編碼（**P**ositional **E**ncoding, PE）是因為這個函數有個很好的特性：給定任一位置 `pos` 的位置編碼 `PE(pos)`，跟它距離 `k` 個單位的位置 `pos + k` 的位置編碼 `PE(pos + k)` 可以表示為 `PE(pos)` 的一個線性函數（linear function）。\n",
    "\n",
    "因此透過在 word embedding 裡加入這樣的資訊，作者們認為可以幫助 Transformer 學會 model 序列中的子詞的相對位置關係。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJSSsPM8BDW-"
   },
   "source": [
    "!quote\n",
    "- 子曰：「由！誨女知之乎？知之為知之，不知為不知，是知也。」\n",
    "- 《論語 為政篇》\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6UfHFTRDv4J"
   },
   "source": [
    "就算我們無法自己想出論文裡頭的位置編碼公式，還是可以直接把 [TensorFlow 官方](https://www.tensorflow.org/beta/tutorials/text/transformer#positional_encoding)的實作搬過來使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "1Rz82wEs5biZ",
    "outputId": "90fa2667-0eee-4176-ebaa-33c4f6f461c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=194541, shape=(1, 50, 512), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.84147096,  0.8218562 ,  0.8019618 , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.9092974 ,  0.9364147 ,  0.95814437, ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        ...,\n",
       "        [ 0.12357312,  0.97718984, -0.24295525, ...,  0.9999863 ,\n",
       "          0.99998724,  0.99998814],\n",
       "        [-0.76825464,  0.7312359 ,  0.63279754, ...,  0.9999857 ,\n",
       "          0.9999867 ,  0.9999876 ],\n",
       "        [-0.95375264, -0.14402692,  0.99899054, ...,  0.9999851 ,\n",
       "          0.9999861 ,  0.9999871 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下直接參考 TensorFlow 官方 tutorial \n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  sines = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  cosines = np.cos(angle_rads[:, 1::2])\n",
    "  \n",
    "  pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "  \n",
    "  pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYQBBzXcG-uZ"
   },
   "source": [
    "一路看下來你應該也可以猜到位置編碼的每一維意義了：\n",
    "- 第 1 維代表 batch_size，之後可以 broadcasting\n",
    "- 第 2 維是序列長度，我們會為每個在輸入 / 輸出序列裡頭的子詞都加入位置編碼\n",
    "- 第 3 維跟詞嵌入向量同維度\n",
    "\n",
    "因為是要跟詞嵌入向量相加，位置編碼的維度也得是 `d_model`。我們也可以把位置編碼畫出感受一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUIRthZB3V1a"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "plt.style.use(\"seaborn-notebook\")\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5LLms7lbJLso"
   },
   "source": [
    "```python\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d_model')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9OR7FPoMdKT"
   },
   "source": [
    "!image\n",
    "- transformer/positional-encoding.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQneziIQGxrk"
   },
   "source": [
    "這圖你應該在很多教學文章以及教授的影片裡都看過了。就跟我們前面看過的各種 2 維矩陣相同，x 軸代表著跟詞嵌入向量相同的維度 `d_model`，y 軸則代表序列中的每個位置。之後我們會看輸入 / 輸出序列有多少個子詞，就加入幾個位置編碼。\n",
    "\n",
    "關於位置編碼我們現在只需要知道這些就夠了，但如果你想知道更多相關的數學計算，可以參考[這個筆記本](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VNZmgkgEQ3f"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFMd1oaYWVlf"
   },
   "source": [
    "Encoder 裡頭主要包含了 3 個元件：\n",
    "- 輸入的詞嵌入層\n",
    "- 位置編碼\n",
    "- N 個 Encoder layers\n",
    "\n",
    "大部分的工作都交給 Encoder layer 小弟做了，因此 Encoder 的實作很單純："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXzsdldVSdxS"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "  # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "  # - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "    # 建立 `num_layers` 個 EncoderLayers\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "    # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "    # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "    input_seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "    # 再加上對應長度的位置編碼\n",
    "    x = self.embedding(x)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "    # 對 embedding 跟位置編碼的總合做 regularization\n",
    "    # 這在 Decoder 也會做\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    # 通過 N 個 EncoderLayer 做編碼\n",
    "    for i, enc_layer in enumerate(self.enc_layers):\n",
    "      x = enc_layer(x, training, mask)\n",
    "      # 以下只是用來 demo EncoderLayer outputs\n",
    "      #print('-' * 20)\n",
    "      #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "      \n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_RLUEfIYEdb"
   },
   "source": [
    "比較值得注意的是我們依照論文將 word embedding 乘上 `sqrt(d_model)`，並在 embedding 跟位置編碼相加以後通過 dropout 層來達到 regularization 的效果。\n",
    "\n",
    "現在我們可以直接將索引序列 `inp` 丟入 Encoder：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "9Z_3e3Y-Y9ie",
    "outputId": "882e0f3d-2d94-40f2-ab2a-740a364259d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.80654097 -0.5846039  -0.31439844  1.7055433 ]\n",
      "  [-0.46891153 -0.57408124 -0.6840381   1.727031  ]\n",
      "  [-0.319709   -0.17782518 -1.1191479   1.616682  ]\n",
      "  [-0.49274105  0.26990706 -1.2412689   1.4641027 ]\n",
      "  [-0.88477194  0.16279429 -0.8493918   1.5713693 ]\n",
      "  [-0.96625364 -0.25279218 -0.4533522   1.6723981 ]\n",
      "  [-0.8476429  -0.5615218  -0.28872433  1.6978891 ]\n",
      "  [-0.61957765 -0.5919263  -0.51938564  1.7308894 ]]\n",
      "\n",
      " [[-0.8083886  -0.56457365 -0.33460823  1.7075704 ]\n",
      "  [-0.50152016 -0.5214133  -0.7037289   1.7266623 ]\n",
      "  [-0.34244898 -0.11313835 -1.1444559   1.6000432 ]\n",
      "  [-0.5072439   0.21401608 -1.2050328   1.4982607 ]\n",
      "  [-0.88611245  0.26368466 -0.9036027   1.5260304 ]\n",
      "  [-0.96629447 -0.21083635 -0.49055386  1.6676848 ]\n",
      "  [-0.86832803 -0.5383212  -0.28836083  1.6950101 ]\n",
      "  [-0.6246328  -0.57586765 -0.5305909   1.7310913 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Encoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out = encoder(inp, training=False, mask=None)\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLkj1Ti-dDNK"
   },
   "source": [
    "注意因為 Encoder 已經包含了詞嵌入層，因此我們不用再像呼叫 [Encoder layer](#Encoder-的小弟) 時一樣還得自己先做 word embedding。現在的輸入及輸出張量為：\n",
    "- 輸入：（batch_size, seq_len）\n",
    "- 輸出：（batch_size, seq_len, d_model）\n",
    "\n",
    "有了 Encoder，我們之後就可以直接把 2 維的索引序列 `inp` 丟入 Encoder，讓它幫我們把裡頭所有的英文序列做一連串的轉換。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o89XsycDSAYG"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YChxwFb91T5"
   },
   "source": [
    "Decoder layer 本來就只跟 Encoder layer 差在一個 MHA，而這邏輯被包起來以後呼叫它的 Decoder 做的事情就跟 Encoder 基本上沒有兩樣了。\n",
    "\n",
    "在 Decoder 裡頭我們只需要建立一個專門給中文用的詞嵌入層以及位置編碼即可。我們在呼叫每個 Decoder layer 的時候也順便把其注意權重存下來，方便我們了解模型訓練完後是怎麼做翻譯的。\n",
    "\n",
    "以下則是實作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38A-OJYcgzh8"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size`\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    # 為中文（目標語言）建立詞嵌入層\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "  \n",
    "  # 呼叫時的參數跟 DecoderLayer 一模一樣\n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    \n",
    "    tar_seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}  # 用來存放每個 Decoder layer 的注意權重\n",
    "    \n",
    "    # 這邊跟 Encoder 做的事情完全一樣\n",
    "    x = self.embedding(x)  # (batch_size, tar_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    \n",
    "    for i, dec_layer in enumerate(self.dec_layers):\n",
    "      x, block1, block2 = dec_layer(x, enc_output, training,\n",
    "                                    combined_mask, inp_padding_mask)\n",
    "      \n",
    "      # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察\n",
    "      attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, tar_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uscq7Qd-pF5k"
   },
   "source": [
    "接著讓我們初始並呼叫一個 Decoder 看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1377
    },
    "colab_type": "code",
    "id": "vdZ1zwAnpFrd",
    "outputId": "b62b30c2-6e9e-47b4-8949-8db9a1b2df47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.80654097 -0.5846039  -0.31439844  1.7055433 ]\n",
      "  [-0.46891153 -0.57408124 -0.6840381   1.727031  ]\n",
      "  [-0.319709   -0.17782518 -1.1191479   1.616682  ]\n",
      "  [-0.49274105  0.26990706 -1.2412689   1.4641027 ]\n",
      "  [-0.88477194  0.16279429 -0.8493918   1.5713693 ]\n",
      "  [-0.96625364 -0.25279218 -0.4533522   1.6723981 ]\n",
      "  [-0.8476429  -0.5615218  -0.28872433  1.6978891 ]\n",
      "  [-0.61957765 -0.5919263  -0.51938564  1.7308894 ]]\n",
      "\n",
      " [[-0.8083886  -0.56457365 -0.33460823  1.7075704 ]\n",
      "  [-0.50152016 -0.5214133  -0.7037289   1.7266623 ]\n",
      "  [-0.34244898 -0.11313835 -1.1444559   1.6000432 ]\n",
      "  [-0.5072439   0.21401608 -1.2050328   1.4982607 ]\n",
      "  [-0.88611245  0.26368466 -0.9036027   1.5260304 ]\n",
      "  [-0.96629447 -0.21083635 -0.49055386  1.6676848 ]\n",
      "  [-0.86832803 -0.5383212  -0.28836083  1.6950101 ]\n",
      "  [-0.6246328  -0.57586765 -0.5305909   1.7310913 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "inp_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[-0.5437632  -1.055963    1.6090912  -0.0093651 ]\n",
      "  [-0.35729456 -1.2363737   1.5295789   0.06408926]\n",
      "  [ 0.35950443 -1.4217519   1.3327445  -0.27049693]\n",
      "  [ 0.00910451 -1.3681054   1.4556323  -0.09663116]\n",
      "  [-0.39842203 -1.0891637   1.6237149  -0.13612938]\n",
      "  [-0.41910946 -1.0254465   1.6521797  -0.20762381]\n",
      "  [-0.36797434 -1.036104    1.6521349  -0.2480565 ]\n",
      "  [-0.19375193 -1.1218892   1.6165614  -0.30092025]\n",
      "  [ 0.40127647 -1.3597702   1.3540744  -0.39558053]\n",
      "  [ 0.17590097 -1.419068    1.3905344  -0.14736754]]\n",
      "\n",
      " [[-0.54991776 -1.0509207   1.6102997  -0.00946123]\n",
      "  [-0.3790077  -1.2450974   1.514628    0.10947719]\n",
      "  [ 0.1746773  -1.3877552   1.415193   -0.20211506]\n",
      "  [-0.03870562 -1.3375971   1.4825788  -0.10627584]\n",
      "  [-0.43508232 -1.067575    1.6293938  -0.12673649]\n",
      "  [-0.41048303 -1.0317237   1.6503688  -0.20816201]\n",
      "  [-0.3626595  -1.0360833   1.652463   -0.25372016]\n",
      "  [-0.24817836 -1.1092765   1.6238651  -0.26641032]\n",
      "  [ 0.1850568  -1.3670969   1.4271388  -0.2450987 ]\n",
      "  [ 0.09142628 -1.3988855   1.4218552  -0.11439597]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "decoder_layer1_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer1_block2.shape: (2, 2, 10, 8)\n",
      "decoder_layer2_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer2_block2.shape: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Decoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 遮罩\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化一個 Decoder\n",
    "decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列以及遮罩丟入 Decoder\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_padding_mask:\", inp_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "dec_out, attn = decoder(tar, enc_out, training=False, \n",
    "                        combined_mask=combined_mask,\n",
    "                        inp_padding_mask=inp_padding_mask)\n",
    "print(\"dec_out:\", dec_out)\n",
    "print(\"-\" * 20)\n",
    "for block_name, attn_weights in attn.items():\n",
    "  print(f\"{block_name}.shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OFLvmhfuCpm"
   },
   "source": [
    "麻雀雖小，五臟俱全。雖然我們是使用 demo 數據，但基本上這就是你在呼叫 Decoder 時需要做的所有事情：\n",
    "- 初始時給它中文（目標語言）的字典大小、其他超參數\n",
    "- 輸入中文 batch 的索引序列\n",
    "- 也要輸入兩個遮罩以及 Encoder 輸出 `enc_out`\n",
    "\n",
    "Decoder 的輸出你現在應該都可以很輕鬆地解讀才是。基本上跟 Decoder layer 一模一樣，只差在我們額外輸出一個 Python dict，裡頭存放所有 Decoder layers 的注意權重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jk7B1OOZSCJH"
   },
   "source": [
    "### 第一個 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnaaJGfdwG5f"
   },
   "source": [
    "沒錯，終於到了這個時刻。在實作 Transformer 之前先點擊影片來簡單回顧一下我們在這一章實作了什麼些玩意兒："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47JITokm2bcg"
   },
   "source": [
    "!mp4\n",
    "- images/transformer/transformer-imple.mp4\n",
    "- images/transformer/transformer-imple.jpg\n",
    "- Transformer 本身只有 3 個 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uERO1y54cOKq"
   },
   "source": [
    "在我們前面已經將大大小小的 layers 一一實作並組裝起來以後，真正的 Transformer  模型只需要 3 個元件：\n",
    "1. Encoder \n",
    "2. Decoder\n",
    "3. Final linear layer\n",
    "\n",
    "馬上讓我們看看 Transformer  的實作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "# Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型\n",
    "class Transformer(tf.keras.Model):\n",
    "  # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, rate)\n",
    "    # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "  # enc_padding_mask 跟 dec_padding_mask 都是英文序列的 padding mask，\n",
    "  # 只是一個給 Encoder layer 的 MHA 用，一個是給 Decoder layer 的 MHA 2 使用\n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           combined_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "    \n",
    "    # 將 Decoder 輸出通過最後一個 linear layer\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZvy2EbDDJVl"
   },
   "source": [
    "扣掉註解，Transformer 的實作本身非常簡短。\n",
    "\n",
    "被輸入 Transformer 的多個 2 維英文張量 `inp` 會一路通過 **Encoder** 裡頭的詞嵌入層，位置編碼以及 N 個 Encoder layers 後被轉換成 Encoder 輸出 `enc_output`，接著對應的中文序列 `tar` 則會在 **Decoder** 裡頭走過相似的旅程並在每一層的 Decoder layer 利用 MHA 2 關注 Encoder 的輸出 `enc_output`，最後被 Decoder 輸出。\n",
    "\n",
    "而 Decoder 的輸出 `dec_output` 則會通過 **Final linear layer**，被轉成進入 Softmax 前的 logits `final_output`，其 logit 的數目則跟中文字典裡的子詞數相同。\n",
    "\n",
    "\n",
    "因為 Transformer 把 Decoder 也包起來了，現在我們連 Encoder 輸出 `enc_output` 也不用管，只要把英文（來源）以及中文（目標）的索引序列 batch 丟入 Transformer，它就會輸出最後一維為中文字典大小的張量。第 2 維是輸出序列，裡頭每一個位置的向量就代表著該位置的中文字的機率分佈（事實上通過 softmax 才是，但這邊先這樣說方便你理解）：\n",
    "- 輸入：\n",
    "  - 英文序列：（batch_size, inp_seq_len）\n",
    "  - 中文序列：（batch_size, tar_seq_len）\n",
    "- 輸出：\n",
    "  - 生成序列：（batch_size, tar_seq_len, target_vocab_size）\n",
    "  - 注意權重的 dict\n",
    "\n",
    "\n",
    "讓我們馬上建一個 Transformer，並假設我們已經準備好用 demo 數據來訓練它做英翻中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "7_f8yN5WjZyM",
    "outputId": "d6bdbaa2-17ac-4841-a939-db9fa9bca6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_inp: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "tar_real: tf.Tensor(\n",
      "[[  10  241   80   27    3 4202    0    0    0]\n",
      " [ 162  467  421  189   14    7  553    3 4202]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "predictions: tf.Tensor(\n",
      "[[[ 0.00929452 -0.01123782  0.05421777 ... -0.01170466  0.00628542\n",
      "   -0.07576236]\n",
      "  [ 0.03640017 -0.01885041  0.05113849 ... -0.02349908  0.01716622\n",
      "   -0.06729948]\n",
      "  [ 0.05617092 -0.02265774  0.04667147 ... -0.02913139  0.0241506\n",
      "   -0.05331099]\n",
      "  ...\n",
      "  [ 0.00905135 -0.01058669  0.05486142 ... -0.01039154  0.0058039\n",
      "   -0.07445519]\n",
      "  [ 0.02215609 -0.01478041  0.05375389 ... -0.0170105   0.01135763\n",
      "   -0.07241639]\n",
      "  [ 0.0478656  -0.02148081  0.04837158 ... -0.02759764  0.02148173\n",
      "   -0.06043392]]\n",
      "\n",
      " [[ 0.00996658 -0.01115559  0.05453676 ... -0.0114185   0.00637141\n",
      "   -0.07500792]\n",
      "  [ 0.03897631 -0.01930442  0.0508956  ... -0.02409907  0.01803425\n",
      "   -0.0656432 ]\n",
      "  [ 0.05387272 -0.02244362  0.04702405 ... -0.02893805  0.02348556\n",
      "   -0.05554678]\n",
      "  ...\n",
      "  [ 0.01048942 -0.01085559  0.05502523 ... -0.01070841  0.0062833\n",
      "   -0.07385261]\n",
      "  [ 0.02370835 -0.01504852  0.05381611 ... -0.01732858  0.01186723\n",
      "   -0.07158875]\n",
      "  [ 0.04920105 -0.02166032  0.0481827  ... -0.02781233  0.02190085\n",
      "   -0.05933255]]], shape=(2, 9, 4203), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 1\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# + 2 是為了 <start> & <end> token\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "output_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 重點中的重點。訓練時用前一個字來預測下一個中文字\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "\n",
    "# 來源 / 目標語言用的遮罩。注意 `comined_mask` 已經將目標語言的兩種遮罩合而為一\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar_inp)\n",
    "look_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化我們的第一個 transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, \n",
    "                          input_vocab_size, output_vocab_size)\n",
    "\n",
    "# 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果\n",
    "predictions, attn_weights = transformer(inp, tar_inp, False, inp_padding_mask, \n",
    "                                        combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_inp:\", tar_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_real:\", tar_real)\n",
    "print(\"-\" * 20)\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQkWX-jy5E8T"
   },
   "source": [
    "有了前面的各種 layers，建立一個 Transformer 並不難。但要輸入什麼數據就是一門大學問了：\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "\n",
    "predictions, attn_weights = transformer(inp, tar_inp, False, ...)\n",
    "\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "為何是丟少了尾巴一個字的 `tar_inp` 序列進去 Transformer，而不是直接丟 `tar` 呢？\n",
    "\n",
    "別忘記我們才剛初始一個 Transformer，裡頭所有 layers 的權重都是隨機的，你可不能指望它真的會什麼「黑魔法」來幫你翻譯。我們得先訓練才行。但訓練時如果你把整個正確的中文序列 `tar`都進去給 Transformer 看，你期待它產生什麼？一首新的中文詩嗎？\n",
    "\n",
    "\n",
    "如果你曾經實作過序列生成模型或是看過[我之前的語言模型文章](https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html)，就會知道在序列生成任務裡頭，模型獲得的正確答案是輸入序列往左位移一個位置的結果。\n",
    "\n",
    "這樣講很抽象，讓我們看個影片了解序列生成是怎麼運作的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rU4ETha-88BF"
   },
   "source": [
    "!mp4\n",
    " - images/transformer/how-sequence-generation-work.mp4\n",
    " - images/transformer/how-sequence-generation-work.jpg\n",
    " - 了解序列生成以及如何訓練一個生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Zwnpoubuec-"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# labels = ['sentence', 'subword']\n",
    "# # plot(inp, name='inp', labels=labels, shape_desc='(batch_size, inp_seq_len)')\n",
    "# # plot(tar, name='tar', labels=labels, shape_desc='(batch_size, tar_seq_len)')\n",
    "\n",
    "# # string\n",
    "# plot(decode(inp), name='inp', is_string=True, \n",
    "#      labels=labels)\n",
    "# plot(decode(tar, lang='zh'), name='tar', is_string=True, \n",
    "#      labels=labels)\n",
    "# plot(decode(tar[:, :-1], lang='zh'), name='tar[:, :-1]', is_string=True, \n",
    "#      labels=labels)\n",
    "# plot(decode(tar[:, 1:], lang='zh'), name='tar[:, 1:]', is_string=True, \n",
    "#      labels=labels)\n",
    "\n",
    "\n",
    "# predicted_indices = tf.argmax(predictions, axis=-1)\n",
    "# predicted_words = decode(predicted_indices, lang=\"zh\")\n",
    "\n",
    "\n",
    "# plot(predicted_words, name='predicted_words', is_string=True, \n",
    "#      labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yIFd5QqeTkww"
   },
   "source": [
    "你現在應該明白 Transformer 在訓練的時候並不是吃整個中文序列，而是吃一個去掉尾巴的序列 `tar_inp`，然後試著去預測「左移」一個字以後的序列 `tar_real`。同樣概念當然也可以運用到以 RNN 或是 CNN-based 的模型上面。\n",
    "\n",
    "從影片中你也可以發現給定 `tar_inp` 序列中的任一位置，其對應位置的 `tar_real` 就是下個時間點模型應該要預測的中文字。\n",
    "\n",
    "序列生成任務可以被視為是一個分類任務（Classification），而每一個中文字都是一個分類。而 Transformer 就是要去產生一個中文字的機率分佈，想辦法跟正解越接近越好。\n",
    "\n",
    "跟用已訓練的 Transformer 做**預測**時不同，在**訓練**時為了穩定模型表現，我們並不會將 Transformer 的輸出再度丟回去當做其輸入（人形蜈蚣？），而是像影片中所示，給它左移一個位置後的序列 `tar_real` 當作正解讓它去最小化 error。\n",
    "\n",
    "這種無視模型預測結果，而將正確解答丟入的訓練方法一般被稱作 [teacher forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)。你也可以參考教授的 [Sequence-to-sequence Learning 教學](https://youtu.be/ZjfjPzXw6og?t=1952)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## 定義損失函數與指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZV7lSNCWbog9"
   },
   "source": [
    "因為被視為是一個分類任務，我們可以使用 cross entropy 來計算序列生成任務中實際的中文字跟模型預測的中文字分佈（distribution）相差有多遠。\n",
    "\n",
    "這邊簡單定義一個損失函式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mU2HYGDYdGxw",
    "outputId": "8cabe79d-81a3-4af0-8518-e134bc6b5589"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=197487, shape=(3,), dtype=float32, numpy=array([0.31326166, 0.31326166, 1.3132616 ], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# 假設我們要解的是一個 binary classifcation， 0 跟 1 個代表一個 label\n",
    "real = tf.constant([1, 1, 0], shape=(1, 3), dtype=tf.float32)\n",
    "pred = tf.constant([[0, 1], [0, 1], [0, 1]], dtype=tf.float32)\n",
    "loss_object(real, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "如果你曾做過分類問題，應該能看出預測序列 `pred` 裡頭的第 3 個預測結果出錯因此 entropy 值上升。損失函數 `loss_object` 做的事情就是比較 2 個序列並計算 cross entropy：\n",
    "- `real`：一個包含 N 個正確 labels 的序列\n",
    "- `pred`：一個包含 N 個維度為 label 數的 logit 序列\n",
    "\n",
    "我們在這邊將 `reduction` 參數設為 `none`，請 `loss_object` 不要把每個位置的 error 加總。而這是因為我們之後要自己把 `<pad>` token 出現的位置的損失捨棄不計。\n",
    "\n",
    "而將 `from_logits` 參數設為 `True` 是因為從 Transformer 得到的預測還沒有經過 softmax，因此加總還不等於 1：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "XwfrEs34ei5i",
    "outputId": "0cf09032-0081-4289-fccb-893740f382a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: tf.Tensor(\n",
      "[[[ 0.00929452 -0.01123782  0.05421777 ... -0.01170466  0.00628542\n",
      "   -0.07576236]\n",
      "  [ 0.03640017 -0.01885041  0.05113849 ... -0.02349908  0.01716622\n",
      "   -0.06729948]\n",
      "  [ 0.05617092 -0.02265774  0.04667147 ... -0.02913139  0.0241506\n",
      "   -0.05331099]\n",
      "  ...\n",
      "  [ 0.00905135 -0.01058669  0.05486142 ... -0.01039154  0.0058039\n",
      "   -0.07445519]\n",
      "  [ 0.02215609 -0.01478041  0.05375389 ... -0.0170105   0.01135763\n",
      "   -0.07241639]\n",
      "  [ 0.0478656  -0.02148081  0.04837158 ... -0.02759764  0.02148173\n",
      "   -0.06043392]]\n",
      "\n",
      " [[ 0.00996658 -0.01115559  0.05453676 ... -0.0114185   0.00637141\n",
      "   -0.07500792]\n",
      "  [ 0.03897631 -0.01930442  0.0508956  ... -0.02409907  0.01803425\n",
      "   -0.0656432 ]\n",
      "  [ 0.05387272 -0.02244362  0.04702405 ... -0.02893805  0.02348556\n",
      "   -0.05554678]\n",
      "  ...\n",
      "  [ 0.01048942 -0.01085559  0.05502523 ... -0.01070841  0.0062833\n",
      "   -0.07385261]\n",
      "  [ 0.02370835 -0.01504852  0.05381611 ... -0.01732858  0.01186723\n",
      "   -0.07158875]\n",
      "  [ 0.04920105 -0.02166032  0.0481827  ... -0.02781233  0.02190085\n",
      "   -0.05933255]]], shape=(2, 9, 4203), dtype=float32)\n",
      "--------------------\n",
      "tf.Tensor(\n",
      "[[1.4971986 3.1899047 4.1454954 3.7353938 2.869739  1.8605256 1.3746347\n",
      "  2.2779167 3.8190796]\n",
      " [1.4881071 3.303587  4.0757227 3.7524652 2.836317  1.9132937 1.4376438\n",
      "  2.3432927 3.8689976]], shape=(2, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"predictions:\", predictions)\n",
    "print(\"-\" * 20)\n",
    "print(tf.reduce_sum(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gMPzch5dQdI"
   },
   "source": [
    "有了 `loss_object` 實際算 cross entropy 以後，我們需要另外一個函式來建立遮罩並加總序列裡頭不包含 `<pad> token 位置的損失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  # 照樣計算所有位置的 cross entropy 但不加總\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask  # 只計算非 <pad> 位置的損失 \n",
    "  \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtkPbR93gpvB"
   },
   "source": [
    "我另外再定義兩個 [tf.keras.metrics](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics)，方便之後使用 [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard?hl=zh-cn) 來追蹤模型 performance："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## 設置超參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "前面實作了那麼多 layers，你應該還記得有哪些是你自己可以調整的超參數吧？\n",
    "\n",
    "讓我幫你全部列出來：\n",
    "- `num_layers` 決定 Transfomer 裡頭要有幾個 Encoder / Decoder layers\n",
    "- `d_model` 決定我們子詞的 representation space 維度\n",
    "- `num_heads` 要做幾頭的自注意力運算\n",
    "- `dff` 決定 FFN 的中間維度\n",
    "- `dropout_rate` 預設 0.1，一般用預設值即可\n",
    "- `input_vocab_size`：輸入語言（英文）的字典大小\n",
    "- `target_vocab_size`：輸出語言（中文）的字典大小\n",
    "\n",
    "論文裡頭最基本的 Transformer 配置為：\n",
    "- `num_layers=6`\n",
    "- `d_model=512`\n",
    "- `dff=2048`\n",
    "\n",
    "有大量數據以及大的 Transformer，你可以在很多機器學習任務都達到不錯的成績。為了不要讓訓練時間太長，在這篇文章裡頭我會把 Transformer 裡頭的超參數設小一點："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lnJn5SLA2ahP",
    "outputId": "c8d5602f-34f1-4c95-bc3e-8d7f62f6cd38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 8137\n",
      "target_vocab_size: 4203\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "dropout_rate = 0.1  # 預設值\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvmjwiZLSsL1"
   },
   "source": [
    "4 層 Encoder / Decoder layers 不算貪心，小巫見大巫（笑\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## 設置 Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7X74hK5BhjTY"
   },
   "source": [
    "我們在這邊跟[論文](https://arxiv.org/pdf/1706.03762.pdf)一致，使用 [Adam optimizer](optimization-6be9a291375c) 以及自定義的 learning rate scheduler：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XShu12MmiU88"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/lr-equation.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mX1CQ0dLi6Br"
   },
   "source": [
    "這 schedule 讓訓練過程的前 `warmup_steps` 的 learning rate 線性增加，在那之後則跟步驟數 `step_num` 的反平方根成比例下降。不用擔心你沒有完全理解這公式，我們一樣可以直接使用 [TensorFlow 官方教學的實作](https://www.tensorflow.org/beta/tutorials/text/transformer?authuser=1#optimizer)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  # 論文預設 `warmup_steps` = 4000\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGsfQdLclSBb"
   },
   "source": [
    "我們可以觀察看看這個 schedule 是怎麼隨著訓練步驟而改變 learning rate 的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fBMgDm7aOlK"
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.style.use(\"ggplot\")\n",
    "# plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f33ZCgvHpPdG"
   },
   "source": [
    "```python\n",
    "d_models = [128, 256, 512]\n",
    "warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "schedules = []\n",
    "labels = []\n",
    "colors = [\"blue\", \"red\", \"black\"]\n",
    "for d in d_models:\n",
    "  schedules += [CustomSchedule(d, s) for s in warmup_steps]\n",
    "  labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]\n",
    "\n",
    "for i, (schedule, label) in enumerate(zip(schedules, labels)):\n",
    "  plt.plot(schedule(tf.range(10000, dtype=tf.float32)), \n",
    "           label=label, color=colors[i // 3])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "835UETXCvMmC"
   },
   "source": [
    "!image\n",
    "- dark\n",
    "- transformer/transformer-custom-lr.jpg\n",
    "- 不同 d_model 以及 warmup_steps 的 learning rate 變化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxW9tvHpmGMN"
   },
   "source": [
    "你可以明顯地看到所有 schedules 都先經過 `warmup_steps` 個步驟直線提升 learning rate，接著逐漸平滑下降。另外我們也會給比較高維的 `d_model` 維度比較小的 learning rate。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## 實際訓練以及定時存檔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbS39iI_5fBa"
   },
   "source": [
    "好啦，什麼都準備齊全了，讓我們開始訓練 Transformer 吧！記得使用前面已經定義好的超參數來初始化一個全新的 Transformer："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "UiysUa--4tOU",
    "outputId": "30d70ea4-74fb-4c88-d71b-778ca8804303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這個 Transformer 有 4 層 Encoder / Decoder layers\n",
      "d_model: 128\n",
      "num_heads: 8\n",
      "dff: 512\n",
      "input_vocab_size: 8137\n",
      "target_vocab_size: 4203\n",
      "dropout_rate: 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "打遊戲時你會記得要定期存檔以防任何意外發生，訓練深度學習模型也是同樣道理。設置 [checkpoint](https://www.tensorflow.org/beta/guide/checkpoints) 來定期儲存 / 讀取模型及 optimizer 是必備的。\n",
    "\n",
    "我們在底下會定義一個 checkpoint 路徑，此路徑包含了各種超參數的資訊，方便之後比較不同實驗的結果並載入已訓練的進度。我們也需要一個 checkpoint manager 來做所有跟存讀模型有關的雜事，並只保留最新 5 個 checkpoints 以避免佔用太多空間：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Tisi3QU68OUt",
    "outputId": "51a9e28d-cd49-4448-ca39-9620859481c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已讀取最新的 checkpoint，模型已訓練 50 epochs。\n"
     ]
    }
   ],
   "source": [
    "# 方便比較不同實驗/ 不同超參數設定的結果\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_perc}train_perc\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n",
    "# 一般來說你會想存下模型以及 optimizer 的狀態\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n",
    "# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果在 checkpoint 路徑上有發現檔案就讀進來\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  \n",
    "  # 用來確認之前訓練多少 epochs 了\n",
    "  last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "  print(f'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。')\n",
    "else:\n",
    "  last_epoch = 0\n",
    "  print(\"沒找到 checkpoint，從頭訓練。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LWx9VWRzLsv"
   },
   "source": [
    "!image\n",
    "- transformer/am-i-a-joke-to-you.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4AnPjQhfe0o"
   },
   "source": [
    "我知道你在想什麼。\n",
    "\n",
    "「誒！？ 你不當場訓練嗎？」「直接載入已訓練的模型太狗了吧！」\n",
    "\n",
    "拜託，我都訓練 N 遍了，每次都重新訓練也太沒意義了。而且你能想像為了寫一個章節我就得重新訓練一個 Transformer 來 demo 嗎？這樣太沒效率了。比起每次重新訓練模型，這才是你在真實世界中應該做的事情：盡可能回復之前的訓練進度來節省時間。\n",
    "\n",
    "不過放心，我仍會秀出完整的訓練程式碼讓你可以執行第一次的訓練。當你想要依照本文訓練自己的 Transformer 時會感謝有 checkpoint manager 的存在。現在假設我們還沒有 checkpoints。\n",
    "\n",
    "在實際訓練 Transformer 之前還需要定義一個簡單函式來產生所有的遮罩："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmKXhNuvLlS8"
   },
   "outputs": [],
   "source": [
    "# 為 Transformer 的 Encoder / Decoder 準備遮罩\n",
    "def create_masks(inp, tar):\n",
    "  # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # 同樣也是英文句子的 padding mask，但是是要交給 Decoder layer 的 MHA 2 \n",
    "  # 關注 Encoder 輸出序列用的\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Decoder layer 的 MHA1 在做自注意力機制用的\n",
    "  # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YlqG41FBZnh"
   },
   "source": [
    "如果沒有本文前面針對遮罩的詳細說明，很多第一次實作的人得花不少時間來確實地掌握這些遮罩的用途。不過對現在的你來說應該也是小菜一碟。\n",
    "\n",
    "一個數據集包含多個 batch，而每次拿一個 batch 來訓練的步驟就稱作 `train_step`。為了讓程式碼更簡潔以及容易優化，我們會定義 Transformer 在一次訓練步驟（處理一個 batch）所需要做的所有事情。\n",
    "\n",
    "不限於 Transformer，一般來說 `train_step` 函式裡會有幾個重要步驟：\n",
    "1. 對訓練數據做些必要的前處理\n",
    "2. 將數據丟入模型，取得預測結果\n",
    "3. 用預測結果跟正確解答計算 loss\n",
    "4. 取出梯度並利用 optimizer 做梯度下降\n",
    "\n",
    "有了這個概念以後看看程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "@tf.function  # 讓 TensorFlow 幫我們將 eager code 優化並加快運算\n",
    "def train_step(inp, tar):\n",
    "  # 前面說過的，用去尾的原始序列去預測下一個字的序列\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  # 建立 3 個遮罩\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要\n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JNXnOjZiRnV"
   },
   "source": [
    "如果你曾經以TensorFlow 2 實作過稍微複雜一點的模型，應該就知道  `train_step` 函式的寫法非常固定：\n",
    "- 對輸入數據做些前處理（本文中的遮罩、將輸出序列左移當成正解 etc.）\n",
    "- 利用 `tf.GradientTape` 輕鬆記錄數據被模型做的所有轉換並計算 loss\n",
    "- 將梯度取出並讓 optimzier 對可被訓練的權重做梯度下降（上升）\n",
    "\n",
    "你完全可以用一模一樣的方式將任何複雜模型的處理過程包在 `train_step` 函式，這樣可以讓我們之後在 iterate 數據集時非常輕鬆。而且最重要的是可以用 [tf.function](https://www.tensorflow.org/beta/tutorials/eager/tf_function) 來提高此函式裡頭運算的速度。你可以點擊連結來了解更多。\n",
    "\n",
    "\n",
    "處理一個 batch 的 `train_step` 函式也有了，就只差寫個 for loop 將數據集跑個幾遍了。我之前的模型雖然訓練了 50 個 epochs，但事實上大概 30 epochs 翻譯的結果就差不多穩定了。所以讓我們將 `EPOCHS` 設定為 30："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HWB5uvOs_sTC",
    "outputId": "e3133b64-372d-484a-9360-15bc1575c0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此超參數組合的 Transformer 已經訓練 50 epochs。\n",
      "剩餘 epochs：0\n"
     ]
    }
   ],
   "source": [
    "# 定義我們要看幾遍數據集\n",
    "EPOCHS = 30\n",
    "print(f\"此超參數組合的 Transformer 已經訓練 {last_epoch} epochs。\")\n",
    "print(f\"剩餘 epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "\n",
    "# 用來寫資訊到 TensorBoard，非必要但十分推薦\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# 比對設定的 `EPOCHS` 以及已訓練的 `last_epoch` 來決定還要訓練多少 epochs\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  # 重置紀錄 TensorBoard 的 metrics\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # 一個 epoch 就是把我們定義的訓練資料集一個一個 batch 拿出來處理，直到看完整個數據集 \n",
    "  for (step_idx, (inp, tar)) in enumerate(train_dataset):\n",
    "    \n",
    "    # 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss\n",
    "    train_step(inp, tar)  \n",
    "\n",
    "  # 每個 epoch 完成就存一次檔    \n",
    "  if (epoch + 1) % 1 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  # 將 loss 以及 accuracy 寫到 TensorBoard 上\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "    tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "  \n",
    "  print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "  print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbQi39G2mSYR"
   },
   "source": [
    "如訊息所示，當指定的 `EPOCHS` 「落後」於之前的訓練進度我們就不再訓練了。但如果是第一次訓練或是訓練到指定 `EPOCHS` 的一部分，我們都會從正確的地方開始訓練並存檔，不會浪費到訓練時間或計算資源。\n",
    "\n",
    "這邊的邏輯也很簡單，在每個 epoch 都：\n",
    "- （非必要）重置寫到 TensorBoard 的 metrics 的值\n",
    "- 將整個數據集的 batch 取出，交給 `train_step` 函式處理\n",
    "- （非必要）存 checkpoints\n",
    "- （非必要）將當前 epoch 結果寫到 TensorBoard\n",
    "- （非必要）在標準輸出顯示當前 epoch 結果\n",
    "\n",
    "\n",
    "是的，如果你真的只是想要訓練個模型，什麼其他事情都不想考慮的話那你可以：\n",
    "\n",
    "```python\n",
    "# 87 分，不能再高了。\n",
    "for epoch in range(EPOCHS):\n",
    "  for inp, tar in train_dataset:\n",
    "    train_step(inp, tar) \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oz1eFJAOCqtU"
   },
   "source": [
    "!image\n",
    "- transformer/go-home-every-body.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TalGPDyaCbOz"
   },
   "source": [
    "嗯 ... 話是這麼說，但我仍然建議你至少要記得存檔並將訓練過程顯示出來。我知道你會好奇訓練一個這樣的 Transformer 要多久時間，讓我把之前訓練的一些 log 顯示出來給你瞧瞧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "VTPvi1nPGWyv",
    "outputId": "9066938e-f1a7-41e3-bbb9-30881bdb1733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Saving checkpoint for epoch 1 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-1\n",
      "Epoch 1 Loss 5.2072 Accuracy 0.0179\n",
      "Time taken for 1 epoch: 206.54558181762695 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-2\n",
      "Epoch 2 Loss 4.2652 Accuracy 0.0560\n",
      "Time taken for 1 epoch: 68.48831677436829 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-3\n",
      "Epoch 3 Loss 3.7987 Accuracy 0.0910\n",
      "Time taken for 1 epoch: 68.41022562980652 secs\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Saving checkpoint for epoch 29 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-29\n",
      "Epoch 29 Loss 1.2693 Accuracy 0.3929\n",
      "Time taken for 1 epoch: 69.18679404258728 secs\n",
      "\n",
      "Saving checkpoint for epoch 30 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-30\n",
      "Epoch 30 Loss 1.2426 Accuracy 0.3965\n",
      "Time taken for 1 epoch: 68.7313539981842 secs\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "print(\"\"\"\n",
    "\n",
    "Saving checkpoint for epoch 1 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-1\n",
    "Epoch 1 Loss 5.2072 Accuracy 0.0179\n",
    "Time taken for 1 epoch: 206.54558181762695 secs\n",
    "\n",
    "Saving checkpoint for epoch 2 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-2\n",
    "Epoch 2 Loss 4.2652 Accuracy 0.0560\n",
    "Time taken for 1 epoch: 68.48831677436829 secs\n",
    "\n",
    "Saving checkpoint for epoch 3 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-3\n",
    "Epoch 3 Loss 3.7987 Accuracy 0.0910\n",
    "Time taken for 1 epoch: 68.41022562980652 secs\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "Saving checkpoint for epoch 29 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-29\n",
    "Epoch 29 Loss 1.2693 Accuracy 0.3929\n",
    "Time taken for 1 epoch: 69.18679404258728 secs\n",
    "\n",
    "Saving checkpoint for epoch 30 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-30\n",
    "Epoch 30 Loss 1.2426 Accuracy 0.3965\n",
    "Time taken for 1 epoch: 68.7313539981842 secs\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTcEvBrCnlMV"
   },
   "source": [
    "事實上我們定義的 4 層 Transformer 大約每 70 秒就可以看完一遍有 3 萬筆訓練例子的數據集，而且你從上面的 loss 以及 accuracy 可以看出來 Transformer 至少在訓練集裡頭進步地挺快的。\n",
    "\n",
    "而就我自己的觀察大約經過 30 個 epochs 翻譯結果就很穩定了。所以你大約只需半個小時就能有一個非常簡單，有點水準的英翻中 Transformer（在至少有個一般 GPU 的情況）。\n",
    "\n",
    "但跟看上面的 log 比起來，我個人還是比較推薦使用 TensorBoard。在 TensorFlow 2 裡頭，你甚至能直接在 [Jupyter Notebook 或是 Colab](https://www.tensorflow.org/tensorboard/r2/get_started#using_tensorboard_with_other_methods) 裡頭開啟它：\n",
    "\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {your_log_dir}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rjg8YICEfEl"
   },
   "source": [
    "<video loop autoplay muted playsinline>\n",
    "  <source src=\"{static}images/transformer/tensorboard.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "<center>\n",
    "    使用 TensorBoard 可以讓你輕鬆比較不同超參數的訓練結果\n",
    "    <br/>\n",
    "    <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HhI2DvxFa3P"
   },
   "source": [
    "透過 TensorBoard，你能非常清楚地比較不同實驗以及不同點子的效果，知道什麼 work 什麼不 work，進而修正之後嘗試的方向。如果只是簡單寫個 `print`，那你永遠只會看到最新一次訓練過程的 log，然後忘記之前到底發生過什麼事。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOLFGB2NGYY-"
   },
   "source": [
    "## 實際進行英翻中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3b6MRh9oPuA"
   },
   "source": [
    "有了已經訓練一陣子的 Transformer，當然得拿它來實際做做翻譯。\n",
    "\n",
    "跟訓練的時候不同，在做預測時我們不需做 teacher forcing 來穩定 Transformer 的訓練過程。反之，我們將 Transformer 在每個時間點生成的中文索引加到之前已經生成的序列尾巴，並以此新序列作為其下一次的輸入。這是因為 Transformer 事實上是一個[自迴歸模型（Auto-regressive  model）](https://zh.wikipedia.org/wiki/%E8%87%AA%E8%BF%B4%E6%AD%B8%E6%A8%A1%E5%9E%8B)：依據自己生成的結果預測下次輸出。\n",
    "\n",
    "\n",
    "利用 Transformer 進行翻譯（預測）的邏輯如下：\n",
    "- 將輸入的英文句子利用 Subword Tokenizer 轉換成子詞索引序列（還記得 `inp` 吧？）\n",
    "- 在該英文索引序列前後加上代表英文 BOS / EOS 的 tokens\n",
    "- 在 Transformer 輸出序列長度達到 `MAX_LENGTH` 之前重複以下步驟：\n",
    "  - 為目前已經生成的中文索引序列產生新的遮罩\n",
    "  - 將剛剛的英文序列、當前的中文序列以及各種遮罩放入 Transformer\n",
    "  - 將 Transformer 輸出序列的最後一個位置的向量取出，並取 argmax 取得新的預測中文索引\n",
    "  - 將此索引加到目前的中文索引序列裡頭作為 Transformer 到此為止的輸出結果\n",
    "  - 如果新生成的中文索引為 `<end>` 則代表中文翻譯已全部生成完畢，直接回傳\n",
    "- 將最後得到的中文索引序列回傳作為翻譯結果\n",
    "\n",
    "是的，一個時間點生成一個中文字，而在第一個時間點因為 Transformer 還沒有任何輸出，我們會丟中文字的 `<start>` token 進去。你可能會想：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEmTntPjW35z"
   },
   "source": [
    "!quote\n",
    "- 為何每次翻譯開頭都是 start token，Transformer 還能產生不一樣且正確的結果？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzPrzLCOXGTo"
   },
   "source": [
    "答案也很簡單，因為 Decoder 可以透過「關注」 Encoder 處理完不同英文句子的輸出來獲得語義資訊，了解它在當下該生成什麼中文字作為第一個輸出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gg_1pn0wW5jx"
   },
   "source": [
    "\n",
    "現在讓我們定義一個 `evaluate` 函式實現上述邏輯。此函式的輸入是一個完全沒有經過處理的英文句子（以字串表示），輸出則是一個索引序列，裡頭的每個索引就代表著 Transformer 預測的中文字。\n",
    "\n",
    "讓我們實際看看 `evaluate` 函式：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-BF-ehpoPiw"
   },
   "outputs": [],
   "source": [
    "# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\n",
    "def evaluate(inp_sentence):\n",
    "  \n",
    "  # 準備英文句子前後會加上的 <start>, <end>\n",
    "  start_token = [subword_encoder_en.vocab_size]\n",
    "  end_token = [subword_encoder_en.vocab_size + 1]\n",
    "  \n",
    "  # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n",
    "  # 並在前後加上 BOS / EOS\n",
    "  inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n",
    "  # 是一個只包含一個中文 <start> token 的序列\n",
    "  decoder_input = [subword_encoder_zh.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n",
    "  \n",
    "  # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 每多一個生成的字就得產生新的遮罩\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "\n",
    "    # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n",
    "    predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n",
    "    if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n",
    "    # 下個中文字的時候關注到最新的 `predicted_id`\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  # 將 batch 的維度去掉後回傳預測的中文索引序列\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-6ixJHIPrth"
   },
   "source": [
    "我知道這章節程式碼很多很長，但搭配註解後你會發現它們實際上都不難，而且這也是你看這篇文章的主要目的：實際了解 Transformer 是怎麼做英中翻譯的。你不想只是紙上談兵，對吧？\n",
    "\n",
    "有了 `evaluate` 函式，要透過 Transformer 做翻譯非常容易：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "q4w-eLCEGYQc",
    "outputId": "a12d87d6-87aa-422d-a337-586755a39e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: China, India, and others have enjoyed continuing economic growth.\n",
      "--------------------\n",
      "predicted_seq: tf.Tensor(\n",
      "[4201   16    4   37  386  101    8   34   32    4   33  110  956  186\n",
      "   14   22   52  107   84    1  104  292   49  218    3], shape=(25,), dtype=int32)\n",
      "--------------------\n",
      "predicted_sentence: 中国、印度和其他国家都享受了经济增长的持续发展。\n"
     ]
    }
   ],
   "source": [
    "# 要被翻譯的英文句子\n",
    "sentence = \"China, India, and others have enjoyed continuing economic growth.\"\n",
    "\n",
    "# 取得預測的中文索引序列\n",
    "predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "# 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子\n",
    "target_vocab_size = subword_encoder_zh.vocab_size\n",
    "predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"-\" * 20)\n",
    "print(\"predicted_seq:\", predicted_seq)\n",
    "print(\"-\" * 20)\n",
    "print(\"predicted_sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKMlPuuqZ1yR"
   },
   "source": [
    "考慮到這個 Transformer 不算巨大（約 400 萬個參數），且模型訓練時用的數據集不大的情況下，我們達到相當不錯的結果，你說是吧？在這個例子裡頭該翻的詞彙都翻了出來，句子本身也還算自然。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "uVKnPNLibFbw",
    "outputId": "21ec0c07-00d7-417f-d76c-b106ba667c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_2 (Encoder)          multiple                  1834624   \n",
      "_________________________________________________________________\n",
      "decoder_2 (Decoder)          multiple                  1596288   \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            multiple                  542187    \n",
      "=================================================================\n",
      "Total params: 3,973,099\n",
      "Trainable params: 3,973,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsciyXXacc_x"
   },
   "source": [
    "## 視覺化注意權重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CygF7e2jj8P"
   },
   "source": [
    "除了其運算高度平行以及表現不錯以外，Transformer 另外一個優點在於我們可以透過視覺化注意權重（attention weights）來了解模型實際在生成序列的時候放「注意力」在哪裡。別忘記我們當初在 Decoder layers 做完 multi-head attention 之後都將注意權重輸出。現在正是它們派上用場的時候了。\n",
    "\n",
    "先讓我們看看有什麼注意權重可以拿來視覺化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "lMMl34CBkPP7",
    "outputId": "e842db0f-52bf-4bfb-d942-8cc422572d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: China, India, and others have enjoyed continuing economic growth.\n",
      "--------------------\n",
      "predicted_seq: tf.Tensor(\n",
      "[4201   16    4   37  386  101    8   34   32    4   33  110  956  186\n",
      "   14   22   52  107   84    1  104  292   49  218    3], shape=(25,), dtype=int32)\n",
      "--------------------\n",
      "attention_weights.keys():\n",
      "decoder_layer1_block1.shape: (1, 8, 25, 25)\n",
      "decoder_layer1_block2.shape: (1, 8, 25, 15)\n",
      "decoder_layer2_block1.shape: (1, 8, 25, 25)\n",
      "decoder_layer2_block2.shape: (1, 8, 25, 15)\n",
      "decoder_layer3_block1.shape: (1, 8, 25, 25)\n",
      "decoder_layer3_block2.shape: (1, 8, 25, 15)\n",
      "decoder_layer4_block1.shape: (1, 8, 25, 25)\n",
      "decoder_layer4_block2.shape: (1, 8, 25, 15)\n",
      "--------------------\n",
      "layer_name: decoder_layer4_block2\n"
     ]
    }
   ],
   "source": [
    "predicted_seq, attention_weights = evaluate(sentence)\n",
    "\n",
    "# 在這邊我們自動選擇最後一個 Decoder layer 的 MHA 2，也就是 Decoder 關注 Encoder 的 MHA\n",
    "layer_name = f\"decoder_layer{num_layers}_block2\"\n",
    "\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"-\" * 20)\n",
    "print(\"predicted_seq:\", predicted_seq)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights.keys():\")\n",
    "for layer_name, attn in attention_weights.items():\n",
    "  print(f\"{layer_name}.shape: {attn.shape}\")\n",
    "print(\"-\" * 20)\n",
    "print(\"layer_name:\", layer_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3Z-xr2Ukbou"
   },
   "source": [
    "`block1` 代表是 Decoder layer 自己關注自己的 MHA 1，因此倒數兩個維度都跟中文序列長度相同；`block2` 則是 Decoder layer 用來關注 Encoder 輸出的 MHA 2 ，在這邊我們選擇最後一個 Decoder layer 的 MHA 2 來看 Transformer 在生成中文序列時關注在英文句子的那些位置。\n",
    "\n",
    "但首先，我們得要有一個繪圖的函式才行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXVthHzZcAHf"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# 你可能會需要自行下載一個中文字體檔案以讓 matplotlib 正確顯示中文\n",
    "zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/SimHei/simhei.ttf')\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "# 這個函式將英 -> 中翻譯的注意權重視覺化（注意：我們將注意權重 transpose 以最佳化渲染結果\n",
    "def plot_attention_weights(attention_weights, sentence, predicted_seq, layer_name, max_len_tar=None):\n",
    "    \n",
    "  fig = plt.figure(figsize=(17, 7))\n",
    "  \n",
    "  sentence = subword_encoder_en.encode(sentence)\n",
    "  \n",
    "  # 只顯示中文序列前 `max_len_tar` 個字以避免畫面太過壅擠\n",
    "  if max_len_tar:\n",
    "    predicted_seq = predicted_seq[:max_len_tar]\n",
    "  else:\n",
    "    max_len_tar = len(predicted_seq)\n",
    "  \n",
    "  # 將某一個特定 Decoder layer 裡頭的 MHA 1 或 MHA2 的注意權重拿出來並去掉 batch 維度\n",
    "  attention_weights = tf.squeeze(attention_weights[layer_name], axis=0)  \n",
    "  # (num_heads, tar_seq_len, inp_seq_len)\n",
    "  \n",
    "  # 將每個 head 的注意權重畫出\n",
    "  for head in range(attention_weights.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head + 1)\n",
    "\n",
    "    # [注意]我為了將長度不短的英文子詞顯示在 y 軸，將注意權重做了 transpose\n",
    "    attn_map = np.transpose(attention_weights[head][:max_len_tar, :])\n",
    "    ax.matshow(attn_map, cmap='viridis')  # (inp_seq_len, tar_seq_len)\n",
    "    \n",
    "    fontdict = {\"fontproperties\": zhfont}\n",
    "    \n",
    "    ax.set_xticks(range(max(max_len_tar, len(predicted_seq))))\n",
    "    ax.set_xlim(-0.5, max_len_tar -1.5)\n",
    "    \n",
    "    ax.set_yticks(range(len(sentence) + 2))\n",
    "    ax.set_xticklabels([subword_encoder_zh.decode([i]) for i in predicted_seq \n",
    "                        if i < subword_encoder_zh.vocab_size], \n",
    "                       fontdict=fontdict, fontsize=18)    \n",
    "    \n",
    "    ax.set_yticklabels(\n",
    "        ['<start>'] + [subword_encoder_en.decode([i]) for i in sentence] + ['<end>'], \n",
    "        fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head + 1))\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNP2Slw9iBmQ"
   },
   "source": [
    "這個函式不難，且裡頭不少是調整圖片的細節設定因此我將它留給你自行參考。\n",
    "\n",
    "比較值得注意的是因為我們在這篇文章是做英文（來源）到中文（目標）的翻譯，注意權重的 shape 為：\n",
    "\n",
    "```text\n",
    "(batch_size, num_heads, zh_seq_len, en_seq_len)\n",
    "```\n",
    "\n",
    "如果你直接把注意權重繪出的話 y 軸就會是每個中文字，而 x 軸則會是每個英文子詞。而英文子詞繪在 x 軸太佔空間，我將每個注意權重都做 transpose 並呈現結果，這點你得注意一下。\n",
    "\n",
    "讓我們馬上畫出剛剛翻譯的注意權重看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "colab_type": "code",
    "id": "hCbtUbqOinIv",
    "outputId": "d53f5460-0ae9-48e0-819c-6ad258bfb174"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMUAAAHvCAYAAABUoWiWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1224x504 with 8 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_attention_weights(attention_weights, sentence, \n",
    "                       predicted_seq, layer_name, max_len_tar=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmoSfgV9nZQ_"
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "很美，不是嗎？\n",
    "\n",
    "如果你還記得，我在本文開頭就曾經秀過這張圖甚至開玩笑地跟你說：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIosrV9RoDbv"
   },
   "source": [
    "!quote\n",
    "- 好黑魔法，不學嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwZCpwKBoDCH"
   },
   "source": [
    "我不知道你當初跟現在的感受，但我相信在你閱讀完本文，尤其是對自注意機制以及 Transformer 有了深刻理解之後，這之間的感受肯定是有不少差異的。\n",
    "\n",
    "儘管其運算機制十分錯綜複雜，閱讀本文後 Transformer 對你來說不再是黑魔法，也不再是遙不可及的存在。如果你現在覺得「Transformer 也不過就這樣嘛！」那就達成我寫這篇文章的目的了。\n",
    "\n",
    "自注意力機制以及 Transformer 在推出之後就被非常廣泛地使用並改進，但在我自己開始接觸相關知識以後一直沒有發現完整的繁中教學，因此寫了這篇當初的我殷殷期盼的文章，也希望能幫助到更多人學習。\n",
    "\n",
    "在進入結語之前，讓我們看看文中的 Transformer 是怎麼逐漸學會做好翻譯的：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FeJq6zQLpCpW"
   },
   "source": [
    "<video loop autoplay muted playsinline>\n",
    "  <source src=\"{static}images/transformer/attention_weights_change_by_time.mp4\" type=\"video/mp4\">\n",
    "    您的瀏覽器不支援影片標籤，請留言通知我：S\n",
    "</video>\n",
    "<center>\n",
    "    Transformer 在訓練過程中逐漸學會關注在對的位置\n",
    "    <br/>\n",
    "    <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5amG5slq7lK"
   },
   "source": [
    "## 在你離開之前\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exJDZsXAwzbG"
   },
   "source": [
    "!quote\n",
    "- 這篇是當初在學習 Transformer 的我希望有人分享給自己的文章。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3ahPxBJtY61"
   },
   "source": [
    "我相信人類之所以強大是因為集體知識：我們能透過書籍、影片以及語言將一個人腦中的知識與思想共享給其他人，讓寶貴的知識能夠「scale」，在更多人的腦袋中發光發熱，創造更多價值。\n",
    "\n",
    "我希望你有從本文中學到一點東西，並幫助我將本文的這些知識「scale」，把文章分享給更多有興趣的人，並利用所學應用在一些你一直想要完成的任務上面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!image\n",
    "- bert/bert-intro.jpg\n",
    "- 以 Transformer 為基礎的語言代表模型 BERT\n",
    "- https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3ahPxBJtY61"
   },
   "source": [
    "如果你想要了解更多 Transformer 的相關應用，我推薦接著閱讀[進擊的 BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)，了解現在 NLP 領域裡頭知名的語言代表模型 BERT。\n",
    "\n",
    "最後一點提醒，就算 Transformer 比古早時代的方法好再多終究也只是個工具，其最大價值不會超過於被你拿來應用的問題之上。就好像現在已有不少超越基本 Transformer 的翻譯方法，但我們仍然持續在追尋更好的機器翻譯系統。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IiHln-Wt07pg"
   },
   "source": [
    "!quote\n",
    "- 工具會被淘汰，需求一直都在。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20190514_machine_translation_with_transformer.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
