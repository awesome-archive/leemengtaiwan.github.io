{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "The goal here is to progressively train deeper and more accurate models using TensorFlow. We will first load the notMNIST dataset which we have done data cleaning. For the classification problem, we will first train two logistic regression models use simple gradient descent, stochastic gradient descent (SGD) respectively for optimization to see the difference between these optimizers. \n",
    "\n",
    "Finally, train a Neural Network with one-hidden layer using ReLU activation units to see whether we can boost our model's performance further.  \n",
    "\n",
    "\n",
    "Previously in [1_notmnist.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb), we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "This post is modified from the [jupyter notebook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb) originated from the Udacity MOOC course: [Deep learning by Google](https://www.udacity.com/course/deep-learning--ud730). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "hidden": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load notMNIST dataset \n",
    "\n",
    "This time we will use the dataset which has been normalized and randomized before to omit the data preprocessing step.\n",
    "\n",
    "Tips:\n",
    "- Release memory after loading big-size dataset using `del`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "cellView": "both",
    "code_folding": [],
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "hidden": true,
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 658.8 MB\n",
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'datasets/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    print('Dataset size: {:.1f} MB'.format(os.stat(pickle_file).st_size / 2 ** 20))\n",
    "\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "## Reformat data for easier training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat both pixels(features) and labels that's more adapted to the models we're going to train:\n",
    "- features(pixels) as a flat matrix with shape = (#total pixels, #instances)\n",
    "\n",
    "<center><img src=\"images/flattened_features.png\" style=\"width:70%\"><caption><center> <u><font color='purple'> Figure 1</u><font color='purple'>: Flattened features <br> <font color='black'> </center>\n",
    "\n",
    "\n",
    "- labels as float 1-hot encodings with shape = (#type of labels, #instances)\n",
    "\n",
    "<center><img src=\"images/flattened_ground_true.png\" style=\"width:70%\"><caption><center> <u><font color='purple'> Figure 2</u><font color='purple'>: Flattened labels <br> <font color='black'> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips:\n",
    "- Notice that we use different shape of matrix with the original TensorFlow example [nookbook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb) because I think it's easier to understand how matrix multiplication work by imagining each training/test instance as a column vector. But in response to this change, we have to modify several code in order to make it works!\n",
    "    * Transpose logits and labels when calling `tf.nn.softmax_cross_entropy_with_logits`\n",
    "    * Set `dim = 0` when using `tf.nn.softmax`\n",
    "    * Set `axis = 0` when using `np.argmax` to compute accuracy\n",
    "- One-hot encode labels by compare the label with the 0-9 array and transform True/False array as float array use `astype(np.float32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "hide_input": false,
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (784, 200000) (10, 200000)\n",
      "Validation set (784, 10000) (10, 10000)\n",
      "Test set (784, 10000) (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32).T\n",
    "    \n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32).T # key point1\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "## Logistic regression with gradient descent\n",
    "For logistic regression, we use the formula $WX + b = Y'$ to do the computation. W is of shape (10, 784), X is of shape (784, m) and Y' is of shape (10, m) where $m$ is the number of training instances/images. After compute the probabilities of 10 classes stored in Y', we will use built-in `tf.nn.softmax_cross_entropy_with_logits` to compute cross-entropy between Y' and Y(train_labels) as cost.   \n",
    "\n",
    "We will first instruct Tensorflow how to do all the computation and make it run the optimization several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Build the Tensorflow computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "cellView": "both",
    "code_folding": [],
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "# when we want to create multiple graphs in the same script,\n",
    "# use this to encapsulate each graph and run session right after graph definition\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:, :train_subset])\n",
    "    tf_train_labels = tf.constant(train_labels[:, :train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([num_labels, image_size * image_size]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels, 1]))\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(weights, tf_train_dataset) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.transpose(tf_train_labels), logits=tf.transpose(logits)))\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits, dim=0)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(weights, tf_valid_dataset) + biases, dim=0)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(weights, tf_test_dataset) + biases, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips:\n",
    "- As we saw before, `logits = tf.matmul(weights, tf_train_dataset) + biases` is equivalent to the logistic regression formula $Y' = WX + b$\n",
    "- Transpose y_hat and y to fit in `softmax_cross_entropy_with_logits`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent by iterating computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Now we can tell TensorFlow to run this computation and iterate.  \n",
    "Here we will use `tqdm` library to help us easily visualize the progress and the time used in the iterations.\n",
    "\n",
    "Tips:\n",
    "- Use `np.argmax(predictions, axis=0)` to transfrom one-hot encoded labels back to singe number for every data points.\n",
    "- Use `.eval()` to get the predictions for test/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a776469c0f394bab94bc900cf03f469d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=801), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at step 0: 20.057. Training acc: 6.4%, Validation acc: 10.0%.\n",
      "Cost at step 100: 2.326. Training acc: 70.9%, Validation acc: 70.7%.\n",
      "Cost at step 200: 1.868. Training acc: 73.9%, Validation acc: 73.4%.\n",
      "Cost at step 300: 1.611. Training acc: 75.4%, Validation acc: 74.5%.\n",
      "Cost at step 400: 1.436. Training acc: 76.4%, Validation acc: 74.8%.\n",
      "Cost at step 500: 1.306. Training acc: 77.1%, Validation acc: 75.1%.\n",
      "Cost at step 600: 1.207. Training acc: 77.8%, Validation acc: 75.5%.\n",
      "Cost at step 700: 1.127. Training acc: 78.5%, Validation acc: 75.6%.\n",
      "Cost at step 800: 1.062. Training acc: 79.2%, Validation acc: 75.9%.\n",
      "\n",
      "Test acc: 82.8%\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tnrange\n",
    "num_steps = 801\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    \"\"\"For every (logit/Z, y) pair, get the (predicted label, label) and count the \n",
    "    occurence where predicted label == label and divide by the total number of \n",
    "    data points.\n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(predictions, axis=0) == np.argmax(labels, axis=0)) \n",
    "            / labels.shape[1] * 100)\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "#     correct_prediction = tf.equal(tf.argmax(predictions), tf.argmax(labels))\n",
    "\n",
    "#     # Calculate accuracy on the test set\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    return accruacy\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in tnrange(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            \n",
    "            print('Cost at step {}: {:.3f}. Training acc: {:.1f}%, Validation acc: {:.1f}%.'\\\n",
    "                  .format(step, l,\n",
    "                          accuracy(predictions, train_labels[:, :train_subset]),\n",
    "                          accuracy(valid_prediction.eval(), valid_labels), \">\"))\n",
    "            \n",
    "    # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "    # just to get that one numpy array. Note that it recomputes all its graph\n",
    "    # dependencies.\n",
    "    print('Test acc: {:.1f}%'.format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with SGD\n",
    "Or more precisely, mini-batch approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "From the result above, we can see it cost about 20 seconds (on my computer) to iterate 10,000 training instances by simple gradient descent. Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`.\n",
    "\n",
    "Tips:\n",
    "- The difference between SGD and gradient descent is that the former don't use whole training set to compute gradient descent, instead just use a 'mini-batch' of it and assume the corresponding gradient descent is the way to optimize. So we will keep using `GradientDescentOptimizer` but with a different `loss` computed from a smaller sub-training set.\n",
    "\n",
    "<center><img src=\"images/sgd_vs_gradient_descent.png\" style=\"width:70%\"><caption><center> <u><font color='purple'> Figure 3</u><font color='purple'>: SGD vs Gradient Descent<br> <font color='black'> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(image_size * image_size, batch_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(num_labels, batch_size))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([num_labels, image_size * image_size]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels, 1]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(weights, tf_train_dataset) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.transpose(tf_train_labels), logits=tf.transpose(logits)))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits, dim=0)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(weights, tf_valid_dataset) + biases, dim=0)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(weights, tf_test_dataset) + biases, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "### Iterate using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2a36e34f9843b5a38a9fe105281093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 20.939. batch acc: 6.2%, Valid acc: 9.7%.\n",
      "Minibatch loss at step 500: 2.546. batch acc: 70.3%, Valid acc: 75.1%.\n",
      "Minibatch loss at step 1000: 1.520. batch acc: 74.2%, Valid acc: 76.3%.\n",
      "Minibatch loss at step 1500: 1.441. batch acc: 76.6%, Valid acc: 77.8%.\n",
      "Minibatch loss at step 2000: 1.135. batch acc: 79.7%, Valid acc: 77.1%.\n",
      "Minibatch loss at step 2500: 1.225. batch acc: 72.7%, Valid acc: 78.8%.\n",
      "Minibatch loss at step 3000: 0.932. batch acc: 76.6%, Valid acc: 79.4%.\n",
      "\n",
      "Test acc: 86.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in tnrange(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[1] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[:, offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[:, offset:(offset + batch_size)]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {\n",
    "            tf_train_dataset: batch_data,\n",
    "            tf_train_labels: batch_labels\n",
    "        }\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step {}: {:.3f}. batch acc: {:.1f}%, Valid acc: {:.1f}%.'\\\n",
    "                  .format(step, l,\n",
    "                          accuracy(predictions, batch_labels),\n",
    "                          accuracy(valid_prediction.eval(), valid_labels)))\n",
    "            \n",
    "    print('Test acc: {:.1f}%'.format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took only about 3 seconds in my computer to finish the optimization using SGD (which took gradient descent about 20 seconds) and got a even slightly better result. The key of SGD is take **randomized** samples / mini-batches and feed that into the model every iteration (thus the `feed_dict` term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "##  2-layer NN with ReLU units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "Instead all just linear combination of features, we want to introduce non-linearlity in our logistic regression. By turning the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes, we should be able to improve validation / test accuracy.\n",
    "\n",
    "A 2-layer NN (1-hidden layer NN) look like this:\n",
    "\n",
    "<center><img src=\"images/2layer_nn.png\" style=\"width:30%\"><caption><center> <u><font color='purple'> Figure 4</u><font color='purple'>: 1 hidden-layer NN <br> <font color='black'> </center>\n",
    "\n",
    "A ReLU activation unit look like this:\n",
    "\n",
    "<center><img src=\"images/relu.png\" style=\"width:30%\"><caption><center> <u><font color='purple'> Figure 5</u><font color='purple'>: ReLU <br> <font color='black'> </center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build compuation graph\n",
    "In this part, use the notation $X$ in replace of  `dataset`. The weights and biases of the hidden layer are denoted as $W1$ and $b1$, and the weights and biases of the output layer are denoted as $W2$ and $b2$.\n",
    "\n",
    "Thus the pre-activation output(logits) of output layer is computed as $ logits = W2 * ReLU(W1 * X + b1) + b2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_unit = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # placeholder for mini-batch when training \n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(image_size * image_size, batch_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(num_labels, batch_size))\n",
    "    \n",
    "    # use all valid/test set\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # initialize weights, biases\n",
    "    # notice that we have a new hidden layer so we now have W1, b1, W2, b2\n",
    "    W1 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_unit, image_size * image_size]))\n",
    "    b1 = tf.Variable(tf.zeros([num_hidden_unit, 1]))\n",
    "    W2 = tf.Variable(\n",
    "        tf.truncated_normal([num_labels, num_hidden_unit]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels, 1]))\n",
    "\n",
    "\n",
    "    # training computation\n",
    "    logits = tf.matmul(W2, tf.nn.relu(tf.matmul(W1, tf_train_dataset) + b1)) + b2    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.transpose(tf_train_labels), logits=tf.transpose(logits)))\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # valid / test prediction - y_hat\n",
    "    train_prediction = tf.nn.softmax(logits, dim=0)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(W2, tf.nn.relu(tf.matmul(W1, tf_valid_dataset) + b1)) + b2, dim=0)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(W2, tf.nn.relu(tf.matmul(W1, tf_test_dataset) + b1)) + b2, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f379ee5b4ed04eb18e5ee8d6b1e69a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 409.203. batch acc: 4.7%, Valid acc: 30.5%.\n",
      "Minibatch loss at step 500: 12.319. batch acc: 75.8%, Valid acc: 80.7%.\n",
      "Minibatch loss at step 1000: 12.638. batch acc: 74.2%, Valid acc: 80.8%.\n",
      "Minibatch loss at step 1500: 7.635. batch acc: 77.3%, Valid acc: 81.2%.\n",
      "Minibatch loss at step 2000: 7.322. batch acc: 80.5%, Valid acc: 81.4%.\n",
      "Minibatch loss at step 2500: 10.451. batch acc: 76.6%, Valid acc: 80.1%.\n",
      "Minibatch loss at step 3000: 3.914. batch acc: 83.6%, Valid acc: 82.7%.\n",
      "\n",
      "Test acc: 88.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # initialized parameters\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(\"Initialized\")\n",
    "    # take steps to optimize\n",
    "    for step in tnrange(num_steps):\n",
    "        \n",
    "        # generate randomized mini-batches\n",
    "        offset = (step * batch_size) % (train_labels.shape[1] - batch_size)\n",
    "        batch_data = train_dataset[:, offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[:, offset:(offset + batch_size)]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset: batch_data,\n",
    "            tf_train_labels: batch_labels\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step {}: {:.3f}. batch acc: {:.1f}%, Valid acc: {:.1f}%.'\\\n",
    "                  .format(step, l,\n",
    "                          accuracy(predictions, batch_labels),\n",
    "                          accuracy(valid_prediction.eval(), valid_labels)))\n",
    "            \n",
    "    print('Test acc: {:.1f}%'.format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Because we use a more complex model(1 hidden-layer NN), it take a little longer to train, but we're able to gain more performance from logistic regression even with the same hyper-parameter settings (learning rate = 0.5, batch_size=128). Better performance may be gained by tuning hyper parameters of the 2 layer NN. Also notice that by using mini-batch / SGD, we can save lots of time training models and even get a better result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "Author": "Lee Meng",
  "Category": "Deep Learning",
  "Date": "2017-09-21 23:50",
  "Description": "The goal here is to progressively train deeper and more accurate models using TensorFlow. We will first load the notMNIST dataset which we have done data cleaning. For the classification problem, we will first train two logistic regression models use simple gradient descent, stochastic gradient descent (SGD) respectively for optimization to see the difference between these optimizers.",
  "Slug": "using-tensorflow-to-train-a-shallow-nn-with-stochastic-gradient-descent",
  "Summary": "Using TensorFlow to Train a Shallow NN with Stochastic Gradient Descent",
  "Tags": "Python, TensorFlow, Deep Learning, Neural Networks, Optimization, NotMNIST, Machine Learning, Image Recognition, SGD, Gradient Descent, Deep Learning by Google, Machine Learning Engineer by kaggle, Udacity",
  "Title": "Using TensorFlow to Train a Shallow NN with Stochastic Gradient Descent",
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "255px",
    "width": "472px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "879px",
    "left": "0px",
    "right": "20px",
    "top": "137px",
    "width": "236px"
   },
   "toc_section_display": "none",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "262px",
    "left": "787.273px",
    "right": "20px",
    "top": "120px",
    "width": "346px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
