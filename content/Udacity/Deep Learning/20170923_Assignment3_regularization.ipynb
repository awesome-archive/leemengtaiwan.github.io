{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author: Lee Meng\n",
    "- category: Deep Learning\n",
    "- date: 2017-09-25 16:00\n",
    "- title: Regularization for Multi-layer Neural Networks in Tensorflow\n",
    "- slug: regularization-for-multi-layer-neural-networks-in-tensorflow\n",
    "- tags: Tensorflow, Python, Deep Learning, Regularization, Deep Learning by Google, Machine Learning Engineer by kaggle, Udacity\n",
    "- description: The goal of this assignment is to explore regularization techniques.\n",
    "- summary: The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "The goal of this assignment is to explore regularization techniques.\n",
    "The original notebook can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/3_regularization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "from tqdm import tnrange\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NotMNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'datasets/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    X_train = save['train_dataset']\n",
    "    Y_train = save['train_labels']\n",
    "    X_valid = save['valid_dataset']\n",
    "    Y_valid = save['valid_labels']\n",
    "    X_test = save['test_dataset']\n",
    "    Y_test = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', X_train.shape, Y_train.shape)\n",
    "    print('Validation set', X_valid.shape, Y_valid.shape)\n",
    "    print('Test set', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings.\n",
    "\n",
    "As I did in previous notebook, this reformat operation will be different from the operation suggested by the original [notebook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/3_regularization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (784, 200000) (10, 200000)\n",
      "Validation set (784, 10000) (10, 10000)\n",
      "Test set (784, 10000) (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32).T\n",
    "    \n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32).T\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "X_train, Y_train = reformat(X_train, Y_train)\n",
    "X_valid, Y_valid = reformat(X_valid, Y_valid)\n",
    "X_test, Y_test = reformat(X_test, Y_test)\n",
    "print('Training set', X_train.shape, Y_train.shape)\n",
    "print('Validation set', X_valid.shape, Y_valid.shape)\n",
    "print('Test set', X_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Accuracy as Default Metric\n",
    "Because as we explored before, there exist no unbalanced problem in the dataset,  \n",
    "so accuracy alone will be sufficient for evaluating performance of our model on the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (\n",
    "        np.sum(np.argmax(predictions, axis=0) == np.argmax(\n",
    "            labels, axis=0)) / labels.shape[1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer NN as base model\n",
    "In order to test the effect with/without regularization, we will use a little more complex neural network with 2 hidden layers as our base model. And we will be using ReLU as our activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 1e-2\n",
    "lamba = 1e-3\n",
    "keep_prob = 0.5\n",
    "batch_size = 128\n",
    "num_steps = 501\n",
    "n0 = image_size * image_size # input size\n",
    "n1 = 1024 # first hidden layer\n",
    "n2 = 512 # second hidden layer\n",
    "n3 = 256 # third hidden layer\n",
    "n4 = num_labels # output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# build a model which let us able to choose different optimzation mechnism\n",
    "def model(lamba=0, learning_rate=learning_rate,\n",
    "          keep_prob=1, learning_decay=False,\n",
    "          batch_size=batch_size, num_steps=num_steps, n1=n1, n2=n2, n3=n3):\n",
    "    print(\n",
    "    \"\"\"\n",
    "    Train 3-layer NN with following settings:\n",
    "    Regularization lambda: {}\n",
    "    Learning rate: {}\n",
    "    learning_decay: {}\n",
    "    keep_prob: {}\n",
    "    Batch_size: {}\n",
    "    Number of steps: {}\n",
    "    n1, n2, n3: {}, {}, {}\"\"\".format(lamba, learning_rate,\n",
    "                                     learning_decay, keep_prob,\n",
    "                                     batch_size, num_steps, n1, n2, n3))\n",
    "    \n",
    "    # construct computation graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # placeholder for mini-batch when training \n",
    "        X = tf.placeholder(tf.float32, shape=(n0, batch_size))\n",
    "        Y = tf.placeholder(tf.float32, shape=(num_labels, batch_size))\n",
    "        global_step = tf.Variable(0)\n",
    "\n",
    "        # use all valid/test set\n",
    "        tf_X_valid = tf.constant(X_valid)\n",
    "        tf_X_test = tf.constant(X_test)\n",
    "\n",
    "        # initialize weights, biases\n",
    "        # notice that we have two hidden \n",
    "        # layers so we now have W1, b1, W2, b2, W3, b3\n",
    "        W1 = tf.Variable(tf.truncated_normal([n1, n0], stddev=np.sqrt(2.0 / n0)))\n",
    "        W2 = tf.Variable(tf.truncated_normal([n2, n1], stddev=np.sqrt(2.0 / n1)))\n",
    "        W3 = tf.Variable(tf.truncated_normal([n3, n2], stddev=np.sqrt(2.0 / n2)))\n",
    "        W4 = tf.Variable(tf.truncated_normal([n4, n3], stddev=np.sqrt(2.0 / n3)))\n",
    "        b1 = tf.Variable(tf.zeros([n1, 1]))\n",
    "        b2 = tf.Variable(tf.zeros([n2, 1]))\n",
    "        b3 = tf.Variable(tf.zeros([n3, 1]))\n",
    "        b4 = tf.Variable(tf.zeros([n4, 1]))\n",
    "\n",
    "\n",
    "        # training computation\n",
    "        Z1 = tf.matmul(W1, X) + b1\n",
    "        A1 = tf.nn.relu(Z1) if keep_prob == 1 else tf.nn.dropout(tf.nn.relu(Z1), keep_prob)\n",
    "        Z2 = tf.matmul(W2, A1) + b2\n",
    "        A2 = tf.nn.relu(Z2) if keep_prob == 1 else tf.nn.dropout(tf.nn.relu(Z2), keep_prob)\n",
    "        Z3 = tf.matmul(W3, A2) + b3\n",
    "        A3 = tf.nn.relu(Z3) if keep_prob == 1 else tf.nn.dropout(tf.nn.relu(Z3), keep_prob)\n",
    "        Z4 = tf.matmul(W4, A3) + b4\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.transpose(Y), logits=tf.transpose(Z4)))\n",
    "        if lamba:\n",
    "            loss += lamba * \\\n",
    "            (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3) + tf.nn.l2_loss(W4))\n",
    "\n",
    "        # optimizer\n",
    "        if learning_decay:\n",
    "            learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        else:\n",
    "            optimizer = (tf.train\n",
    "                         .GradientDescentOptimizer(learning_rate).minimize(loss))\n",
    "\n",
    "        # valid / test prediction\n",
    "        Y_pred = tf.nn.softmax(Z4, dim=0)\n",
    "        Y_vaild_pred = tf.nn.softmax(\n",
    "            tf.matmul(W4, tf.nn.relu(\n",
    "                tf.matmul(W3, tf.nn.relu(\n",
    "                    tf.matmul(W2, tf.nn.relu(\n",
    "                        tf.matmul(W1, tf_X_valid) + b1)) + b2)) + b3)) + b4, dim=0)\n",
    "        Y_test_pred = tf.nn.softmax(\n",
    "            tf.matmul(W4, tf.nn.relu(\n",
    "                tf.matmul(W3, tf.nn.relu(\n",
    "                    tf.matmul(W2, tf.nn.relu(\n",
    "                        tf.matmul(W1, tf_X_test) + b1)) + b2)) + b3)) + b4, dim=0)\n",
    "    \n",
    "    # define training\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # initialized parameters\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in tnrange(num_steps):\n",
    "\n",
    "            # generate randomized mini-batches from training data\n",
    "            offset = (step * batch_size) % (Y_train.shape[1] - batch_size)\n",
    "            batch_X = X_train[:, offset:(offset + batch_size)]\n",
    "            batch_Y = Y_train[:, offset:(offset + batch_size)]\n",
    "\n",
    "            # train model\n",
    "            _, l, batch_Y_pred = sess.run(\n",
    "                [optimizer, loss, Y_pred], feed_dict={X: batch_X, Y: batch_Y})\n",
    "\n",
    "            if (step % 200 == 0):\n",
    "                print('Minibatch loss at step {}: {:.3f}. batch acc: {:.1f}%, Valid acc: {:.1f}%.'\\\n",
    "                      .format(step, l,\n",
    "                              accuracy(batch_Y_pred, batch_Y),\n",
    "                              accuracy(Y_vaild_pred.eval(), Y_valid)))\n",
    "\n",
    "        print('Test acc: {:.1f}%'.format(accuracy(Y_test_pred.eval(), Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Train 3-layer NN with following settings:\n",
      "    Regularization lambda: 0\n",
      "    Learning rate: 0.5\n",
      "    learning_decay: False\n",
      "    keep_prob: 1\n",
      "    Batch_size: 128\n",
      "    Number of steps: 1601\n",
      "    n1, n2, n3: 1024, 512, 256\n",
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a1e075513f4d02a8579c4fcd5b8509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1601), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.374. batch acc: 14.1%, Valid acc: 28.4%.\n",
      "Minibatch loss at step 200: 0.600. batch acc: 82.0%, Valid acc: 84.9%.\n",
      "Minibatch loss at step 400: 0.429. batch acc: 89.8%, Valid acc: 85.8%.\n",
      "Minibatch loss at step 600: 0.372. batch acc: 87.5%, Valid acc: 85.7%.\n",
      "Minibatch loss at step 800: 0.454. batch acc: 89.1%, Valid acc: 87.7%.\n",
      "Minibatch loss at step 1000: 0.374. batch acc: 87.5%, Valid acc: 88.1%.\n",
      "Minibatch loss at step 1200: 0.251. batch acc: 91.4%, Valid acc: 88.8%.\n",
      "Minibatch loss at step 1400: 0.397. batch acc: 89.8%, Valid acc: 89.0%.\n",
      "Minibatch loss at step 1600: 0.470. batch acc: 82.0%, Valid acc: 88.9%.\n",
      "\n",
      "Test acc: 94.2%\n"
     ]
    }
   ],
   "source": [
    "model(learning_rate=0.5, num_steps=1601)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "## L2 regularization\n",
    "\n",
    "Introduce and tune L2 regularization for the models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Train 3-layer NN with following settings:\n",
      "    Regularization lambda: 0.1\n",
      "    Optimizer: sgd\n",
      "    Learning rate: 0.01\n",
      "    Batch_size: 128\n",
      "    Number of steps: 501\n",
      "    n1, n2: 512, 256\n",
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6986da296646b9aac2266326edcf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=501), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 22969.777. batch acc: 9.4%, Valid acc: 19.3%.\n",
      "Minibatch loss at step 200: 13876.185. batch acc: 74.2%, Valid acc: 75.2%.\n",
      "Minibatch loss at step 400: 9266.566. batch acc: 78.1%, Valid acc: 74.3%.\n",
      "\n",
      "Test acc: 81.4%\n"
     ]
    }
   ],
   "source": [
    "# for lamda in [1 / 10 ** i for i in list(np.arange(1, 4))]:\n",
    "#     model(lamba=lamda)\n",
    "    \n",
    "model(lamba=0.1, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "## Case of overfitting\n",
    "\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Train 3-layer NN with following settings:\n",
      "    Regularization lambda: 0\n",
      "    Learning rate: 0.01\n",
      "    learning_decay: False\n",
      "    keep_prob: 1\n",
      "    Batch_size: 128\n",
      "    Number of steps: 10\n",
      "    n1, n2, n3: 1024, 512, 256\n",
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312b63e26f364cec95540b452b4ec95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.442. batch acc: 8.6%, Valid acc: 11.4%.\n",
      "\n",
      "Test acc: 20.7%\n"
     ]
    }
   ],
   "source": [
    "model(num_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "## Dropout\n",
    "\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "\n",
    "<img src=\"images/dropout1_kiank.mp4\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Train 3-layer NN with following settings:\n",
      "    Regularization lambda: 0\n",
      "    Learning rate: 0.01\n",
      "    learning_decay: False\n",
      "    keep_prob: 0.5\n",
      "    Batch_size: 128\n",
      "    Number of steps: 10\n",
      "    n1, n2, n3: 1024, 512, 256\n",
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a5be47b2a14f488d9d302df5d5988d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.784. batch acc: 7.0%, Valid acc: 10.0%.\n",
      "\n",
      "Test acc: 17.3%\n"
     ]
    }
   ],
   "source": [
    "model(num_steps=10, keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "## Boost performance by using Multi-layer NN\n",
    "\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Train 3-layer NN with following settings:\n",
      "    Regularization lambda: 0\n",
      "    Learning rate: 0.01\n",
      "    learning_decay: True\n",
      "    keep_prob: 1\n",
      "    Batch_size: 128\n",
      "    Number of steps: 1501\n",
      "    n1, n2, n3: 1024, 512, 256\n",
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2b75cf5f6542f99a17c5ad02bc49fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1501), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.395. batch acc: 12.5%, Valid acc: 37.0%.\n",
      "\n",
      "Minibatch loss at step 200: 0.589. batch acc: 82.0%, Valid acc: 84.7%.\n",
      "Minibatch loss at step 400: 0.409. batch acc: 89.1%, Valid acc: 86.2%.\n",
      "Minibatch loss at step 600: 0.396. batch acc: 88.3%, Valid acc: 86.5%.\n",
      "Minibatch loss at step 800: 0.435. batch acc: 88.3%, Valid acc: 87.6%.\n",
      "Minibatch loss at step 1000: 0.407. batch acc: 85.2%, Valid acc: 88.5%.\n",
      "Minibatch loss at step 1200: 0.262. batch acc: 91.4%, Valid acc: 88.9%.\n",
      "Minibatch loss at step 1400: 0.411. batch acc: 87.5%, Valid acc: 88.8%.\n",
      "\n",
      "Test acc: 94.3%\n"
     ]
    }
   ],
   "source": [
    "model(learning_decay=True, num_steps=1501, lamba=0, keep_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "Author": "Lee Meng",
  "Category": "Deep Learning",
  "Date": "2017-09-25 16:00",
  "Description": "The goal of this assignment is to explore regularization techniques.",
  "Slug": "regularization-for-multi-layer-neural-networks-in-tensorflow",
  "Summary": "Using L2 and dropout regularization for nn",
  "Tags": "Tensorflow, Python, Deep Learning, Regularization, Deep Learning by Google, Machine Learning Engineer by kaggle, Udacity",
  "Title": "Regularization for Multi-layer Neural Networks in Tensorflow",
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {
    "height": "172px",
    "width": "375px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "325px",
    "left": "833.636px",
    "right": "20px",
    "top": "120px",
    "width": "315px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
