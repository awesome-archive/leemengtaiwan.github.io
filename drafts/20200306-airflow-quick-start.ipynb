{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ＴＯＤＯ：簡介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "draw.io 圖片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workflow management system (WMS)\n",
    "Workflow, DAG, operator, task, task instance, operator\n",
    "Sensor, operator\n",
    "meta-data db, scheduler, webserver, log, ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#blockquote\n",
    "The Airflow Platform is a tool for describing, executing, and monitoring workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/airflow/airflow-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "小知識\n",
    "- 預設用 sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conda create -n airflow python=3 -y\n",
    "\n",
    "source activate airflow\n",
    "\n",
    "export AIRFLOW_HOME=\"$(pwd)\"\n",
    "\n",
    "echo $AIRFLOW_HOM\n",
    "\n",
    "pip install apache-airflow\n",
    "\n",
    "pip install cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow import DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow.operators.bash_operator import BashOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PythonOperator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 常用 CLI 指令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看有什麼 DAGs\n",
    "```bash\n",
    "airflow list_dags\n",
    "```\n",
    "\n",
    "看某個 DAG 裡頭有什麼 tasks （以樹狀呈現）。要注意呈現的樹狀從左到右， dependencies 是從下到上。\n",
    "```bash\n",
    "airflow list_tasks DAG_ID --tree\n",
    "\n",
    "<Task(BashOperator): transform>\n",
    "    <Task(BashOperator): extract>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試 tasks\n",
    "測試的特性：\n",
    "- 不管該 task 的 dependencies，直接執行該 task。\n",
    "- 不會把 log 存起來，而是直接丟到 stdout 讓我們查看。\n",
    "\n",
    "基本格式：\n",
    "```bash\n",
    "airflow test DAG_ID TASK_ID EXECUTE_DATE\n",
    "```\n",
    "\n",
    "範例：\n",
    "```bash\n",
    "airflow test etl extract 2018-03-16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfill\n",
    "主要應用應該是在建立 / 測試好一個新的 DAG 以後，把過去的資料也同樣做轉換。使用 [Datestamp 做 Data Partition 可以讓 Data Backfilling 更有效率](https://towardsdatascience.com/a-beginners-guide-to-data-engineering-part-ii-47c4e7cbda71)。這樣我們就可以只使用一個 ETL flow 就把新舊資料做完轉換。\n",
    "\n",
    "特性\n",
    "- 會看 tasks 之間的 dependencies。\n",
    "- 這邊的時間是模擬此 DAG 在這個時間範圍內執行。\n",
    "- 結果會輸出到硬碟上的 log 檔案以及資料庫裡頭。\n",
    "\n",
    "基本格式：\n",
    "```bash\n",
    "airflow backfill DAG_ID -s START_DATE -e END_DATE\n",
    "```\n",
    "\n",
    "範例：\n",
    "```bash\n",
    "airflow backfill etl -s 2018-03-05 -e 2018-03-06\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 雜項\n",
    "刪除某 DAG 所有的 task instances\n",
    "```bash\n",
    "airflow clear DAG_ID\n",
    "```\n",
    "\n",
    "清空 db\n",
    "```bash\n",
    "airflow resetdb\n",
    "```\n",
    "\n",
    "讓 scheduler 開始監控 DAG\n",
    "```bash\n",
    "airflow unpause DAG_ID\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推薦閱讀\n",
    "- Airflow 作者的 [The Rise of Data Engineer](https://medium.freecodecamp.org/the-rise-of-the-data-engineer-91be18f1e603)\n",
    "- Airbnb 的資料科學家 Robert Chang 對 [Data Enginner 的介紹]((https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7).\n",
    "    * data warehousing(DWH) 的重要以及與其息息相關的 ETL 的關係\n",
    "    * ETL (Extract, Transform, Load）是如何對應到 Airflow 的 Sensor, Operator\n",
    "    * JVM-centric ETL 跟 SQL-centric ETL 的比較\n",
    "- [Quizlet 介紹他們將 Airflow 當作 WMS 使用的系列文](https://medium.com/tech-quizlet/going-with-the-flow-how-quizlet-uses-apache-airflow-to-execute-complex-data-processing-pipelines-1ca546f8cc68)\n",
    "    * [Part1: 為何要使用CRON以外的方法？](https://medium.com/@dustinstansbury/beyond-cron-an-introduction-to-workflow-management-systems-19987afcdb5e)\n",
    "        * WMS 比 CRON 好的地方\n",
    "            * handle task dependencies\n",
    "            * error handling, retry the task and all dependencies\n",
    "            * 專案越多使用 CRON 就有用多種用法，沒有 owners 難以管理\n",
    "    * [Part2：一個好的WMS要有什麼功能？](https://towardsdatascience.com/why-quizlet-chose-apache-airflow-for-executing-data-workflows-3f97d40e9571)\n",
    "        * smart scheduling, 定期要跑的 task 超過他預期跑的時間怎解？\n",
    "        * 依賴管理，失敗的 task 以及相關的 tasks 要能 retry, 沒有依賴的 task 要能最有效率先跑\n",
    "        * 當 task 失敗或者是違反 SLA 時通知我們\n",
    "        * pipeline 是用程式定義的，而非寫死 (e.g., Airflow 的 Python, Jinja)\n",
    "        * 其他：scalability, flexibility(什麼 task 都能處理)\n",
    "    * [Part3：實際Airflow的操作](https://medium.com/@dustinstansbury/understanding-apache-airflows-key-concepts-a96efed52b1a)\n",
    "        * Define workflow in code 的好處\n",
    "        * 如何透過繼承 BaseOperator 自訂 Sensor, Operator\n",
    "        * Airflow 結構, 各個 components 在做什麼 (scheduler, webserver, executers, UI, meta-data DB)\n",
    "    * [Part4: 實際 deploy 的注意事項以及 Airflow 缺點](https://medium.com/@dustinstansbury/how-quizlet-uses-apache-airflow-in-practice-a903cbb5626d)\n",
    "        * Airflow 不適合 streaming data, 如果需要可以考慮 [Google Dataflow](https://cloud.google.com/dataflow/)\n",
    "        * 還有一些小細節值得注意，像是 `skipped != success`, 不要隨便換 `DAG_ID`\n",
    "- [Airflow 的注意事項(Gotcha)](https://gtoonstra.github.io/etl-with-airflow/gotchas.html)\n",
    "    \n",
    "    \n",
    "* import python scripts in subdirectory\n",
    "    * https://stackoverflow.com/questions/1260792/import-a-file-from-a-subdirectory\n",
    "* 跑 python script\n",
    "    * https://stackoverflow.com/questions/39610313/how-to-use-airflow-to-run-a-list-of-python-tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "出現找不到 dag 然後確定有用 python 通過 parse 測驗以後, 確認目前的 airflow 的執行路徑, 不對的話就設定 export ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 疑難雜症\n",
    "- 產生 fernet key: https://bcb.github.io/airflow/fernet-key\n",
    "- [task instance table not found](https://groups.google.com/forum/#!searchin/airbnb_airflow/upgradedb%7Csort:date/airbnb_airflow/ueaGk6FFtqw/KGpmMuzVDgAJ)\n",
    "- UI break: https://github.com/puckel/docker-airflow/issues/116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ignore_from_here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ME 應用\n",
    "將公司所有 ETL / data workflow 整合管理. fault-torelance (retry) 確保\n",
    "\n",
    "### TODO\n",
    "- 與 AWS lambda 的整合, 看 airflow repo\n",
    "\n",
    "\n",
    "\n",
    "### 實例\n",
    "- series, maker master 等爬下來的資料，如果之後想用別的 logic 重新 parse, 可以使用 backfilling\n",
    "- mongo2 -> es, 失敗的時候不止送email 可以做別的處理\n",
    "- 孫現在做的 biztel 也一樣\n",
    "- NPS 定期更新\n",
    "- 未來 marketing, ReRe 的 google data + sales data + google data studio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "Author": "Lee Meng",
  "Category": "",
  "Date": "",
  "Slug": "gentle-introduction-to-airflow-platform",
  "Summary": "Airflow 實戰手冊",
  "Tags": "python, airflow, data engineering",
  "Title": "Airflow 實戰手冊",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
